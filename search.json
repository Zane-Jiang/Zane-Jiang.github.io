[{"title":"CUDA算子优化-Reduce","path":"/2025/09/06/CUDA 算子优化系列/CUDA算子优化-Reduce/","content":"本文是对官方reduce优化的精简，方便个人复习,详细回顾参考知乎深入浅出系列 leetGPU 问题解决 展示 reduce的7种优化 V0_0 naive跨步相加，非全局内存访问， __global__ void reduce_naive(float *g_idata, float *g_odata, unsigned int n) unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx = n) return; // 直接在全局内存上进行跨步归约 for (unsigned int stride = 1; stride blockDim.x; stride *= 2) // 确保所有前一轮的写入对下一轮可见 if (threadIdx.x % (2 * stride) == 0) g_idata[idx] += g_idata[idx + stride]; __syncthreads(); // 一个线程块的结果写回全局内存 if (threadIdx.x == 0) g_odata[blockIdx.x] = g_idata[blockIdx.x * blockDim.x]; 整个过程都在读写缓慢的全局内存，延迟很高，O(N)次操作需要访问全局内存O(NlogN)次全局内存，效率低 V0 shared_memory.unacawfvhnxu{zoom:50%;} __global__ void reduce0(int *g_idata, int *g_odata) extern __shared__ int sdata[]; // each thread loads one element from global to shared mem\tunsigned int tid = threadIdx.x;\tunsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\tsdata[tid] = g_idata[i]; __syncthreads();\t// do reduction in shared mem\tfor(unsigned int s=1; s blockDim.x; s *= 2) if (tid % (2*s) == 0) sdata[tid] += sdata[tid + s]; __syncthreads(); // write result for this block to global mem\tif (tid == 0) g_odata[blockIdx.x] = sdata[0]; 问题： 会造成线程束分化，同一个warps内执行的操作不一致 V1 线程束分化__global__ void reduce1(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*blockDim.x+threadIdx.x; unsigned int tid=threadIdx.x; sdata[tid]=d_in[i]; __syncthreads(); // do reduction in shared mem for(unsigned int s=1; sblockDim.x; s*=2) int index = 2*s*tid; if(index blockDim.x)//不同线程束执行不同 sdata[index]+=sdata[index+s]; __syncthreads(); // write result for this block to global mem if(tid==0)d_out[blockIdx.x]=sdata[tid]; 本质，将区别通过index转换迁移到线程束之间而不是线程束内部，以前是每个线程束中都会出现if else，限制是一个线程束中的程序都一起执行或者一起不执行（最后几轮除外） 继续假定block中存在256个thread，即拥有256328个warp。当进行第1次迭代时，0-3号warp的indexblockDim.x， 4-7号warp的indexblockDim.x。对于每个warp而言，都只是进入到一个分支内，所以并不会存在warp divergence的情况。当进行第2次迭代时，0、1号两个warp进入计算分支。当进行第3次迭代时，只有0号warp进入计算分支。当进行第4次迭代时，只有0号warp的前16个线程进入分支。此时开始产生warp divergence。通过这种方式，我们消除了前3次迭代的warp divergence。 来自知乎《深入浅出GPU优化系列》 V2 bank冲突0号和32号元素冲突 __global__ void reduce2(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*blockDim.x+threadIdx.x; unsigned int tid=threadIdx.x; sdata[tid]=d_in[i]; __syncthreads(); // do reduction in shared mem for(unsigned int s=blockDim.x/2; s0; s=1) if(tid s) sdata[tid]+=sdata[tid+s]; __syncthreads(); // write result for this block to global mem if(tid==0)d_out[blockIdx.x]=sdata[tid]; 让一开始的访存跨度最大，恰好在元素多的时候错开，一个warp中不会同时访问同一个bank 问题： 一半的线程是空闲的 V3 空闲线程优化__global__ void reduce3(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*(blockDim.x*2)+threadIdx.x; //核心，每个block处理两个block unsigned int tid=threadIdx.x; sdata[tid]=d_in[i] + d_in[i+blockDim.x]; __syncthreads(); // do reduction in shared mem for(unsigned int s=blockDim.x/2; s0; s=1) if(tid s) sdata[tid]+=sdata[tid+s]; __syncthreads(); // write result for this block to global mem if(tid==0)d_out[blockIdx.x]=sdata[tid]; V4 展开最后一次计算减少同步__device__ void warpReduce(volatile float* cache,int tid) //volatile 的作用：禁止编译器优化，每次访问都必须真正从共享内存里取值/写值,防止缓存到寄存器里，然后重复使用寄存器的值，而不是每次都从共享内存读取 //32到1是跨度s cache[tid]+=cache[tid+32]; cache[tid]+=cache[tid+16]; cache[tid]+=cache[tid+8]; cache[tid]+=cache[tid+4]; cache[tid]+=cache[tid+2]; cache[tid]+=cache[tid+1];__global__ void reduce4(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*(blockDim.x*2)+threadIdx.x; unsigned int tid=threadIdx.x; sdata[tid]=d_in[i] + d_in[i+blockDim.x]; __syncthreads(); // do reduction in shared mem for(unsigned int s=blockDim.x/2; s32; s=1) if(tid s) sdata[tid]+=sdata[tid+s]; __syncthreads(); // write result for this block to global mem if(tid32)warpReduce(sdata,tid); if(tid==0)d_out[blockIdx.x]=sdata[tid]; 一个SIMD单元工作时，避免__syncthreads同步 V5 暴力for循环展开其实个人无法理解，现代版本中感觉不需要了 template unsigned int blockSize__device__ void warpReduce(volatile float* cache,int tid) if(blockSize = 64)cache[tid]+=cache[tid+32]; if(blockSize = 32)cache[tid]+=cache[tid+16]; if(blockSize = 16)cache[tid]+=cache[tid+8]; if(blockSize = 8)cache[tid]+=cache[tid+4]; if(blockSize = 4)cache[tid]+=cache[tid+2]; if(blockSize = 2)cache[tid]+=cache[tid+1];template unsigned int blockSize__global__ void reduce5(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*(blockDim.x*2)+threadIdx.x; unsigned int tid=threadIdx.x; sdata[tid]=d_in[i] + d_in[i+blockDim.x]; __syncthreads(); // do reduction in shared mem if(blockSize=512) if(tid256) sdata[tid]+=sdata[tid+256]; __syncthreads(); if(blockSize=256) if(tid128) sdata[tid]+=sdata[tid+128]; __syncthreads(); if(blockSize=128) if(tid64) sdata[tid]+=sdata[tid+64]; __syncthreads(); // write result for this block to global mem if(tid32)warpReduceblockSize(sdata,tid); if(tid==0)d_out[blockIdx.x]=sdata[tid]; V6版本和文章中讲的有些差距，本质一个线程处理多个数。 template unsigned int blockSize, int NUM_PER_THREAD__global__ void reduce6(float *d_in,float *d_out, unsigned int n) __shared__ float sdata[blockSize]; // each thread loads NUM_PER_THREAD element from global to shared mem unsigned int tid = threadIdx.x; unsigned int i = blockIdx.x * (blockSize * NUM_PER_THREAD) + threadIdx.x; sdata[tid] = 0; #pragma unroll for(int iter=0; iterNUM_PER_THREAD; iter++) sdata[tid] += d_in[i+iter*blockSize]; __syncthreads(); // do reduction in shared mem if (blockSize = 512) if (tid 256) sdata[tid] += sdata[tid + 256]; __syncthreads(); if (blockSize = 256) if (tid 128) sdata[tid] += sdata[tid + 128]; __syncthreads(); if (blockSize = 128) if (tid 64) sdata[tid] += sdata[tid + 64]; __syncthreads(); if (tid 32) warpReduceblockSize(sdata, tid); // write result for this block to global mem if (tid == 0) d_out[blockIdx.x] = sdata[0]; reduce6THREAD_PER_BLOCK, NUM_PER_THREADGrid,Block(d_a, d_out, N); V7 shuffle优化todo How_to_optimize_in_GPUreducereduce_v7_shuffle.cu at master · Liu-xiandongHow_to_optimize_in_GPU 采用了shuffle指令之后，warp内的线程可以直接对其他线程的寄存器进行访存。通过这种方式可以减少访存的延时。 #define THREAD_PER_BLOCK 256#define WARP_SIZE 32template unsigned int blockSize__device__ __forceinline__ float warpReduceSum(float sum) if (blockSize = 32)sum += __shfl_down_sync(0xffffffff, sum, 16); // 0-16, 1-17, 2-18, etc. if (blockSize = 16)sum += __shfl_down_sync(0xffffffff, sum, 8);// 0-8, 1-9, 2-10, etc. if (blockSize = 8)sum += __shfl_down_sync(0xffffffff, sum, 4);// 0-4, 1-5, 2-6, etc. if (blockSize = 4)sum += __shfl_down_sync(0xffffffff, sum, 2);// 0-2, 1-3, 4-6, 5-7, etc. if (blockSize = 2)sum += __shfl_down_sync(0xffffffff, sum, 1);// 0-1, 2-3, 4-5, etc. return sum;template unsigned int blockSize, int NUM_PER_THREAD__global__ void reduce7(float *d_in,float *d_out, unsigned int n) float sum = 0; // each thread loads one element from global to shared mem unsigned int tid = threadIdx.x; #pragma unroll //线程级局部规约 for(int iter=0; iterNUM_PER_THREAD; iter++) sum += d_in[i+iter*blockSize]; // Shared mem for partial sums (one per warp in the block) static __shared__ float warpLevelSums[WARP_SIZE]; const int laneId = threadIdx.x % WARP_SIZE; const int warpId = threadIdx.x / WARP_SIZE; sum = warpReduceSumblockSize(sum); if(laneId == 0 )warpLevelSums[warpId] = sum; __syncthreads(); // read from shared memory only if that warp existed sum = (threadIdx.x blockDim.x / WARP_SIZE) ? warpLevelSums[laneId] : 0; // Final reduce using first warp if (warpId == 0) sum = warpReduceSumblockSize/WARP_SIZE(sum); // write result for this block to global mem if (tid == 0) d_out[blockIdx.x] = sum;reduce7THREAD_PER_BLOCK, NUM_PER_THREADGrid,Block(d_a, d_out, N); 优化总结 使用共享内存 减少线程束分化（差异保留在线程束之间） 减少bank冲突；变换角标，错位（+1与跨度错位） 减少空闲线程，一个线程取两个元素 减少__syncthreads同步，一个SIMD内可以不使用 合理设置block数量 使用shuffle指令","tags":["CUDA.C++"],"categories":["CUDA"]},{"title":"CUDA算子优化-SoftMax","path":"/2025/09/06/CUDA 算子优化系列/CUDA算子优化-SoftMax/","content":"TongkaioCUDA_Kernel_Samples: CUDA 算子手撕与面试指南","tags":["CUDA.C++"],"categories":["CUDA"]},{"title":"GGML源码浅析(1) 基础数据结构、内存管理、后端管理","path":"/2025/08/01/GGML源码浅析/","content":"GGML 源码浅析（1）前言1.阅读路线 ​\t1.内存管理：不使用后端时（参见examplesimple-ctx）介绍ggml中的重要数据结构以及内存管理 ​\t2.后端的设计逻辑 3.基于gpt-2学习模型构建过程中权重与kv-cache管理 2.重点 ​\t1.关于ggml-context的相关数据结构，需要理解ggml-contxt中内存分配结构，可以参看其中给的几张图 ​\t2.通过输出结果反推构建计算图 ​\t3.后端设计中，一定用面向对象的方式来思考相关后端实现，其实就是用c实现了C++的面向对象，自己实现了虚函数表，私有成员变量（void*) ​\t4.一次操作的逻辑 大佬教程 深入理解GGML（一）模型和计算图 - 知乎 1.核心数据结构1.1ggml_contextstruct ggml_context size_t mem_size; void* mem_buffer; //cpu采用由poxsi_align分配的内存对齐内存，对于不同的后端，分配不同的内存 bool mem_buffer_owned;//外部分配还是属于本处，决定内存分配权？ bool no_alloc; bool no_alloc_save; // this is used to save the no_alloc state when using scratch buffers int n_objects; struct ggml_object * objects_begin;//ggml_object维护的是一个链表 struct ggml_object * objects_end; struct ggml_scratch scratch; struct ggml_scratch scratch_save;; struct ggml_object size_t offs; size_t size; struct ggml_object * next; enum ggml_object_type type; char padding[4]; ; ggml_context是最核心的数据结构，所有的张量、计算图都依赖于这个数据结构: mem_size表示ggml_init时分配一块多大的内存，后续的张量都从这块内存中分配空间，避免反复的malloc mem_buffer_owned表示这个mem_buffer是外部传进来的还是自己分配的，决定分配权，避免误释放 。 no_malloc是表示张量分配内存的时候是真的分配内存还是仅仅用于占位 no_alloc_save使用了scratch会强制开启内存，所以需要暂存一下no_malloc scratch是一块临时内存，用于存放中间的临时结果、缓存 scratch_save暂存scatch的状态 n_objects以及两个指针维护了一个对象双向链表，记录所有已创建的对象，可以看到这个ggml_object主要维护了偏移offs和size,容易理解实际数据就是分配在ggml_context的buffer中，而要找到这些数据就通过object链表来进行查找与分配。 ggml_object这个结构体本本身也是分配在ggml_context.mem_buffer上的，之后跟着对应的数据。 ggml-object-ggml_tensor-tensor_data 这一部分的实现具体查看ggml_new_object static struct ggml_object * ggml_new_object(struct ggml_context * ctx, enum ggml_object_type type, size_t size) // always insert objects at the end of the contexts memory pool struct ggml_object * obj_cur = ctx-objects_end; const size_t cur_offs = obj_cur == NULL ? 0 : obj_cur-offs; const size_t cur_size = obj_cur == NULL ? 0 : obj_cur-size; const size_t cur_end = cur_offs + cur_size; // align to GGML_MEM_ALIGN size_t size_needed = GGML_PAD(size, GGML_MEM_ALIGN); char * const mem_buffer = ctx-mem_buffer; struct ggml_object * const obj_new = (struct ggml_object *)(mem_buffer + cur_end); if (cur_end + size_needed + GGML_OBJECT_SIZE ctx-mem_size) GGML_PRINT(%s: not enough space in the contexts memory pool (needed %zu, available %zu) , __func__, cur_end + size_needed, ctx-mem_size); assert(false); return NULL; *obj_new = (struct ggml_object) .offs = cur_end + GGML_OBJECT_SIZE, .size = size_needed, .next = NULL, .type = type, ; GGML_ASSERT_ALIGNED(mem_buffer + obj_new-offs); if (obj_cur != NULL) obj_cur-next = obj_new; else // this is the first object in this context ctx-objects_begin = obj_new; ctx-objects_end = obj_new; //printf(%s: inserted new object at %zu, size = %zu , __func__, cur_end, obj_new-size); return obj_new; 实际使用过程中，可以看到所有的操作都挂靠到ggml_context上 内存分配： struct ggml_init_params params = .mem_size = 64 * 1024 * 1024, .mem_buffer = malloc(mem_size), .no_alloc = false;struct ggml_context * ctx = ggml_init(params); 张量分配： struct ggml_tensor * a = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 10); ​\t可以查看ggml.c/ggml_new_tensor_impl()具体实现，最终分配的tensor的data,指针指向的数据还是ggml_contxt.mem_buf中的对应偏移量。 计算操作： struct ggml_tensor * c = ggml_add(ctx, a, b); 执行计算图： struct ggml_cgraph graph = ggml_build_forward(c);ggml_graph_compute_with_ctx(ctx, graph, n_threads); 下边小红书博主**TransormerX**绘制的这几张图 清晰 美观的展示了内存分配情况： .fqfbycwpbnru{zoom:80%;} .dqvigblpekab{zoom:67%;} .bwqcknrglpxo{zoom:50%;} 除了图中绘制的tensor是按照这样的内存分布以外，其他类型graph、workbuffer等类型的object都是采用这样的内存排布，即：object结构体-对应类型结构体（tensor\\graph)-相应data 比如图的存储就是object-ggml_cgraph(结构体本身)-节点叶子等指针数组 WORK_BUFFER就是object–work_data （直接被cplan中的指针所指） 1.2ggml_state整个ggml程序运行过程中有一个全局的g_state. static struct ggml_state g_state;#define GGML_MAX_CONTEXTS 64struct ggml_state struct ggml_context_container contexts[GGML_MAX_CONTEXTS]; struct ggml_numa_nodes numa;;struct ggml_context_container bool used; struct ggml_context context;;struct ggml_numa_nodes enum ggml_numa_strategy numa_strategy; struct ggml_numa_node nodes[GGML_NUMA_MAX_NODES]; uint32_t n_nodes; uint32_t total_cpus; // hardware threads on system uint32_t current_node; // node on which main process is execting#if defined(__gnu_linux__) cpu_set_t cpuset; // cpuset from numactl#else uint32_t cpuset; // no NUMA support outside of Linux at this time. Use a portable datatype#endif; ggml_state中包含了一个ggml_context（额外加一个used标识而已）数组。 在第一次调用ggml_init时，会对g_state进行初始化。 //位于ggml.c/ggml_initif (is_first_call) //.....初始化激活函数表 //初始化g_state g_state = (struct ggml_state) /*.contexts =*/ 0 , /*.numa =*/ .n_nodes = 0, .total_cpus = 0, , ; for (int i = 0; i GGML_MAX_CONTEXTS; ++i) g_state.contexts[i].used = false; 于是可以理解到，程序运行时，会在全局静态内存区分配一个g_state,包含了一个g_context数组，每次ggml_init的时候就去g_state中找一个没有使用的g_context,获得其指针后进行每个字段的初始化以及填充，之后就依靠这个ggml_context进行一次一次。 目前个人看代码觉得每次模型执行只需要一个ggml_conterxt，但是给了一个64个ggml_context组成的数组，是因为可能需要支持模型的并行（这个观点参考GPT） 1.3ggml_tensor// n-dimensional tensor struct ggml_tensor enum ggml_type type; //数据类型 GGML_DEPRECATED(enum ggml_backend_type backend, use the buffer type to find the storage location of the tensor); struct ggml_backend_buffer * buffer;//一个表示数据实际存储的内存后端，多态实现，一个重要的结构体 int64_t ne[GGML_MAX_DIMS]; // number of elements//每一维的元素个数 size_t nb[GGML_MAX_DIMS]; // stride in bytes: //每一位走到下一个元素的字节数 // nb[0] = ggml_type_size(type) // nb[1] = nb[0] * (ne[0] / ggml_blck_size(type)) + padding // nb[i] = nb[i-1] * ne[i-1] // compute data enum ggml_op op;//这个tensor是通过什么操作计算得到的 // op params - allocated as int32_t for alignment int32_t op_params[GGML_MAX_OP_PARAMS / sizeof(int32_t)]; //存储当前op的参数op = CONV2D 时，可能存储 kernel size、stride int32_t flags; struct ggml_tensor * grad; //如果当前张量支持反向传播，则存储梯度张量 struct ggml_tensor * src[GGML_MAX_SRC]; //当前张量是某个张量的运算结果，则记录输入张量 // source tensor and offset for views struct ggml_tensor * view_src; size_t view_offs; void * data;//实际张量的数据指针，通常指向ggml_context char name[GGML_MAX_NAME]; void * extra; // extra things e.g. for ggml-cuda.cu // char padding[4]; ; 这里有一个比较重要的概念是view,view不会信分配内存。一个简单的例子：如果一个短向量是一个长向量的子向量，可以理解为短向量是长向量的子向量，也就是长向量的视图，分配短向量时，可以利用长向量已经分配的内存，从而避免了内存的分配。 1.4 ggml_cgraphstruct ggml_cgraph int size; int n_nodes;//节点 int n_leafs;//叶子节点数 struct ggml_tensor ** nodes;//中间节点数 struct ggml_tensor ** grads; struct ggml_tensor ** leafs;//叶子节点 struct ggml_hash_set visited_hash_set;//反向构建中 hash去重 enum ggml_cgraph_eval_order order; ; 2.核心操作2.1模型加载gguf结构与解析gguf定义了模型的权重保存方式，以下为gguf的结构 主要可以分为以下4部分： 1.header 包含模式，tensor数量、kv元数据数、版本等 2.模型元数据（KV表示） 3.每个tensor的信息（offset等，不包含tensor的值） 4.tensor的值 文件ggml.c中，定义了gguf相关的结构体，其中gguf_context对应于一个文件，包含header、kv、tensor_info等 union gguf_value uint8_t uint8; int8_t int8; uint16_t uint16; int16_t int16; uint32_t uint32; int32_t int32; float float32; uint64_t uint64; int64_t int64; double float64; bool bool_; struct gguf_str str; struct enum gguf_type type; uint64_t n; // GGUFv2 void * data; arr;;struct gguf_kv struct gguf_str key; enum gguf_type type; union gguf_value value;;struct gguf_header char magic[4]; uint32_t version; uint64_t n_tensors; // GGUFv2 uint64_t n_kv; // GGUFv2;struct gguf_tensor_info struct gguf_str name; uint32_t n_dims; uint64_t ne[GGML_MAX_DIMS]; enum ggml_type type; uint64_t offset; // offset from start of `data`, must be a multiple of `ALIGNMENT` // for writing API const void * data; size_t size;;struct gguf_context struct gguf_header header; struct gguf_kv * kv; struct gguf_tensor_info * infos; size_t alignment; size_t offset; // offset of `data` from beginning of file size_t size; // size of `data` in bytes //uint8_t * padding; void * data;; ggml.c/gguf_init_from_file实现了从文件加载模型到结构体中 2.2计算图（前向图）构建以simple-ctx为例说明计算图构建流程 struct ggml_cgraph * build_graph(const simple_model model) struct ggml_cgraph * gf = ggml_new_graph(model.ctx); // result = a*b^T struct ggml_tensor * result = ggml_mul_mat(model.ctx, model.a, model.b); ggml_build_forward_expand(gf, result); return gf;struct ggml_tensor * ggml_mul_mat( struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b) GGML_ASSERT(ggml_can_mul_mat(a, b)); GGML_ASSERT(!ggml_is_transposed(a)); bool is_node = false; if (a-grad || b-grad) is_node = true; const int64_t ne[4] = a-ne[1], b-ne[1], b-ne[2], b-ne[3] ; struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, 4, ne); result-op = GGML_OP_MUL_MAT; result-grad = is_node ? ggml_dup_tensor(ctx, result) : NULL; result-src[0] = a; result-src[1] = b; return result; ggml_new_graph会在ggml-contxt中相关的内存区域分配ggml_cgraph的内存，进行初始化； 而后进行实际的算子构建，这个过程每一步得到的输出张量会记录对应的输入、以及操作类型，参照上边的ggml_tensor的数据结构 最后核心的函数是ggml_build_forward_expand，这个过程根据输出的tensor，反向添加graph中的计算节点。 即流程为手动设置最终输出tensor的过程，利用tensor中的变量反向构建静态输出图，成功在graph中添加不同的节点，完成graph的维护 具体构建过程见：ggml.c/ggml_visit_parents简言之就是后序遍历+hash去重，填充cgraph中的相关指针 static void ggml_visit_parents(struct ggml_cgraph * cgraph, struct ggml_tensor * node) if (node-grad == NULL) // this usually happens when we generate intermediate nodes from constants in the backward pass // it can also happen during forward pass, if the user performs computations with constants if (node-op != GGML_OP_NONE) //GGML_PRINT_DEBUG(%s: warning: node %p has no grad, but op %d , __func__, (void *) node, node-op); // check if already visited if (ggml_hash_insert(cgraph-visited_hash_set, node) == GGML_HASHSET_ALREADY_EXISTS) return; for (int i = 0; i GGML_MAX_SRC; ++i) const int k = (cgraph-order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i : (cgraph-order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) : /* unknown order, just fall back to using i*/ i; if (node-src[k]) ggml_visit_parents(cgraph, node-src[k]); if (node-op == GGML_OP_NONE node-grad == NULL) // reached a leaf node, not part of the gradient graph (e.g. a constant) GGML_ASSERT(cgraph-n_leafs cgraph-size); if (strlen(node-name) == 0) ggml_format_name(node, leaf_%d, cgraph-n_leafs); cgraph-leafs[cgraph-n_leafs] = node; cgraph-n_leafs++; else GGML_ASSERT(cgraph-n_nodes cgraph-size); if (strlen(node-name) == 0) ggml_format_name(node, node_%d, cgraph-n_nodes); cgraph-nodes[cgraph-n_nodes] = node; if (cgraph-grads) cgraph-grads[cgraph-n_nodes] = node-grad; cgraph-n_nodes++; 后向图的构建过程更加复杂一些，会同时保存相关的梯度信息。 后向图的构建依赖前向图，后向图靠拷贝了前向计算图， 2.3计算核心函数 enum ggml_status ggml_graph_compute_with_ctx(struct ggml_context * ctx, struct ggml_cgraph * cgraph, int n_threads) struct ggml_cplan cplan = ggml_graph_plan(cgraph, n_threads, NULL); struct ggml_object * obj = ggml_new_object(ctx, GGML_OBJECT_TYPE_WORK_BUFFER, cplan.work_size); cplan.work_data = (uint8_t *)ctx-mem_buffer + obj-offs; return ggml_graph_compute(cgraph, cplan); 主要步骤为， 1.构建执行计划ggml_graph_plan,生成执行顺序和任务计划、所有算子中的预计分配内存大小、线程数量。线程分配数量由ggml_get_n_tasks中的一个switch case决定。其中有一段work_size += CACHE_LINE_SIZE * n_threads;的代码是避免多个线程访问同一个缓存和，造成伪共享。一些可以被多线程执行的算子计算预计内存分配的时候还会乘以线程数，保证不会导致额外的内存同步开销。 2.ggml_new_object生成任务缓冲区,ggml_context中 。 3.执行计算图ggml_graph_compute ggml_graph_compute 执行ggml_graph_compute时，首先初始化或者设置ggml_threadpool中的相关参数，包括cplan等 只有调用ggml_graph_compute_thread进行多线程处理，启用omp进行多线程处理，否则单线程 其中遍历cgraph中的每个算子节点，通过ggml_compute_forward转发到不同的算子进行计算，每个算子内部区分不同的精读，转发到不同的精度处理函数，最后不同的精度计算算子函数 通过ith th考虑线程分配，内存存取 将线程划分计算的权利交给了算子自身来执行。 例如以f16_add为例,通过ith划分举证的行和列，每个线程计算不同的小矩阵，实现并行化。 static void ggml_compute_forward_add_f16_f16( const struct ggml_compute_params * params, struct ggml_tensor * dst) const struct ggml_tensor * src0 = dst-src[0]; const struct ggml_tensor * src1 = dst-src[1]; GGML_ASSERT(ggml_are_same_shape(src0, src1) ggml_are_same_shape(src0, dst)); const int ith = params-ith; const int nth = params-nth; const int nr = ggml_nrows(src0); GGML_TENSOR_BINARY_OP_LOCALS GGML_ASSERT(src0-type == GGML_TYPE_F16); GGML_ASSERT(src1-type == GGML_TYPE_F16); GGML_ASSERT(dst-type == GGML_TYPE_F16); GGML_ASSERT( nb0 == sizeof(ggml_fp16_t)); GGML_ASSERT(nb00 == sizeof(ggml_fp16_t)); // rows per thread const int dr = (nr + nth - 1)/nth; // row range for this thread const int ir0 = dr*ith; const int ir1 = MIN(ir0 + dr, nr); if (nb10 == sizeof(ggml_fp16_t)) for (int ir = ir0; ir ir1; ++ir) // src0, src1 and dst are same shape = same indices const int i3 = ir/(ne2*ne1); const int i2 = (ir - i3*ne2*ne1)/ne1; const int i1 = (ir - i3*ne2*ne1 - i2*ne1); ggml_fp16_t * dst_ptr = (ggml_fp16_t *) ((char *) dst-data + i3*nb3 + i2*nb2 + i1*nb1); ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0-data + i3*nb03 + i2*nb02 + i1*nb01); ggml_fp16_t * src1_ptr = (ggml_fp16_t *) ((char *) src1-data + i3*nb13 + i2*nb12 + i1*nb11); for (int i = 0; i ne0; i++) dst_ptr[i] = GGML_FP32_TO_FP16(GGML_FP16_TO_FP32(src0_ptr[i]) + GGML_FP16_TO_FP32(src1_ptr[i])); else // src1 is not contiguous GGML_ABORT(fatal error); 3.后端实现快速入门技巧： 用面向对象思想来阅读！ 3.1相关数据结构ggml_backend查看examplesimple代码时，后端的模型中增加了一个ggml_backend结构。 ggml_backend可以理解为一个设备管理器 struct ggml_backend ggml_guid_t guid;//用于唯一标识 backend 实例，便于注册、查找或调试。 struct ggml_backend_i iface;//定义后端行为的函数虚表，包含后端行为的函数指针 ggml_backend_context_t context;//void *，具体内容依赖后端实现; ggml_backend_bufferstruct ggml_backend_buffer struct ggml_backend_buffer_i iface; ggml_backend_buffer_type_t buft; ggml_backend_buffer_context_t context; size_t size; enum ggml_backend_buffer_usage usage; ; 又包含了ggml_backend_buffer_type struct ggml_backend_buffer_type struct ggml_backend_buffer_type_i iface; ggml_backend_buffer_type_context_t context; ; 这里结构体比较多，看起来比较复杂，用这篇博客中的图片描述是这样的 .gqofjebghtjg{zoom: 80%;} 但是如果用面向对象的思想来理解，一切都很简单。 最开始的初衷是有一个后端基类，不同类型的后端基于此派生，而后端包含有不同的buffer，因此创建了后端buffer一组基类，并作为后端基类的成员变量，相同的方式有了后端buffer-type类 不同的后端有不同的成员变量，也就是ctx，通过void*实现 3.2CPU后端示例讲解load model加载模型之前先根据不同的宏启用不同的后端初始化函数， 之后创建对应的ggml_contxt， 创建向量时，首先会将向量存储在CPU， 之后通过ggml_backend_alloc_ctx_tensors 创建对应的后端buffer,然后通过ggml_backend_tensor_set将向量从CPU内存搬到后端内存 void load_model(simple_model model, float * a, float * b, int rows_A, int cols_A, int rows_B, int cols_B) // initialize the backend#ifdef GGML_USE_CUDA fprintf(stderr, %s: using CUDA backend , __func__); model.backend = ggml_backend_cuda_init(0); // init device 0 if (!model.backend) fprintf(stderr, %s: ggml_backend_cuda_init() failed , __func__); #endif#ifdef GGML_USE_METAL fprintf(stderr, %s: using Metal backend , __func__); ggml_backend_metal_log_set_callback(ggml_log_callback_default, nullptr); model.backend = ggml_backend_metal_init(); if (!model.backend) fprintf(stderr, %s: ggml_backend_metal_init() failed , __func__); #endif // if there arent GPU Backends fallback to CPU backend if (!model.backend) model.backend = ggml_backend_cpu_init(); int num_tensors = 2; struct ggml_init_params params /*.mem_size =*/ ggml_tensor_overhead() * num_tensors, /*.mem_buffer =*/ NULL, /*.no_alloc =*/ true, ; // create context model.ctx = ggml_init(params); // create tensors model.a = ggml_new_tensor_2d(model.ctx, GGML_TYPE_F32, cols_A, rows_A); model.b = ggml_new_tensor_2d(model.ctx, GGML_TYPE_F32, cols_B, rows_B); // create a backend buffer (backend memory) and alloc the tensors from the context model.buffer = ggml_backend_alloc_ctx_tensors(model.ctx, model.backend); // load data from cpu memory to backend buffer ggml_backend_tensor_set(model.a, a, 0, ggml_nbytes(model.a)); ggml_backend_tensor_set(model.b, b, 0, ggml_nbytes(model.b)); ggml_backend_cpu_init ggml_backend_cpu_init XXX_backend_cpu_init负责创建对应的ggml_backend_t结构体 可以看到结构体中的ctx是每个后端有一个自己的结构体 虚函数表interface是全局定义了每个后端的表，如cpu_backend_i ggml_backend_t ggml_backend_cpu_init(void) struct ggml_backend_cpu_context * ctx = malloc(sizeof(struct ggml_backend_cpu_context)); if (ctx == NULL) return NULL; ctx-n_threads = GGML_DEFAULT_N_THREADS; ctx-threadpool = NULL; ctx-work_data = NULL; ctx-work_size = 0; ctx-abort_callback = NULL; ctx-abort_callback_data = NULL; ggml_backend_t cpu_backend = malloc(sizeof(struct ggml_backend)); if (cpu_backend == NULL) free(ctx); return NULL; *cpu_backend = (struct ggml_backend) /* .guid = */ ggml_backend_cpu_guid(), /* .interface = */ cpu_backend_i, /* .context = */ ctx ; return cpu_backend;static struct ggml_backend_i cpu_backend_i = /* .get_name = */ ggml_backend_cpu_name, /* .free = */ ggml_backend_cpu_free, /* .get_default_buffer_type = */ ggml_backend_cpu_get_default_buffer_type, /* .set_tensor_async = */ NULL, /* .get_tensor_async = */ NULL, /* .cpy_tensor_async = */ NULL, /* .synchronize = */ NULL, /* .graph_plan_create = */ ggml_backend_cpu_graph_plan_create, /* .graph_plan_free = */ ggml_backend_cpu_graph_plan_free, /* .graph_plan_update = */ NULL, /* .graph_plan_compute = */ ggml_backend_cpu_graph_plan_compute, /* .graph_compute = */ ggml_backend_cpu_graph_compute, /* .supports_op = */ ggml_backend_cpu_supports_op, /* .supports_buft = */ ggml_backend_cpu_supports_buft, /* .offload_op = */ NULL, /* .event_new = */ NULL, /* .event_free = */ NULL, /* .event_record = */ NULL, /* .event_wait = */ NULL, /* .event_synchronize = */ NULL,; ggml_backend_alloc_ctx_tensors 核心代码alloc_tensor_range本质就是构造函数 ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors(struct ggml_context * ctx, ggml_backend_t backend) return ggml_backend_alloc_ctx_tensors_from_buft(ctx, ggml_backend_get_default_buffer_type(backend));ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_context * ctx, ggml_backend_buffer_type_t buft) GGML_ASSERT(ggml_get_no_alloc(ctx) == true); size_t alignment = ggml_backend_buft_get_alignment(buft); size_t max_size = ggml_backend_buft_get_max_size(buft); ggml_backend_buffer_t * buffers = NULL; size_t n_buffers = 0; size_t cur_buf_size = 0; struct ggml_tensor * first = ggml_get_first_tensor(ctx); for (struct ggml_tensor * t = first; t != NULL; t = ggml_get_next_tensor(ctx, t)) size_t this_size = 0; if (t-data == NULL t-view_src == NULL) this_size = GGML_PAD(ggml_backend_buft_get_alloc_size(buft, t), alignment); if (this_size max_size) fprintf(stderr, %s: tensor %s is too large to fit in a %s buffer (tensor size: %zu, max buffer size: %zu) , __func__, t-name, ggml_backend_buft_name(buft), this_size, max_size); for (size_t i = 0; i n_buffers; i++) ggml_backend_buffer_free(buffers[i]); free(buffers); return NULL; if ((cur_buf_size + this_size) max_size) // allocate tensors in the current buffer if (!alloc_tensor_range(ctx, first, t, buft, cur_buf_size, buffers, n_buffers)) return NULL; first = t; cur_buf_size = this_size; else cur_buf_size += this_size; // allocate remaining tensors if (cur_buf_size 0) if (!alloc_tensor_range(ctx, first, NULL, buft, cur_buf_size, buffers, n_buffers)) return NULL; if (n_buffers == 0) #ifndef NDEBUG fprintf(stderr, %s: all tensors in the context are already allocated , __func__);#endif return NULL; ggml_backend_buffer_t buffer; if (n_buffers == 1) buffer = buffers[0]; else buffer = ggml_backend_multi_buffer_alloc_buffer(buffers, n_buffers); free(buffers); return buffer; 总结1.ggml中分明很多地方都是面向对象的思想写的，但是为什么会用C语言写呢？ 更加稳定的ABI，不和编译器强相关 零依赖，轻量化 没有运行时，手动控制内存更方便","tags":["LLM/ggml"],"categories":["源码解析/大模型"]},{"title":"几种程序接口重定向、插桩方式比较","path":"/2025/07/29/几种程序接口重定向、插桩方式比较/","content":"最近在实验中需要分析程序中的堆变量内存分配情况，一开始自己的实现是采用llvm IR Pass修改的方式，后来在OSDI的论文中发现相关的方法采用的是LD_PRELOAD的方式实现，一开始认为这种方法会更加简单，于是进行了实现，结果发现各有特点。 1. LLVM IR 方式2. LD_PRELOAD方式代码 LD_PRELOAD的全进程级别：LD_PRELOAD是基于全进程级别的动态链接符号重定向。 启动加载时加载指定的.so文件，然后后续所有的调用都会使用so文件中提供的实现，这包括引用程序代码、第三方库、libc 比如：重定向了一个malloc，记录程序中的malloc地址然后使用LD_PRELOAD的方式进行重定向。 #include stdio.h#include stdlib.hint main() void* p = malloc(64); // printf(p: %p , p); free(p); return 0; 理论以上代码值调用一个malloc,但是拦截重定向以后发现有3个malloc,不注释代码中的printf函数，还会多出一个malloc。 这样的全进程级别特性可以采集到lib c本身的特性，但是也可能会对程序分析不必要的麻烦，比如我本身只想分析应用程序级别的事务，实现程序代码中的过滤可以通过调用栈过滤等方式实现。","tags":["LD_PRELOAD","LLVM","PIN"]},{"title":"论文中常见内存性能分析workloads","path":"/2025/07/27/论文中常见内存性能分析workloads/","content":"一、分类说明整理所读到论文中经常使用的内存分析工作负载。 按照特点可以分为延迟敏感型、带宽密集型；按照作用可以分为AI、HPC、Database等 博客用于内存性能评估的workload中整理了常见的workloads,但是主要还是重在基本介绍，没有对其访存特征等镜像介绍。本文借鉴这篇博客，自己分析、运行相关workloads。 二、LLM inference1.llama.cpp轻量化的大模型推理框架、适用于嵌入型系统、边缘节点上进行大模型推理。 访存特征： 模型权重加载阶段：采用大块连续内存、或者可选mmap()映射模型参数。具有较好的空间局部性、较差的时间局部性(一次加载一次使用) 推理阶段：主要是KV-Cache需要大量内存访问，读写频繁、更新频繁、各个层中存在一些张量操作也需要访存。高读密集型 内存需求： ​\t同其他大模型推理需求相似，内存需求量主要来自模型权重与KV-cache。模型权重内存使用量与精读有关、KV-Cache与依赖层数有关。一个qwen-7b在llama.cpp的内存占用量约为14~16B 其他： ​\tllama.cpp的内存管理采用统一预分配内存池，使用offset二次分配与访问，最后集中释放的方式进行内存管理。其ggml内存中有对内存的一次性malloc（ggml_init）ggml_new_tensor、ggml_new_tensor_1d进行内存内存二次分配、ggml_free进行内存释放。Arena 分配器（“批发内存，零售指针，整单清场”）","tags":["内存性能分析workloads"],"categories":["内存性能分析workloads"]},{"title":"Qt源码阅读与设计模式","path":"/2025/07/18/Qt源码阅读与设计模式/","content":"todo","tags":["Qt","设计模式"],"categories":["Qt"]},{"path":"/about/index.html","content":"🧑‍💻 About me Hello, welcome to my blog. I’m Zane Jiang. I graduated with a bachelor’s degree in Computer Science from Nanjing Normal University(NNU), and I am currently pursuing a master’s degree at the College of Computer Science, Chongqing University(CQU). I have worked as a C++ software development engineer for one year in a company specializing in LED control systems. My current areas of interest include OS,LLVM, CXL, Qt, and more. This blog is used to document some of my reflections on life, work, and study notes. 📫 Contact Email: 2129056867@qq.com GitHub"},{"title":"C++11新特性","path":"/notebooks/Interview/C++11新特性.html","content":"很好的面试资料： 整理了一年的Linux C++武林秘籍，你早晚用得到（C++进阶必看） - 知乎 转载并简单整理自知乎 程序喵大人 1.左值引用与完美转发等基本概念: 左值、右值 ：放在等号左边，取地址并且有名字 的就叫左值，反之叫右值 int a = b + c ;int a = 4; 纯右值:用于初始化一个对象。 运算表达式产生的临时变量、不和对象关联的原始字面量、返回非引用的函数调用、lambda表达式、this指针 将亡值：代表一个“即将结束生命周期”的对象， int main() std::string str = Hello World; // str 是一个左值 // std::move(str) 将左值 str 转换为一个将亡值 // 它在告诉编译器：“str 即将消亡，请移动它而不是拷贝它” std::string new_str = std::move(str); // 此时，str 的资源（动态分配的字符数组）被“移动”到了 new_str 中 // str 本身仍然存在（是一个合法的对象），但处于“有效但未指定状态” // 通常是一个空字符串，但你不能依赖这一点，只能对它进行重新赋值或销毁 std::cout str after move: \\ str \\ std::endl; // 可能是 std::cout new_str: \\ new_str \\ std::endl; // Hello World return 0;//std::move(str) 就是一个将亡值。它不是一个新创建的值（不是纯右值），而是即将失效的现有对象 str 的另一种表现形式//主要产生方法：//1. std::move(a);A a;auto c = std::move(a); // std::move(a) 是一个将亡值//2.static_castA(a)A a;auto d = static_castA(a); // static_castA(a) 是一个将亡值 左值、右值引用: 右值引用。是一种特殊的引用类型，主要设计用来绑定到右值（特别是将亡值），并表明所引用的对象资源可以被安全地”移动”或”窃取”。 int a=5;int b=a;//b是左值引用b=4;int c = 10;//error，10无法取地址，无法进行引用const int d = 10;//ok，因为是常引用，引用常量数字，这个常量数字会存储在内存中，可以取地址int a=4;int b=a;//error,a是左值int c=std::move(a);//ok 移动语义浅拷贝容易导致资源重复释放等问题，深拷贝会导致额外的开销。 因此引入了移动语义，也就是实现了所有权管理，偷掉一些将亡值的资源 C++中所有的STL都实现了移动语义，方便使用。 实现移动语义的关键组件 1.右值引用（T) 2.移动构造函数 class MyString private: char* m_data; size_t m_size; public: // 移动构造函数 MyString(MyString other) noexcept : m_data(other.m_data), m_size(other.m_size) // 偷资源 // 将源对象置于有效但空的状态 other.m_data = nullptr; other.m_size = 0; ; 3.移动赋值运算符 class MyString public: // 移动赋值运算符 MyString operator=(MyString other) noexcept if (this != other) delete[] m_data; // 释放当前资源 // 偷资源 m_data = other.m_data; m_size = other.m_size; // 置空源对象 other.m_data = nullptr; other.m_size = 0; return *this; ;//赋值和构造的区别：赋值是替换，构造是创建加赋值 编译器优化：RVONRVO现代编译器会进行返回值优化（RVO）和命名返回值优化（NRVO），有时甚至比移动语义更高效 返回值优化的时机： return的值类型与函数的返回值类型相同 return的是一个局部对象 MyString create_string() return MyString(Hello); // RVO：直接在目标位置构造，无拷贝无移动MyString create_string_nrvo() MyString result(Hello); return result; // NRVO：直接在目标位置构造result 完美转发完美转发指的是在函数模板中，将参数以原始的值类别（左值或右值）和constvolatile限定符完全不变地转发给另一个函数。 简单来说：保持参数的所有特性不变地传递下去。 为什么需要完美转发？在没有完美转发之前，泛型编程中会遇到参数类别丢失的问题： void wrapper(Foo arg) callee(arg); void wrapper(const Foo arg) callee(arg); void wrapper(Foo arg) callee(std::move(arg)); 使用模板可以简化 templatetypename Tvoid wrapper(T arg) callee(arg); 但问题是： 如果传的是右值，T 会推导成 非引用类型，导致右值变成左值。 constvolatile 属性也可能丢失。 万能引用与引用折叠 templatetypename Tvoid wrapper(T arg); 这里的T 当传入右值时：T 推导为 U，于是参数类型为 U → 右值引用； 当传入左值时：T 推导为 U，于是参数类型为 U → 引用折叠 → U。 引用折叠规则： → → → → 所以 T 在模板中既能绑定左值，也能绑定右值。 std::forward在 wrapper 中直接写： callee(arg); 会把所有参数当成左值传递，右值的性质丢失。 需要用 std::forwardT(arg) 来“条件转发”： 如果 T 推导为 U，则 std::forwardT(arg) 返回 U。 如果 T 推导为 U，则 std::forwardT(arg) 返回 U。 这样右值保持右值，左值保持左值，实现完美转发。 典型写法#include iostream#include utilityvoid callee(int x) std::cout callee(int) ;void callee(const int x) std::cout callee(const int) ;void callee(int x) std::cout callee(int) ;templatetypename Tvoid wrapper(T arg) callee(std::forwardT(arg)); // 完美转发int main() int a = 42; const int b = 100;\t//思考输出？ wrapper(a); // callee(int) wrapper(b); // callee(const int) wrapper(10); // callee(int) 典型应用场景1.壳函数 例如日志、装饰器模式、函数调用计时器 templatetypename F, typename... Argsvoid log_and_call(F f, Args... args) std::cout Calling function... ; std::forwardF(f)(std::forwardArgs(args)...); 2.容器构造器 //vector::emplace_back, map::emplace templatetypename... Argsvoid emplace_back(Args... args) new (storage[size++]) T(std::forwardArgs(args)...); // push_back：需要显式移动 names.push_back(std::move(temp)); // 移动语义 // emplace_back：完美转发参数，直接构造 names.emplace_back(David); // 直接在vector中构造，更高效！ 2.智能指针C++11 标准库在 memory 头文件中提供了三种主要智能指针： std::unique_ptr std::shared_ptr std::weak_ptr unique_ptrstd::unique_ptr是一个独占型的智能指针，它不允许其它智能指针共享其内部指针，也不允许unique_ptr的拷贝和赋值。 #include iostream#include memorystruct Foo Foo() std::cout Foo ctor ; ~Foo() std::cout Foo dtor ; ;int main() std::unique_ptrFoo p1(new Foo()); // 独占所有权 // std::unique_ptrFoo p2 = p1; // 错误，不能拷贝 std::unique_ptrFoo p3 = std::move(p1); // 可以转移所有权 if (!p1) std::cout p1 is empty ; 其内部就是对指针包了一层、然后禁用其拷贝构造、赋值函数： templatetypename T, typename Deleter = std::default_deleteTclass unique_ptr private: T* ptr; // 原始指针 Deleter del; // 删除器，默认调用 deletepublic: explicit unique_ptr(T* p = nullptr) : ptr(p) ~unique_ptr() if (ptr) del(ptr); // 禁止拷贝 unique_ptr(const unique_ptr) = delete; unique_ptr operator=(const unique_ptr) = delete; // 支持移动 unique_ptr(unique_ptr other) noexcept : ptr(other.ptr) other.ptr = nullptr; unique_ptr operator=(unique_ptr other) noexcept if (this != other) reset(); ptr = other.ptr; other.ptr = nullptr; return *this; T* get() const return ptr; T operator*() const return *ptr; T* operator-() const return ptr; void reset(T* p = nullptr) if (ptr) del(ptr); ptr = p; ; shared_ptr shared_ptr 使用了引用计数，每一个shared_ptr的拷贝都指向相同的内存，每次拷贝都会触发引用计数+1， 每次生命周期结束析构的时候引用计数-1，在最后一个shared_ptr析构的时候，内存才会释放。 注意点： 可以自定义删除器，在引用计数为0的时候自动调用删除器来释放对象的内存： std::shared_ptrptr(newint,[](int*p)deletep;);std::shared_ptrptr(newint,[](int*p)deletep;); 不要用一个裸指针初始化多个shared_ptr，会出现double_free导致程序崩溃 通过shared_from_this()返回this指针，不要把this指针作为shared_ptr返回出来，因为this指针本质就 是裸指针，通过this返回可能会导致重复析构，不能把this指针交给智能指针管理。 classA shared_ptrAGetSelf() returnshared_from_this(); //returnshared_ptrA(this);错误，会导致doublefree ; 尽量使用make_shared，少用new。 不要delete get()返回来的裸指针。 要避免循环引用，循环引用导致内存永远不会被释放，造成内存泄漏。 #include iostream#include memorystruct Node std::shared_ptrNode next; std::shared_ptrNode prev; ~Node() std::cout Node destroyed ; ;int main() auto n1 = std::make_sharedNode(); auto n2 = std::make_sharedNode(); n1-next = n2; n2-prev = n1; std::cout main end ; main函数解释时，栈上的 n1 被销毁 - n1对象 计数 -1 1 栈上的 n2 被销毁 - n2对象 计数 -1 1 两者的引用计数都不为0 ，可能导致内存泄漏 实现： templatetypename Tclass shared_ptr private: T* ptr; // 指向资源 ControlBlock* ctrl; // 控制块（包含引用计数） struct ControlBlock size_t strong_count; // 强引用计数（shared_ptr 数量） size_t weak_count; // 弱引用计数（weak_ptr 数量） ControlBlock() : strong_count(1), weak_count(0) ;public: explicit shared_ptr(T* p = nullptr) ptr = p; if (p) ctrl = new ControlBlock(); else ctrl = nullptr; ~shared_ptr() release(); void release() if (ctrl) if (--ctrl-strong_count == 0) delete ptr; // 最后一个强引用销毁资源 if (ctrl-weak_count == 0) delete ctrl; // 控制块也销毁 // 拷贝构造：增加 strong_count shared_ptr(const shared_ptr other) ptr = other.ptr; ctrl = other.ctrl; if (ctrl) ctrl-strong_count++; // 移动构造：转移所有权 shared_ptr(shared_ptr other) noexcept ptr = other.ptr; ctrl = other.ctrl; other.ptr = nullptr; other.ctrl = nullptr; // 计数接口 size_t use_count() const return ctrl ? ctrl-strong_count : 0; ; weak_ptr解决循环引用可能出现内存泄漏的问题 弱引用不参与资源的管理 #include iostream#include memorystruct Node std::shared_ptrNode next; std::weak_ptrNode prev; // 改成 weak_ptr ~Node() std::cout Node destroyed ; ;int main() auto n1 = std::make_sharedNode(); auto n2 = std::make_sharedNode(); n1-next = n2; n2-prev = n1; // weak_ptr，不增加 strong_count std::cout main end ; 在使用 weak_ptr 访问对象时，必须检查其有效性。 3.函数式编程 std::fuction与lambdalambda表达式，提供匿名函数的方式，快速定义和使用函数对象 要注意悬空引用 std::functionvoid() createCallback() int local_var = 42; // 危险！捕获了局部变量 local_var 的引用 return []() std::cout local_var; ; // local_var 在这里被销毁int main() auto cb = createCallback(); cb(); // 未定义行为！打印的是已销毁栈上的垃圾值。 for(int id = enCoef9_Rr ; id = enCoef9_Bb ;id++) connect(m_spinBoxs[id],QOverloaddouble::of(QDoubleSpinBox::valueChanged),this,[](double value) OnCoefMatrixSlot(id,value);//id 恒等于 0,未定义行为 ); 默认情况下，以值方式 [=] 捕获的变量在 Lambda 体中是 const 的。如果你想修改它们，必须在参数列表后加上 mutable 关键字。 int main() int count = 0; // 错误：无法在非 mutable lambda 中修改按值捕获的变量 // auto f = [count]() count++; ; // 正确：使用 mutable auto f = [count]() mutable count++; std::cout count; // 输出的是副本，外部的 count 不变 ; f(); // 输出 1 std::cout count; // 输出 0 （外部变量未被修改） std::fuction通用的函数包装器，为各种可调用实体（普通函数、函数指针、lambda、std::bind 表达式、函数对象等）提供了一个统一的类型。这使得我们可以像使用普通变量一样存储和传递函数，极大地增加了代码的灵活性。 std::bind它可以用来绑定一个可调用对象的部分参数，重新排列参数顺序，或者设置默认参数，从而生成一个新的可调用对象 #include iostream#include functionalusing namespace std::placeholders; // 对于 _1, _2, _3...void print_sum(int a, int b, int c) std::cout a + b + c std::endl;void print_coordinates(int x, int y, int z) std::cout ( x , y , z ) ;class MyClass public: void member_func(int x, const std::string msg) std::cout Member func: x , msg std::endl; ;int main() // 1. 绑定参数：将 print_sum 的第三个参数固定为 10 auto bind_func1 = std::bind(print_sum, _1, _2, 10); // _1 是第一个参数，_2 是第二个参数 bind_func1(5, 3); // 等价于 print_sum(5, 3, 10); 输出 18 // 2. 重排序参数：改变参数顺序 auto bind_func2 = std::bind(print_coordinates, _3, _1, _2); // 新顺序：第三、第一、第二 bind_func2(10, 20, 30); // 等价于 print_coordinates(30, 10, 20); 输出 (30, 10, 20) // 3. 绑定成员函数 MyClass obj; // 绑定成员函数需要传递一个对象实例的指针或引用（这里用 obj） // _1 将作为成员函数的第一个参数 (int x) auto bind_member = std::bind(MyClass::member_func, obj, _1, Hello); bind_member(42); // 等价于 obj.member_func(42, Hello); // 4. 与 std::function 结合使用 std::functionvoid(int, int) func = std::bind(print_sum, _1, _2, 100); func(50, 25); // 等价于 print_sum(50, 25, 100); 输出 175 return 0; 4.线程C++11 是 C++ 标准中第一次正式引入跨平台的线程库支持的版本，它将并发和多线程编程纳入了语言和标准库，使得开发者不再依赖 pthread、Windows API 等平台特定的接口。 c++11新特性之线程相关所有知识点 - 简书 基础实现：std::thread用于创建和管理线程。 构造时接收一个可调用对象（函数、lambda、函数对象等），作为线程的入口。 提供 join()（阻塞等待子线程结束）和 detach()（分离线程，让其后台运行）两种线程生命周期管理方式。 禁止拷贝（避免二义性），支持移动语义 #include iostream#include threadvoid worker(int id) std::cout Thread id is running ;int main() std::thread t(worker, 1); t.join(); // 等待线程结束 std::mutexstd::mutex m; std::lock_guardstd::mutex lock(m); // 临界区std::unique_lockstd::mutex lock(m);lock.unlock(); // 可提前解锁lock.lock(); // 再次加锁unique_lock开销比lock_guard更大 std::condition_variable用于线程之间同步,必须与互斥量一起使用。 wait(std::unique_lockstd::mutex lock, Predicate pred)：阻塞当前线程，直到被唤醒且条件谓词 pred 为 true。它会自动释放锁，并在被唤醒后重新获取锁。 notify_one()：唤醒一个等待中的线程（如果有）。 notify_all()：唤醒所有等待中的线程。 典型的生产者消费者写法： #include iostream#include thread#include mutex#include condition_variable#include queuestd::queueint data_queue;std::mutex mtx;std::condition_variable cv;constexpr int MAX_ITEMS = 10;void producer() for (int i = 0; i MAX_ITEMS; ++i) std::this_thread::sleep_for(std::chrono::milliseconds(100)); std::lock_guardstd::mutex lock(mtx); data_queue.push(i); std::cout Produced: i std::endl; // lock 在这里析构解锁 cv.notify_one(); // 通知一个消费者 void consumer() while (true) std::unique_lockstd::mutex lock(mtx); // wait 会释放 lock，并在被唤醒后重新获取 lock // 如果 lambda 返回 false，继续等待；返回 true，则继续执行 cv.wait(lock, [] return !data_queue.empty(); ); int value = data_queue.front(); data_queue.pop(); lock.unlock(); // 可以提前手动解锁，减少锁的持有时间 std::cout Consumed: value std::endl; if (value == MAX_ITEMS - 1) break; int main() std::thread prod(producer); std::thread cons(consumer); prod.join(); cons.join(); std::future 和 std::promise - 异步结果std::future表示一个未来会产生的值，用get()获取结果,可能会阻塞 std::promise提供结果的生产者，和future成对出现，也就是说一个是生产者，一个是消费者 #include iostream#include thread#include futurevoid worker(std::promiseint p) int result = 42; p.set_value(result); // 把结果传递出去int main() std::promiseint p; std::futureint f = p.get_future(); std::thread t(worker, std::move(p)); std::cout Result from worker: f.get() std::endl; t.join(); #include iostream#include thread#include futurevoid worker(std::promiseint p) try throw std::runtime_error(something went wrong); catch (...) p.set_exception(std::current_exception()); // 传递异常 int main() std::promiseint p; std::futureint f = p.get_future(); std::thread t(worker, std::move(p)); try std::cout f.get() std::endl; // 这里会抛出异常 catch (const std::exception e) std::cout Caught exception: e.what() std::endl; t.join(); std::packaged_task#include future#include iostreamint heavy_work(int x) // 模拟繁重计算 return x * x;int main() // 将函数 heavy_work 包装成一个任务 std::packaged_taskint(int) task(heavy_work); // 获取与任务结果关联的 future std::futureint fut = task.get_future(); // 在线程中运行任务（任务对象不可复制，必须移动） std::thread t(std::move(task), 10); t.detach(); // 可以分离，用 future 来获取结果 std::cout Result: fut.get() std::endl; // 输出 100 return 0; std::async#include iostream#include future#include chronoint compute() // 模拟耗时操作 std::this_thread::sleep_for(std::chrono::seconds(2)); return 42;int main() // 异步启动 compute 函数 std::futureint fut = std::async(std::launch::async, compute); // 在主线程做其他事情... std::cout Doing other work... ; // 当需要结果时，get() 会阻塞直到结果就绪 int result = fut.get(); std::cout The answer is: result std::endl; return 0; std::atomicstd::atomic 是一个模板类（如 std::atomicint, std::atomicbool），它包装了一个类型，并提供了一系列保证原子操作的成员函数。原子操作意味着该操作从任何线程的视角看，都是不可分割的 #include atomicstd::atomicint counter(0);// 线程 1 (Writer)int new_value = compute_expensive_value();counter.store(new_value, std::memory_order_release); // 原子写// 线程 2 (Reader)int current_value = counter.load(std::memory_order_acquire); // 原子读//std::atomic允许根据场景选择不同的同步强度，在保证正确性的前提下追求极致性能。//内存顺序通过 std::memory_order 枚举来指定，作为参数传递给 load, store, fetch_* 等操作。counter.fetch_add(1, std::memory_order_relaxed); 关于内存序，这是一个比较复杂的问题，X86\\Riscv的不同强弱内存序也有不同定义与实现。 std::atomic_flag 是专为实现自旋锁而设计的最简单的原子布尔类型 //自旋锁是一种忙等待（Busy-Waiting） 锁。当一个线程尝试获取一个已经被其他线程持有的自旋锁时，它不会立即进入睡眠状态（像互斥锁那样），而是会在一个循环中不断地检查锁是否已经被释放（即“自旋”），直到最终获取到锁为止。#include atomicclass Spinlock private: // ATOMIC_FLAG_INIT 确保标志初始为“清除”（false）状态 std::atomic_flag lock_flag = ATOMIC_FLAG_INIT;public: void lock() // test_and_set() 是原子操作： // 1. 读取当前值 // 2. 无论当前值是什么，都将其设置为 true // 3. 返回它读取到的**旧值** while (lock_flag.test_and_set(std::memory_order_acquire)) // 如果旧值是 true，说明锁已被占用，循环继续自旋 // 如果旧值是 false，说明成功获取锁，循环结束 // 可选：在自旋等待时提示CPU降低功耗或切换超线程 // __builtin_ia32_pause(); // GCC/Clang intrinsic for x86 // std::this_thread::yield(); // 如果等待时间可能较长，可主动让出时间片 void unlock() // 将标志清除（设为false），释放锁 lock_flag.clear(std::memory_order_release); ;// 使用示例Spinlock my_lock;int shared_data = 0;void critical_section() my_lock.lock(); // 获取锁，若失败则自旋等待 shared_data++; // 安全地修改共享数据 // ...其他操作 my_lock.unlock(); // 释放锁 自旋锁与互斥锁的比较 特性 自旋锁 (Spinlock) 互斥锁 (Mutex, e.g., std::mutex) 等待机制 忙等待 (Busy-Waiting)。线程在CPU上循环检查，不放弃CPU时间片。 阻塞等待 (Blocking-Wait)。线程被操作系统挂起，放入等待队列，放弃CPU时间片。 开销 获取释放锁的开销极小（主要是原子CPU指令）。但等待期间消耗CPU周期。 获取释放锁的开销较大（需要进入操作系统内核进行线程调度）。但等待期间不消耗CPU。 适用场景 1. 临界区代码非常短（执行速度快）。 2. 等待时间极短。 3. 不希望发生线程上下文切换（因其开销可能比短暂等待更大）。 例如： 内核编程、无锁数据结构、性能关键的底层代码。 1. 临界区代码执行时间较长。 2. 等待时间可能较长或不可预测。 3. 适用于绝大多数应用程序级别的并发。 例如： 文件操作、数据库访问、复杂的计算过程。 缺点 浪费CPU资源。如果锁被长时间持有，自旋线程会空转CPU，导致性能下降（“烧CPU”）。 上下文切换开销。线程切换需要保存和恢复寄存器状态，开销较大。 短期持有用自旋，长期持有用互斥 5.类型推导auto让编译器在编译期根据初始化表达式自动推导出变量的类型 decltype查询一个表达式（而非初始化器）的类型。它返回该表达式的精确类型，包括引用和 const 限定符。","tags":[null]},{"title":"CPP杂记","path":"/notebooks/Interview/CPP杂记.html","content":"全局静态变量、函数内静态变量、attribute((destructor))析构顺序构造析构顺序的不确定性 以及静态函数获取的单例。 C++ 标准规定：同一个编译单元（同一个 cpp 文件）内，静态全局对象的析构顺序与构造顺序相反。 但不同编译单元（不同 cpp 文件so）之间的析构顺序是未定义的。 局部 static（即函数内 static）对象的析构顺序与其定义顺序有关，但也只在同一编译单元内有保证。 若一个变量仅在单个文件中可见，则建议将这个变量声明为静态全局变量，static修饰的静态全局变量仅在当前文件中可见。 如果一个全局变量只被单个函数使用,将其改为该函数的静态局部变量可以进一步限制变量的作用域,提高代码的内聚性,降低耦合度。静态局部变量具有全局寿命但局部作用域的特点, 静态全局变量是存储在**静态数据区的,**而不是栈区,因此静态全局变量的大小不会导致栈溢出。栈溢出通常是由于函数调用层次过深或局部变量过大导致的。 类的内存占用 1.32位系统中虚函数指针为4字节，64位为8字节 2.只需要考虑虚函数指针，虚函数表不计入某个类的资源 3.char占一字节，但是需要考虑内存调用 4.如果有虚继承，则多一个虚基类指针。 5.空类占一个字节（用于标识） 指针好题 int arr[5]{1,2,3,4,5};在这个数组的定义中，通常的理解arr是数组的地址即数组首元素的地址，进一步理解arr是一个int型的指针常量，常量+1地址偏移sizeof(int)，所以arr+1是首元素下一个元素的地址；考虑到这一层就不难理解**arr*的含义，arr是对arr取地址，结果也是个地址，只是这个地址的类型是指向有5个int类型数据的数组的指针常量，这个常量+1地址偏移5sizeof(int)。 各级指针算各级的： 主要就是理解 和 * 的“升级降级”； 链接：https://www.nowcoder.com/exam/test/89156461/submission?examPageSource=Intelligentpid=62380309testCallback=https%3A%2F%2Fwww.nowcoder.com%2Fexam%2Fintelligent%3FquestionJobId%3D10%26subTabName%3Dintelligent_page%26tagId%3D21000testclass%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91 Copy and Swap传统做法operator= class MyString private: char* data; // 动态分配的字符串public: // 赋值运算符重载 MyString operator=(const MyString other) // 检查自赋值 if (this == other) return *this; // 释放当前对象的资源 delete[] data; // 复制数据 data = new char[std::strlen(other.data) + 1]; std::strcpy(data, other.data); // 返回当前对象的引用 return *this; ; Copy and Swap MyString operator=(const MyString other) // 检查自赋值 if (this == other) return *this; MyString tmpother; std::swap(data,other.data); return *this; 优势： 异常安全：传统方法new抛出异常时，对象出于无效状态。data已经被删除，但是分配失败 强异常安全性: 如果一个操作因为异常而失败，程序的状态会回滚到操作之前的样子，就像这个操作从来没执行过一样 代码复用：复用拷贝构造函数 自动资源管理：自动释放tmp资源 C++11 写法 MyString operator=(MyString other) //值传递 //传入左值：拷贝构造 //传入右值：移动构造 std::swap(data,other.data); return *this; 移动构造还是拷贝构造？ 左值：叫得出名字 右值：叫不出名字（临时变量，std::move） 左值用拷贝构造，右值用移动构造（偷） 模板模板特化、偏特化模板特化是为特定的模板参数提供特殊实现的技术。当通用模板不适合某些特定类型时，可以为其提供定制版本。 所有模板参数都指定具体类型就叫模板全特化 偏特化只对部分模板参数进行特化,函数模板不支持偏特化，只有类模板支持 函数模板特化 #include iostream#include cstring// 通用模板templatetypename Tvoid print(T value) std::cout General: value std::endl;// 全特化 - 为const char*类型templatevoid printconst char*(const char* value) std::cout C-string: \\ value \\ std::endl;// 全特化 - 为int类型templatevoid printint(int value) std::cout Integer: value (0x std::hex value ) std::endl;int main() print(3.14); // 调用通用版本 print(Hello); // 调用const char*特化 print(42); // 调用int特化 return 0; 类模板特化 #include iostream// 通用模板templatetypename Tclass TypeInfo public: static const char* name() return Unknown; ;// 全特化 - int类型templateclass TypeInfoint public: static const char* name() return int; ;// 全特化 - double类型templateclass TypeInfodouble public: static const char* name() return double; ;// 全特化 - const char*类型templateclass TypeInfoconst char* public: static const char* name() return const char*; ;int main() std::cout TypeInfofloat::name() std::endl; // Unknown std::cout TypeInfoint::name() std::endl; // int std::cout TypeInfoconst char*::name() std::endl; // const char* return 0; 偏特化 #include iostream// 通用模板 - 两个类型参数templatetypename T, typename Uclass Pair public: void print() std::cout General PairT, U std::endl; ;// 偏特化 - 两个类型相同templatetypename Tclass PairT, T public: void print() std::cout Specialized PairT, T std::endl; ;// 偏特化 - 第二个参数为inttemplatetypename Tclass PairT, int public: void print() std::cout Specialized PairT, int std::endl; ;int main() Pairdouble, char p1; p1.print(); // General PairT, U Pairfloat, float p2; p2.print(); // Specialized PairT, T Pairstd::string, int p3; p3.print(); // Specialized PairT, int return 0;//指针类型进行偏特化// 通用模板templatetypename Tclass Wrapper public: static const char* type() return Value; ;// 偏特化 - 指针类型templatetypename Tclass WrapperT* public: static const char* type() return Pointer; ;// 偏特化 - 指向指针的指针templatetypename Tclass WrapperT** public: static const char* type() return Pointer-to-Pointer; ;int main() std::cout Wrapperint::type() std::endl; // Value std::cout Wrapperint*::type() std::endl; // Pointer std::cout Wrapperint**::type() std::endl; // Pointer-to-Pointer return 0; 可变参数模板基本用法： templatetypename... Args // Args是模板参数包void function(Args... args) // args是函数参数包 // 处理参数... #include iostream// 基础情况：0个参数时返回0templatetypename TT sum(T value) return value;// 递归求和templatetypename T, typename... ArgsT sum(T first, Args... args) return first + sum(args...);// 计算参数个数templatetypename... Argsstd::size_t count(Args... args) return sizeof...(args); // sizeof... 操作符获取参数包大小int main() std::cout sum(1, 2, 3, 4, 5) std::endl; // 15 std::cout sum(1.5, 2.5, 3.5) std::endl; // 7.5 std::cout count(1, a, hello, 3.14) std::endl; // 4 return 0; 非类型模板参数(常量模板)cuda中用的多，用于编译器常量优化 template int Nint sumArray(int (arr)[N]) int sum = 0; for (int i = 0; i N; i++) sum += arr[i]; return sum;int a[5] = 1,2,3,4,5;int total = sumArray(a); // N=5，编译期确定 类型转换运算符static_cast 静态转换用于在编译时已知的，有逻辑关联的类型之间的转换 场景 //基本数据类型之间的转换int i = 10;double d = static_castdouble(i); // int - double//派生类指针/引用 - 基类指针/引用（向上转换，Upcasting）。这是安全的，并且是隐式转换的显式写法。class Base ;class Derived : public Base ;Derived derived;Base* basePtr = static_castBase*(derived); // 安全//基类指针/引用 - 派生类指针/引用（向下转换，Downcasting）。不安全Base* basePtr = new Derived(); // 实际上指向一个Derived对象Derived* derivedPtr = static_castDerived*(basePtr); // 可以，但有风险//任何具有转换构造函数的类型转换class MyClass public: MyClass(int x) // 转换构造函数;int num = 5;MyClass obj = static_castMyClass(num); dynamic_cast动态转换用于安全的在继承参差结构中进行向下转换或者交叉转换，依赖运行时类型信息RTTI 用法： 将基类指针或引用安全的转换为派生类的指针或者引用，必须用于多态类型（至少含有一个虚函数） ​\t转换成功会返回目标类型的指针，转换失败返回null或者std::bad_cast class Base virtual void foo() ; // 多态基类（有虚函数）class Derived : public Base ;Base* basePtr = new Derived(); // 正确指向派生类// 安全向下转换Derived* derivedPtr = dynamic_castDerived*(basePtr);if (derivedPtr != nullptr) // 转换成功，可以使用derivedPtr else // 转换失败Base* basePtr2 = new Base(); // 指向基类本身Derived* derivedPtr2 = dynamic_castDerived*(basePtr2);// derivedPtr2 将是 nullptr！// 引用转换try Derived derivedRef = dynamic_castDerived(*basePtr); catch (const std::bad_cast e) std::cout 转换失败: e.what() std::endl; const_cast 常量转换用于修改类型的 const 或 volatile 属性。这是唯一能“去掉” const 属性的转换操作符 主要使用方式就是去掉const属性，以便接受一个非const参数但是不会修改还旧API void printString(char* str) // 一个旧的、不修改str的函数 std::cout str std::endl;const char* myStr = Hello, World!;// printString(myStr); // 错误：不能将const char* 传递给char*printString(const_castchar*(myStr)); // 可行，但有风险 reinterpret_cast 重新解释转换进行低级的、底层的、“重新解释”比特模式的转换。它可以将一个指针转换为任何其他类型的指针，甚至是一个整数。 不进行任何格式检查或安全性检查，可移植性差 场景 //在函数指针类型之间进行转换。//在指针和足够大的整数类型之间进行转换（如 void* 转 uintptr_t）。int* ip = new int(65);// 将int指针毫无关系地转换为char指针，然后打印其指向的值char* cp = reinterpret_castchar*(ip);std::cout *cp; // 输出 A (因为65是A的ASCII码)","tags":[null]},{"title":"nk01(tx-network)","path":"/notebooks/Interview/nk01(tx-network).html","content":"1.5层网络分别是什么，讲讲每层什么协议，有什么东西2.MTU包含哪些层的数据，大小一般多大怎么计算3.MTU和MSS区别4.TCP怎么保证可靠的13.你作为一个一端的程序，是怎么知道是网络拥塞还是是链路丢包的 27.k个一组翻转链表 作者：静静地看你们表演链接：https://www.nowcoder.com/feed/main/detail/ebd659f61ec743d89c402d24311ca250来源：牛客网","tags":[null,null]},{"title":"侯捷C++内存管理机制","path":"/notebooks/Interview/侯捷C++内存管理机制.html","content":"C++内存调用 第一节 C++ primitives","tags":[null]},{"title":"ai_hpc_cuda","path":"/notebooks/Interview/ai_hpc_cuda.html","content":"NvidiaGPUCUDA相关：1.描述一下SM的结构，在写kernel的时候共享内存大小和寄存器文件数量需要注意吗？ SM是NVIDIA GPU的核心计算单元,包含 CUDA core，最核心的基本计算单元，处理整形和单精度浮点运算。 寄存器文件、 Warp Scheduler线程数调度器， 共享内存 L1cache 等 写kernel时共享内存大小和寄存器文件数量直接影响SM的活跃线程束数量（Occupancy）—-即SM上同时可执行的线程束数与最大可支持线程束数的比率。 共享内存大小： 注意：必须注意。共享内存是按块分配的有限资源。 影响：每个块申请的共享内存越大，一个SM上能同时驻留的线程块就越少，会降低占用率（Occupancy），可能影响性能。 寄存器数量： 注意：必须注意。寄存器是按线程分配的有限资源。 影响： 每个线程使用的寄存器越多，一个SM上能同时驻留的线程就越少，同样会降低占用率。 寄存器使用过多会导致寄存器溢出（Register Spilling），编译器被迫将变量存储到慢速的全局内存中，严重损害性能。 2.共享内存和寄存器分别应该存放哪些数据，其用量与SM上活跃的线程块的关系。共享内存 从全局内存预加载的数据块、中间计算结果（如规约运算的局部和） 作用：协作、缓存、通信 寄存器 线程私有的局部数据 作用：私有性、高性能 关系 SM上的活跃线程块数量同时受共享内存总量和寄存器总量的硬性约束，最终的实际数量是以下三个计算结果中的最小值： SM支持的最大线程块数（架构限制）。 SM共享内存总量 / 每块申请的共享内存大小。 SM寄存器总量 / (每线程寄存器数量 * 每块线程数)。 3.bank冲突是什么？描述具体结构，如何解决？Bank冲突为了提供高带宽，共享内存被物理上划分为若干个（通常是32个，与Warp大小对应）同样大小的、能同时被访问的内存模块，这些模块称为 Bank。 理想情况（无冲突）： 如果一个Warp中的32个线程分别访问32个不同Bank中的地址（或者访问同一个Bank中的完全相同的一个地址，即广播），那么所有这些访问都可以在一个时钟周期内一次性完成。 冲突情况： 如果一个Warp中的多个线程访问了同一个Bank中的不同地址，就会发生Bank冲突。硬件必须将这些冲突的访问拆分成多个没有冲突的周期依次执行。有n个线程访问同一个Bank的不同地址，就需要n个时钟周期来完成原本一个周期就能完成的工作。 Bank编号Bank编号 (地址字节偏移量 4字节) % 32 解决核心思路是：改变数据在共享内存中的布局或访问模式，确保一个Warp内的线程访问不同的Bank。 方法一：内存填充（Memory Padding） 问题： 在操作二维数组（例如矩阵）的Tile时，如果数组的宽度是Bank数量（32）的整数倍，那么同一行中相邻的元素会位于不同的Bank，但同一列中相邻的元素会因为固定的步长而落在同一个Bank里。当线程按列读取时，就会导致严重的Bank冲突。 解决方案： 在声明共享内存数组时，人为地给每一行增加一些多余的“填充”元素，使实际的行长（Pitch）不再是32的整数倍。 示例：一个32x32的Tile cpp // 可能产生Bank冲突的声明__shared__ float tile[32][32];// 使用填充避免Bank冲突的声明（例如，多加1个元素）__shared__ float tile_padded[32][33]; // 33不再是32的因数 这样，原来在同一列上的元素 tile[0][0], tile[1][0], tile[2][0]… 现在变成了 tile_padded[0][0], tile_padded[1][0], tile_padded[2][0]…。计算它们的Bank编号： (0 / 4) % 32 = 0 ( (1 * 33 * 4) / 4 ) % 32 = (33) % 32 = 1 ( (2 * 33 * 4) / 4 ) % 32 = (66) % 32 = 2它们被巧妙地分散到了不同的Bank中，从而避免了冲突。 方法二：改变访问模式或算法设计核函数时，尽量让一个Warp内的线程访问连续的共享内存地址。因为连续地址通常映射到不同的Bank，这是最友好的访问模式。 方法三：使用不同的广播机制如果确实需要让多个线程读取同一个值，应尽量确保它们访问的是完全相同的地址，这会触发广播机制，在一个周期内完成操作，而不是产生冲突。 4.说一下分支冲突（线程束分化），如果warp内有冲突，部分符合if条件，部分符合else条件，是否需要等待？分支冲突发生在同一个Warp内部的线程执行了不同的控制流路径时。例如，部分线程满足if条件，而另一部分线程满足else条件。 串行化执行： GPU的Warp调度器会让Warp先执行所有走if路径的线程。此时，那些本该走else路径的线程在这个阶段是被禁用（masked out） 的，它们不会执行任何操作，但必须等待。 再次执行： 当if路径执行完毕后，调度器会再让Warp执行所有走else路径的线程。同样，此时走if路径的线程被禁用并等待。 汇合后继续： 当所有不同的控制流路径都执行完毕后，Warp内的所有线程才会在汇合点（reconvergence point） 重新同步，并继续一起执行后续的相同指令。 因此，分支冲突的性能代价是：执行时间变成了所有不同路径执行时间的总和，而不是其中最长路径的时间。 怎样避免线程束分化 核心思想：同一个Warp内的数据具有相同的特性，从而执行相同的指令路径。 预处理数据，使同一Warp数据特征一致 谓词执行 用条件赋值 ?: 替代短小的 if-else 5.用过TensorCore吗？了解TensorCore的原理吗？Tensor Core是一种专为执行矩阵乘累加运算而设计的专用硬件单元，从Volta架构开始引入。其核心原理可以概括为： 1. 计算模式：D A * B + C它的核心是执行一个固定的计算操作：接收两个小矩阵A和B，与一个累加矩阵C相乘后相加，得到结果矩阵D。 A, B, C, D 都是特定维度的小矩阵（如 16x16, 8x32, 32x8 等）。 计算A * B是完整精度的，其结果与C相加后，再以目标精度（如FP32FP16INT8）存储到D。 2. 混合精度计算：这是其实现性能突破的关键。它使用低精度输入来实现高吞吐量和低功耗，但使用高精度进行累加以保持数值稳定性。 常见模式： FP16 输入 (A, B) + FP16 或 FP32 的累加器 (C) - FP16 或 FP32 输出 (D)。 其他模式： 也支持INT8、INT4、BF16等输入精度，以及TF32（在Ampere及以后架构中）。 3. 极高的吞吐量：每个Tensor Core每个时钟周期可以执行大量的乘加运算（FMA operations）。例如，一个V100的SM中的每个Tensor Core每周期可以执行一个 8x4 * 4x8 - 8x8 的MMA操作，这相当于 64 次乘加运算（128 FLOPS） 每周期。这与传统的CUDA Core（每周期1次乘加）相比，吞吐量提升了两个数量级。 4. 编程模型：Warp-Level OperationTensor Core的操作是在线程束级别进行的。一个Warp内的线程需要协作来共同加载一个大的输入矩阵的各个小块（Tile）到寄存器中，然后调用一条指令（如wmma::mma_sync）来让Tensor Core硬件执行整个小矩阵的运算。 为什么用float4向量来存取数据？有什么好处？ 为什么用双缓冲优化？了解cuda流和cuda graph吗？ 除了MPI，有知道现在用的更多的GPU通信库吗？ 在Nsight Computing中，经常关注的与内存相关的指标。有关注L1 Cache命中率吗？ GPU指令集优化方面了解吗？有做过PTX相关的优化吗？ GEMM是计算密集型还是访存密集型算子？ 知道cutlass中如何对GEMM进行优化的吗？ 训练推理了解Transformer吗？底层是什么结构？cuda中如何优化？说一下你对大模型的理解。cuda中如何写Softmax？某个参数过大如何解决？Dropout和BatchNorm在训练和推理时有什么区别？说一下你了解的无监督学习算法。知道Faster Transformer吗？有了解如何实现的吗？Paged Attention有了解吗？知道TensorRT吗？部署过推理模型吗？","tags":[null,null]},{"title":"控台项目与iSet项目已使用Qt技能点整理","path":"/notebooks/Interview/技能点整理.html","content":"Qt与QML知识总结Qt中的设计模式中间滑动的slider 2. doubleSpinBox3. iSet新建项目的两种模式ListWidget、选中状态设置为图标模式4.QTreeWidget 框架、表头、样式表、代理 Qt QTreeWidget树形控件用法详解_qtreewidget用法_睿科知识云的博客-CSDN博客 2. Qt大模块1. Qt DP指针2. 视图模型机制3. 视图框架4. QStyle 高度自定义实现5. 元对象与信号槽机制6.构建系统7.QInvokeMethod8.国际化3. Qt细节1. 基础控件2. Undo操作3. svg操作4. Qt不在对象树的工具汇总5.Qt中有有用的宏：Q_LIKELY 6.paint在install中绘制 4. Qt锦上添花1. 动画知识2. 拖拽3. 插件机制4.模块化构建5. Qt中的设计模式1. 单例模式2. 责任链模式3. 接口模式4. 适配器模式5.观察者模式， 线程观察、时间压缩 6.全局信号单例转发类 MAINOPERATIONVIEW_EXPORT bool MainOperationView_Init(IISetWidget ** ppWidget, QWidget * parent) if (ppWidget == nullptr || parent == nullptr) return false; *ppWidget = new CMainOperationView(parent); return true; 6. C++知识c++11新特性，所有知识点都在这了！ - 知乎 (zhihu.com) this_thread ++ lambda的坑 for(int id = enCoef9_Rr ; id = enCoef9_Bb ;id++) connect(m_spinBoxs[id],QOverloaddouble::of(QDoubleSpinBox::valueChanged),this,[](double value) OnCoefMatrixSlot(id,value); ); id 恒等于 0 多继承与QOBject private 虚继承 class CBaseprivate:\tvirtual void virtualPrivateFuntion() std::coutbase virtualPrivateFuntion;\t；class CSub :public CBase\tprivate:\tvirtual void virtualPrivateFuntion() override std::coutsub virtualPrivateFuntion;\tint main()\tCBase* pObject = new CSub();\tpObject-virtualPrivateFuntion();//结果：sub virtualPrivateFuntion，//结论：子类继承父类的private virtual可以重写 可变参数、变参模板 函数包装器 std::shared_ptrreset、make_shared 右值引用C++反射元编程Metaprogram is a program about a program. 7.杂项QStatusBar插件机制解耦快捷键ISet7.0 接口设计 MVC8.轮子1.单例2.工作线程封装3.stl 迭代器模式和适配器模式报错整理： QMenu 没有添加Action时，不能直接visible或这exec； setGeometry: Unable to set geometry 的一种解决办法是重写sizehint，而不是使用setFixedSize； 工具使用AddressSanitizer（不适用与MinGW）[AddressSanitizer 定位嵌入式cc++内存错误 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/436177229#:~:text=AddressSanitizer （又名 ASan）是 C%2FC%2B%2B 的内存错误检测器。 AddressSanitizer 由 google,的一部分，而从 4.8 版开始逐渐成为 GCC 的一部分。 这也意味着如果交叉编译器版本低于 4.8 ，是无法使用的。) Qt 拾遗 008 在 Qt 中使用 Address Sanitizer - 简书 (jianshu.com) 在Qt中使用gcc 4.8.0的地址消毒剂(Address Sanitizer) MTuner软件【精选】基于MTuner软件进行qt的mingw编译程序的内存泄漏检测_mtuner怎么使用_yantuguiguziPGJ的博客-CSDN博客","tags":[null]},{"title":"Hexo","path":"/notebooks/other/hexo.html","content":"Hexo部署过程中遇到的一些问题图片与Typora兼容网上给了很多方法，尝试没有效果，自己尝试的方案： 卸载插件hexo-asset-image: yarn remove hexo-asset-image,GitHub有修改的对应插件，但是使用无效果 typora配置 Typora新插入图片路径显示为： Hexo部署时正常显示 Hexo不生效问题Hexo部署到github上不生效，网上解决方案说需要除Main或者Master分支以外额外构建一个分支，然后网站推送到这个分支上，实际使用不管用。 首先，只需要维护一个主分支，不需要其他分支； 其次，hexo -d执行以后，main分支中即使已经更新了代码，但是github page更新也需要一段时间，部署频繁会产生排队 仓库主页github-pages上可以观察到生成静态页面的进度。","tags":[null]},{"title":"perf使用","path":"/notebooks/other/perf使用.html","content":"Perf使用 杂记1.文档1.man+perf -h遇到新工具当然最好的方式是读文档，但是网络上找半天也没有找到很好的perf手册，最后发现在GPT的指引下，最好的手册害得是linux man手册。 上手最快最全面的方式：help+man 首先perf -h查看各条子命令基本功能。 在使用子命令时 man perf-record、man perf-script查看各条子命令的详细功能。 踩坑案例： 折腾半天perf record记录的L3 miss，perf script查看对应的结果，发现里面有一些地址，和程序malloc时的地址进行匹配发现都没有访问程序malloc的位置，最后发现其中的地址是ip而不是访问的内存地址。几番查找资料无果，最后发现perf record中使用-d参数才会显示addr 2.event查看各个事件的详细描述，在intel 官方的事件库中查找对应的事件功能 https://perfmon-events.intel.com/index.html?pltfrm=skylake_server.html 3.文章与优化案例https://www.brendangregg.com/ https://weedge.github.io/perf-book-cn/zh/chapters/3-CPU-Microarchitecture/3-8_Modern_CPU_design_cn.html 2.踩坑杂记perf事件冲突","tags":[null]},{"title":"ASPLOS`25 Systematic CXL Memory Characterization and Performance Analysis at Scale","path":"/notebooks/paper/ASPLOS-25-Systematic-CXL-Memory-Characterization-and-Performance-Analysis-at-Scale.html","content":"Systematic CXL Memory Characterization and Performance Analysis at Scale Jinshu Liu Virginia Tech https://github.com/MoatLab/Melody 1.IntroductionCurrently, there is a significant gap in research that explores detailed CXL characteristics and their impact on memory-intensive workloads at scale, in depth, and across the full spectrum of sub-μs latencies. In particular, how do CXL devices differ in detailed performance characteristics beyond average latency and bandwidth metrics? How (much) does CXL’s long (and longer) latency affect CPU efficiency and workload performance? What are the underlying causes and how do we analyze it? Exsiting works focus on coarse-grained analysis and overlook several critical aspects: (i) CXL performance stability (i.e., tail latencies); (ii) CPU tolerance to prolonged CXL latencies across various workloads, and the architectural implications of CXL; and (iii) the lack of systematic approach to dissect workload performance and CPU inefficiency under CXL. So: introduce Melody, a comprehensive framework for detailed CXL performance characterization. The first analysis of CXL characteristics beyond average latency and bandwidth across 4 real CXL devices. An extensive evaluation of CXL’s performance implications across diverse workloads. A systematic approach for workload performance analysis under CXL. contributions(in my view): ​\t1.MELODY,a framwork to measure CXL perfomence. ​\t2.An in-depth study of CXL tail latencies (like caption). ​\t3.Root-cause analysis approach 2.BackgroundHow CPU backend and CXL MC process Load and Store request? .nnbutvhwqoog{zoom:200%;} Request types: The CPU issues two types of load requests: Demand and Prefetch. Demand loads are memory reads that CPU requests from (CXL) MC only when it is needed for computation. Prefetch reads are predictive reads directed by prefetchers, e.g., “L1PF” and “L2PF” in Figure 2a. Stores are first queued in the “store buffer.” Each store request triggers a Read-for-ownership (RFO) for cache coherence from CXLDRAM, followed by a Write upon cache eviction. MC ： Memory requests to the CXL MC are encapsulated(compress) in a specific packet format, known as Flits , for transmission over CXLPCIe. Upon arrival, the CXL controller (“CXL Ctrl”) parses the request and places it in the request queue. The request scheduler then selects the next request to process based on the scheduling policy and other factors such as thermal management for low latency, high bandwidth, and reliability. Requests are then passed to the command scheduler, which issues appropriate low-level DDR commands to the DRAM chips. 3.CXL Device Characterization3.1 Testbed Concern: work load: cloud workloads (in-memory caching and databases such as Redis [13] and VoltDB [21], CloudSuite [1], and Phoronix [12]), graph processing (GAPBS [22], PBBS [19]), data analytics (Spark [30]), MLAI (GPT-2 [5], MLPerf [14], Llama [9]), SPEC CPU 2017 [18], and PARSEC [24]. 3.2 CXL latency stability and its relationship with bandwidth Terms distinction: Loaded latencies: memory access latency under high utilization Idle latency: occurs when the system experiences minimal load 这一部分实现了一个MIO，通过多次指针追踪记录一次rdtsc时间戳来计算average latency，并采用MLC来验证MIO。测试了一些tail latency 与bandwidth之间的关系，结果均可以想到。 一个测量内存压力的方法：将指针追踪访问线程和32个AVX访存线程一起bind到一个numa nod(co-locate) CXL latency vs. bandwidth under various readwrite ratios. Local DRAM achieves the highest bandwidth under a read-only workload, whereas NUMA and all CXL devices (except CXL-C) achieve minimal bandwidth in read-only scenarios. This is because NUMA and CXL links are bidirectional, allowing them to sustain higher bandwidth under mixed readwrite workloads CXL devices demonstrate significant variability Impact of CPU prefetchers on (tail) latency. Prefetching does not fully mitigate CXL-induced tail latencies. Reasoning. 本节中测出的结果发现尾延迟等性能差距很大，这样的结论其实作用不大。但是性能差异大可以作为其他性能研究的挑战和动机 1.CXL协议传输层与连接层的实现本身引入了性能开销 2.MC 控制器实现本身 4 Workload Characterization 讨论了一些工作负载的延迟敏感性等，此前论文已经有过 5 Spa for CXL Slowdown Analysis5.2 Challenges and Limitations of State-of-the-ArtChallenge:1.Identifying the underlying CPU eventsmetrics that can correlate to the slowdowns is challenging. It is even more challenging to establish a precise correlation between workload performance and architecture-level performance metrics, Why not TMA? TMA does not provide a differential analysis to interpret pipeline differences resulting from varying backend memory (i.e., CXL vs. local DRAM). TMA is unable to precisely correlate architecture level metrics with workload slowdowns. 5.3 Spa: A Bottom-Up Approach DRAM (Demand Load) Slowdown:These misses denote demand read misses, excluding RFO and prefetch requests. Store Slowdown :Incoming store requests queued in the store buffer are dequeued upon completion. Some writes issue RFO requests before execution. If the store buffer fills up, these RFOs would hinder load efficiency, causing CPU stalls. Cache Slowdown:On SKX, most cache slowdown occurs in L2 due to a significant rise in stall cycles for L1 load misses with CXL. Conversely, on SPREMR, LLC experiences the bulk of slowdown, with a notable increase in stall cycles for L2 load misses with CXL. key finding: This reduces L2 prefetcher’s coverage of both demand reads and L1 prefetch. L1 prefetches would either miss entirely in L2 or at best, they would hit on a pending L2 prefetch in L2. Consequently, CXL also negatively impacts L1 prefetcher’s timeliness.Loads that would have otherwise hit in the cache if L1 prefetches were timely, now are delayed. Consequently, overall prefetch efficiency suffers and stalls on caches increase. 由于CXL的长延迟，L2预取的信息时效性降低，当L1需要相应数据的时候，L2还没有预取回来，导致L1认为miss，于是访问L2,再次发出请求。原本可以命中的Load请求变得不命中。 intel没有计数器直接观测L1Pf-L2-hit与miss的情况，可以通过一些其他的计数器间接的观测情况。 发现：L2PF-L3-miss减少，L1PF-L3-miss增多，L2PFL3-hit不变，因此推导出：L2预取器低效预取，L1预取增多。 5.5 5.6 Workload Slowdown Diversity Period-based Slowdown AnalysisAn approach to convert time-based sampling data into a period-based slowdown analysis. 5.7 Spa Use CasesPerformance tuning. For example, to mitigate the slowdown bursts observed in 605.mcf (Figure 16b), we first identify memory accesses during bursty periods (e.g., exceeding 10%) using binary instrumentation via Intel Pin. Next, we pinpoint the source code responsible for high slowdowns using addr2line. Our analysis reveals that two performance-critical objects, each 2GB in size, are contributing to the slowdown. 作者提到的两个case，一个是用来做性能优化，一个是作为性能指标来进行分层，这两个点其实都是和后一篇论文有联系，做铺垫。 不足与机会： 作者在验证cache slow down的主要原因的 方法是： To validate this, we disable all the hardware prefetchers (L1 and L2) and measure workload slowdowns. With prefetchers off, we found virtually no stall cycles on cache。这样的方法并不深入，为什么降低？这些值得深挖，但是需要一些硬件探索。 关于预取，可以增加预取器的深度(也就是再多预取几个周期)直接解决这个问题。 5.7中讲到了一个关于SPA的使用案例，通过剖析SPA中的slowdown来分析slowdown，然后把slowdown严重的变量放置到CXL，这样的方法我觉得十分适用。","tags":[null]},{"title":"CYY-RV64.zip","path":"/notebooks/paper/CYY-RV64.zip.html","content":"学习一下CYY师兄的工作，part1 https://www.rv64.zip/ BackgroundMotivation1.理想的RISC-V板子应当包含一组标准的指令拓展RVA23U64,但是目前的生态下，不同硬件支持的拓展与标准并不一致； 2.编译器和CPU没有针对新拓展进行优化的话，盲目打开新拓展反而会导致程序性能下降 Existing solution1.target_clones attributes 改代码，维护成本高 间接调用开销： When using target_clones or target_version, the compiler will use GNU IFUNC to dispatch the function call to the correct version at runtime. This introduces an overhead of an indirect function call, and also refuses some optimizations such as inlining. When compiling without -fno-plt or with -fno-pic, things will be worse since it requires 2 level call to the function (the first level is PLT call). 前置知识： PLT（Procedure Linkage Table) PLT 是动态链接（共享库）中用于实现 延迟绑定（Lazy Binding） 的核心结构，解决程序调用共享库函数时的跳转问题。 首次调用函数时： 程序跳转到 PLT 表中的对应条目（如 printf@plt）。 PLT 条目包含一条跳转指令，默认指向 动态链接器（_dl_runtime_resolve）。 动态链接器解析函数真实地址，并回填到 GOT（Global Offset Table）。 后续调用时： PLT 直接通过 GOT 跳转到真实函数地址（无需再次解析）。 -fno-plt 选项可绕过 PLT，直接通过 GOT 调用 IFUCN（Indirect Function） 运行时动态选择函数的具体实现,通过函数指针跳转，比直接调用多一次寻址，地址运行时确定，无法内联 定义时通过 __ attribute__((ifunc(resolver)))标记函数，提供一个解析器函数。 static void* my_func_resolver() if (__builtin_cpu_supports(avx2)) return my_func_avx2; else return my_func_default;void my_func() __attribute__((ifunc(my_func_resolver))); 最佳案例：GLib通过IFUNC为memcpy提供了多个实现。 Solution1.Decoupled function clone table​\t相当于将target_clones attributes从函数层提升到了文件层，不需要修改源代码。patch 2.Automatic function clone table generation​\t根据perf结果选择最佳的result 3.针对直接调用提了一些编译器端和CPU端的支持 期待CYY师兄的正式论文,后续继续学习 Other1.英语表达读起来好舒服 2.类似的方案在box64之类的二进制翻译场景下也大有用处。","tags":[null]},{"title":"ISCA`25 LIA A Single-GPU LLM Inference Acceleration with Cooperative AMX-Enabled CPU-GPU Computation and CXL Offloading","path":"/notebooks/paper/ISCA-25-LIA-A-Single-GPU-LLM-Inference-Acceleration-with-Cooperative-AMX-Enabled-CPU-GPU-Computation-and-CXL-Offloading.html","content":"Abstract​\t单GPU的内存容量限制了大模型推理，使得使用成本高昂的多GPU部署或者在慢速PCIE传输导致性能受限的CPU-GPU部署十分必要。在这个工作中，我们首先benchmark了最新的带有AMX的Intel CPU，包括4th SPR架构和 6th GNR架构的至强处理器，证明矩阵乘法的带宽达到了20TFLOPS和40TFLOPS，都比得上一些最新的GPU.\t这些发现解锁了更加广泛的CPU计算卸载，减少CPU-GPU传输，与之前代际的CPU相比缓解了带宽瓶颈。 补充：算力分析1.TFLOPS(Trillions of Floating-Point Operations Per Second):每秒万亿次浮点数运算 20~40TFLOPS为中等算力水平 2.算力场景中，更常使用吞吐量(Throughput)TFLOPS而非理论峰值算力，Throughput 更贴近实际任务的性能表现，Throughput 也是优化 LLM 推理性能的核心目标 3.常见GPU 算力： 4090 83TFLOPS | A100 312 FLOPS 基于以上发现，我们设计了LIA，一个单GPU大模型推理加速框架，协同AMX使能的CPU-GPU计算以及CXL卸载，LIA系统的卸载了计算到CPU上，优化了延迟和带宽。这个框架同样介绍了一个内存卸载策略，这个策略无缝的集成了便宜的CXL-DDR内存增强了带宽驱动型任务的性能表现。在有一张H100的SPR系统上，LIA对比之前的单GPU推理框架，达到了5.1到19倍的延迟降低以及3.7到5.1倍的带宽。并且，LIA部署了CXL卸载，产生了一个额外的1.5倍带宽提升（对比纯DDR方案）和1.8倍的最大batch size 提升。 1 Introduction​\tLLM在许多领域释放巨大的潜力，然而，这种前所未有的能力伴随着巨大的花费（指不断扩大的参数规模）。最近的大模型都设计了巨大的参数量，并且 似乎举例参数上限稳定线还十分遥远。这些参数量的增加引发了一个巨大的技术挑战：在一张GPU内保存模型参数和中间值（比如KV-cache和激活值）变得infeasible。即使最新的GPU例如H100达到了94GB的板载HBM内存，但是依旧难以面对推理时的内存需求。针对单GPU的能力限制，最近的一些工作已经转向了多GPU部署，这利用了模型的并行性。然而这些方法在非常昂贵的，同时操作起来十分复杂。例如，部署一个175bilion的参数的OPT模型至少需要5张H100GPU，总花费超过150000刀。因此，通过增加GPU的数量是一种在一些高性价比的经济场景中不是一个可行的方案。许多压缩技术，例如量化、剪枝、以及模型蒸馏（distillation)已经被提出来减轻大模型可拓展性的负担，尽管这些方法减轻了内存需求，然而他们通常以损失模型精度为代价，并且仍然需要多GPU。一个可选择的方向是系统级别的卸载，将模型参数存储在更大的CPU内存中然后按需传输到GPU中。然而，这些方法面临他自己的内存瓶颈，原因是PCIE的带宽有限（H100 的PCIE带宽是64GB，PCIe5.0),这见满了CPU-GPU的传输，导致了大量的推理延迟。 ​\t为了减轻大量数据的传输开销，一些方法已经提出选择一些层在CPU上进行计算，然而，这些CPU、GPU协同的框架的效率被CPU计算带宽限制，FlexGen和FastDecode这两个工作卸载了计算最不敏感的子层到CPU，然而PowerInfoer卸载冷神经元到CPU，这显著降低了模型的精度。除了复杂性以外，大模型需要根据应用的需求运行不同batch size的推理操作。虚拟助手、搜索引擎这这类面向用户接口的请求具有小batch 低延迟的特点，快速响应影响了用户的使用体验。相反，benchmark、信息提取之类的任务具有延迟不敏感性的任务，大batch、高带宽是非常重要的。 ​\t为了处理这些挑战，我们推出了LIA框架，一个单GPU推理加速框架，使用了AMX和CXL技术适配了小batch和大batch场景。LIA主要有3点贡献： AMX矩阵乘法的综合性能分析。 AMX驱动的CPU-GPU协同大模型推理激素 使用CXL内存来拓展带宽 LIA主要有两个组件组成：C1:前端算法，决定了那些子层卸载到CPU；C2：后端执行，无缝的集成了AMX CPU 、GPU。LIA允许所有的子层卸载到AMX使能的CPU上。 前端组件考虑了一些历史因素来决策最优的卸载策略，这些因素包括每个给定了batch size 和token length子层的每字节操作量、CPU-GPU传输数据量、CPU与GPU的计算吞吐量和内存带宽。将这些变量考虑在内，LIA最大化了资源利用率，最小化了端到端延迟LIA利用了大模型的一个特性：也就是每字节操作比会随着批次大小和输入长度动态波动。这使得在以后系统中，利用所给的批大小和输入长度决定卸载策略，能够得到最小的卸载延迟。 后端拓展了IPEX(这个拓展原本是用来给GPU或者单一的CPU进行加速)来无缝的集成CPU和GPU。后端拓展同样引入了进一步增强GPU内存和CPU-GPU计算资源的优化。 使用CXL内存时主要将模型参数卸载 到CXL内存，而DDR内存则存储中间值。 2.Background2.1 LLM inference 大模型由embeddingencoding layer组成，N个decoder layers ,LM head(linear 和 softmax). decoder layers主导了推理时间和内存消耗 一个decoder layer由多个子层组成，包括矩阵乘法、 层归一化、残差和sotfmax。 N个decoder 有一样的结构，不一样的参数。 模型接受了输入token序列，然后生成输出token，而后又把新输出的token作为输入，直到结束。 3 Performance Bottlenecks of Offloading Frameworks for LLM Inference","tags":[null,null]},{"title":"OCP China 2024 CXL 论坛学习笔记","path":"/notebooks/paper/OCP-China-2024-CXL-论坛学习笔记.html","content":"OCP China 2024 CXL论坛 学习笔记会议链接 阿里云 数据中心高性能Scale Up 互联系统趋势 孔阳 阿里云超高速互联负责人 胡文普 CXL部分 Scale Up 云的角度 关注两个计算 ： 通用计算、GPU计算 通用计算上： 考虑弹性分析：存储上-云盘技术 网络-CIPU网络虚拟化 内存上-CXL GPU上：大模型单次任务，数据并行、流水线并行、tensor并行、专家并行，都具有较高的带宽要求 CXL 机柜内的资源弹性实现需要满足高性能接口、资源共享、极致弹性、软件生态兼容性等要求。CXL特性符合这些要求。CXL将一致性访问从CPU内部拓展到CPU和内存之间，实现多服务器之间的互联。 CXL的演进 GIM ： P2P ： DSP与type3互联，type3可以分配给DSP或者host HBR-PBR:不止树状、星状、网络状 E3.s 热插拔，便于可拓展 JBOM 大容量 PEMEM ：支持2.0，满足redis实时性持久化等要求 基于 内存与CPU实现资源解耦 CXL在小数据上传输性能大大提高； intel: CXL on Intel Xeon@ Platform 赵森林 CXL overview CXL Specification Summary","tags":[null]},{"title":"龙蜥CXL讲解-高显扬","path":"/notebooks/paper/龙蜥CXL讲解-高显扬.html","content":"CXL技术介绍对视频链接的PPT搬运，方便快速阅读 高显杨 浪潮 综述 协议协议演进 1.1内存拓展 2.0内存赤化 3.0特性 CXL子协议 CXL设备 CXL Fabric 参考VPN，下边两张为单switch CXL RAS特性 内存热插拔？如何避免宕机 CXL初始化 RCRB CXL 1.1跑CXL2实现协议兼容 CXL方案 池化管理：FM中的bind和UnBind 业界方案 补充","tags":[null]},{"title":"OSDI'25 Tiered Memory Management Beyond Hotness","path":"/notebooks/paper/OSDI-25-Tiered-Memory-Management-Beyond-Hotness.html","content":"Tiered Memory Management Beyond Hotness Jinshu Liu Hamid Hadian Hanchen Xu Huaicheng Li Virginia Tech https://github.com/MoatLab/SoarAlto/ Virginia 这个MoatLab对CXL内存的研究很深入，之前的Pond(ASPLOS `23)、Melody（ASPLOS`25）都出自这个实验室 1.IntroductionHot data is not always performance-critical and can reside in the slow-tier without degrading performance . Latency mitigation techniques, such as memory-level parallelism (MLP), obscure the true cost of memory accesses 1.先前的工作通过启发式或者内存访问成本间接的反应MLP的影响，但是仍然缺少准确的MLP建模和指标。 2.现有的内存分层方法具有难以轻量化和不准确的特点。尤其是一开始先放置本地内存的方法本身就是次优的，并且激进的迁移策略会导致过分的迁移。 所以作者就定义了一个MLP影响的指标，AOL，并且用来辅助放置决策和迁移决策。 Propose Amortized Offcore Latency (AOL), a novel performance metric that accurately quantifies the performance impact of memory accesses by integrating memory latency and MLP. 放置决策策略SOAR基于AOL进行排序，然后以此决定放置。ALTO依靠AOL来进行页面提升的过滤，可以与TPP等策略进行结合。 2.Background and MotivationMLP反映等待内存控制器实现的内存请求数量。 high-MLP access patterns： array traversals。 low MLP：pointer-chasing with depedent requests 把两个类型的访存一起跑，然后不同的分层策略依照不同的策略跑了测性能，发现把数组访问的放到快速层，反而会使性能降低。 3.Memory Performance PredictionRelating Slow-tier Performance to CPU Stalls离线分析 Performance degradation on the slow-tier is predominantly caused by increased CPU stalls due to LLC misses, which we refer to as LLC-Stalls 强调区分LLC-miss和LLC-Stall 慢速层单次miss造成的延迟更长，假设相同的miss,慢速层也会有更长的LLS-Stall. 论文中讲到基于LLC-Stall来预测减速的误差低于4%，开源以后可以预测一下基于LLC-Miss的（考虑预取器的影响）。 LLC-Stalls for Performance Prediction在线预测 发现快速层发生CPUStall的在慢速层也会发生。 所以用P SLLCc来预测慢速层的减速。 AOL for Accurate Prediction进一步研究发现，P在低MLP的场景下准确，但是在高MLP的场景下并不准确、主要是忽略了MLP的影响，高MLP会减少长延迟的影响。 随着延迟的增加，MLP的延迟掩盖受益会降低。 因此定义了AOL： 指标 事件 含义 𝑠𝐿𝐿𝐶 CYCLE ACTIVITY.STALLS L3 MISS L3 Miss时导致的Stall c CPU_CLK_UNHALTED.THREAD 非Halt下的时钟周期数 A1 OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_DATA_RD L2 Miss 后、请求完成前，这些内存读取请求在 SQ 中等待的周期数换言之，每个时钟周期检查是否存在至少一个load请求，有就加1 A2 OFFCORE_REQUESTS_OUTSTANDING.DEMAND_DATA_RD L2 miss 后，每个周期有多少个未完成的 Demand Load 请求在SQ中等待，即：请求堆积的深度压力。 A3 OFFCORE REQUESTS.DEMAND DATA RD L2 miss 后，被发往 uncore 的 load 请求的次数。 延迟计算运用到了排队论中的Little`s法则:平均在系统中的项数 L = 到达率 λ × 平均响应时间 W 结合事件： L 某个时间段内，在 uncore 正在等待完成的请求数（单位：个），即 “每周期 outstanding 的请求数” λ 请求到达速率（单位：请求周期）≈ 总请求数 总周期数 W 每个请求在系统中停留的时间（单位：周期）→ 平均延迟 W = L / λ = 平均 outstanding 请求数 / 到达速率 根据排队理论，计算得1。 MLP是平均每个周期内多少个inflight内存请求，衡量内存访问的并行度。 A2代表总的堆积请求数，总的堆积请求数（A2）除以总的堆积周期数（A1),得到每个周期的平均inflight请求数。 1、2代入的AOL。 延迟除以并行请求，得到了一个并行请求下单个请求的延迟影响。 然后作者定义了减速模型：S P x K. k f(AOL)的函数是用SP与AOL进行分析，反向呈现出特定的渐进双曲线，反推出了其数学模型。 关于a和b，与硬件相关而与工作负载无关的常数，两个特定的场景（指针追踪、数组访问）能够推出（估计待定系数法） 分析： AOL增加时（MLP减小或者Latency增大），K趋近到上界1，S接近P,预测由LLC.stallc决定。相反（MLP增大或者Latency减少），K趋近下界0，S减小 有了预测模型以后，基于时间序列预测了 4.Soar: Rank-based Static Object Allocation现有的初次放置的分层方案目的是最大化利用快速层内存。 作者希望寻找一种方法最初就能精准放置内存，降低内存迁移开销。 读到这句话的时候这难受。。 挑战： While AOL-based prediction is effective at the workload level, it falls short for individual objects due to the semantic gap between architectural events and object-level memory accesses. 尽管AOL预测在workload级别能够表现得很好，但是却无法在单个变量上表现很好。 key insight： ​\tdistribute CPU stalls across objects proportionally to their relative access frequencies based on the observed MLP and latencies, thereby approximating each object’s performance impact to application performance accurately. Object-Level Performance Profiling​\tPeriodically collects and processes three types of metrics: object metadata via object tracking memory accesses via PEBS-based LLC-miss sampling temporal performance via AOL-based prediction ①-②Object TrackingFlow 通过LD_PRELOAD的方式拦截修改，记录五元组对象流 ③-④用PEBS记录LLC misses、访问时间戳和vaddr ⑤-⑥基于AOL预测性能 ⑦ 合并三个对象流，基于时间戳来判断地址，有了访问时间戳，可以计算访问次数以及访问比例。 ⑧将访存比例与AOL减速预测结合，计算减速得分 具体计算算法： 极端场景下并行少，MLP1，减速打分等于时间段减速P*访存比例R 高MLP时，缩小评分 低MLP时，放大评分 作者随后解释了怎样设计的factor，以及计算单位字节得分等。 Object Allocation依然是打分之后进行排序，topk 放置到快速层 影响排名不一定与请求顺序相同，如果打分低的先到了，后续打分高的请求到了会使得打分低的请求降级。 问题：是依据调用栈来进行分组对变量进行标识的，这样在一个函数内部进行内存分配时，大家调用栈都相同，这样并无法区分。 具体要看代码实现是否区分时空调用？ 特别指出可以与一些异构内存感知的内存分配器同时使用（memkind、Unified Memory Framework) Use Cases and Limitations1.HPC、在线服务这种长时间访问的应用，静态分配不再最优 2.假设对象是均匀的，对象内部的访问每一页频率都差不多。 5.Alto: AOL-based Adaptive Page Migrations现在方案的不足： 1.某些迁移没有。只是表面热 2.迁移开销很大，策略到单次迁移需要12us,访问到迁移中的页导致CPU stall 3.CXL与local的延迟和带宽都在缩小，迁移开销的影响就显得很大 4.冷页不是真的冷。 虽然用 AOL（Amortized Offcore Latency）来设计基于性能感知的页迁移（page-level migration）策略是很有前景的，但目前仍面临一些独特的挑战，特别是在如何用现有粗粒度硬件性能计数器（performance counters）准确估算单个内存页的性能影响方面。 方法很简单，就是用AOL辅助平时的方法决策一下： 而后讲了与TPP、Nomad、NBT等方法的集成。 6.Evaluation 关注1：CXL模拟方式：SKX lowering the uncore frequency and disabling cores on one NUMA node 关注2：workloads : GAPBS、ML、caching、SPEC2017 执行过程中的排序 ALTO效果。 不足与机会：1.从MLP的角度分析，和预取有关系吗？对于预取的影响（如果这个东西能量化，也能分析出很多东西） 2.并没有考虑区分读写比例的影响？ 3.本文中第4节提到的通过访存比例来分配内存slowdown的做法是否合理？ 是因为其指标是基于perf stat的，如果全部用perf record的方法是否会更加精确？这一点作者没有详细描述。 hard 4.全部放在运行时进行变量分析会不会发生采样不准确的问题，在NeoMem中也有？但是如何解决？可不可以结合NeoMem完全捕获数据流？ 基于采样，长生命周期的可能采集到，短生命周期取样。 机器的拓展性，针对SPX，其他的SPR、EMR的对应事件的拓展性替代如何，是否都只是PEBS？ 乱序校正的影响大不大？ 例如三段式的，第一次进行热点代码识别，第二次将热点代码全部卸载到CXL，然后利用Neomem捕获trace，从而准确感知，最后根据决策实现数据放置？ hard 5.结合内存分配器进行小变量页内集中优化，大变量的访问是否集中？不集中的话可以用perf采集地址，然后绘制访存直方图。","tags":[null]},{"title":"LLVM","path":"/notebooks/compiler_kernel/LLVM/LLVM.html","content":"入门llvm笔记 1.新旧新增pass流程 2CRTP（奇异递归模板模式）(以下内容基于AI生成后修改) 通常通过继承的方式实现单例模式也是这样，只是不知道叫这个名字 CRTP（Curiously Recurring Template Pattern，奇异递归模板模式）是C++中的一种高级模板编程技术，通过让一个类继承自以自身为模板参数的基类模板，实现静态多态或代码复用。 template typename Derivedclass Base /* 基类使用Derived类型 */ ;class MyClass : public BaseMyClass /* 派生类将自身作为模板参数传递给基类 */ ; 核心作用1. 静态多态（编译时多态） 动态多态（虚函数）的问题：运行时虚表查找导致性能开销。 CRTP的解决方案：基类在编译时通过模板参数直接调用派生类的方法。 template typename Derivedclass Animal public: void speak() static_castDerived*(this)-speakImpl(); // 编译时确定调用 ;class Cat : public AnimalCat public: void speakImpl() std::cout Meow ; ;class Dog : public AnimalDog public: void speakImpl() std::cout Woof ; ;// 使用AnimalCat cat;cat.speak(); // 输出 Meow（无虚函数开销） 2. 代码复用 基类可提供通用逻辑，派生类通过特化实现差异部分。 template typename Derivedclass Counter protected: static int count;public: Counter() ++count; static int getCount() return count; ;template typename Derivedint CounterDerived::count = 0;// 统计对象实例数的类class Widget : public CounterWidget ;class Gadget : public CounterGadget ;// 使用Widget w1, w2;Gadget g1;std::cout Widget::getCount(); // 输出 2std::cout Gadget::getCount(); // 输出 1 CRTP vs 虚函数 特性 CRTP 虚函数 多态时机 编译时 运行时 性能 无额外开销（直接调用） 虚表查找开销 灵活性 类型固定（模板参数需明确） 支持运行时类型动态替换 适用场景 高性能库、框架基础设施 需要运行时动态行为的情况 CRTP的典型应用场景 编译时多态：如数学库中的向量矩阵运算（Eigen库）。 对象计数：统计不同派生类的实例数量。 Mixin模式：为类动态添加功能（如LLVM的PassInfoMixin）。 链式调用：返回派生类引用以实现链式语法（return static_castDerived(*this);）。 3 LLVM与CPP实现DenseMap与stdMapdyn_cast与RTTI isadef-use user-value为什么llvm中User会继承value 4 支配树基本概念支配树 wiki 严格支配（Strict Domination）：如果 A 支配 B 且 A ≠ B，则称 A 严格支配 B。 立即支配者（Immediate Dominator, idom）：对于某个基本块 B，其被严格支配的所有基本块中，离 B 最近的那个称为 B 的立即支配者。 LLVM中的使用场景 死代码消除 (Dead Code Elimination) 死代码消除有多种方法多种粒度，这里block为粒度可以使用支配树方法 如果一个基本块不在入口块的支配树上，那说明它在某些路径上根本无法到达，因此可能是不可达代码。 if (!DT-dominates(entry, B)) B-eraseFromParent(); // 删除不可达的基本块 循环识别 (Loop Identification) LICM（Loop-Invariant Code Motion）是一种经典的循环优化策略。要安全地将一条指令从循环体中移出（外提到前置块中），必须保证其在所有循环入口都被执行。 判断某条指令的基本开是够被循环入口块支配，如果支配，则说明每次都会执行这条指令，可以外提 静态单赋值节点Φ 节点插入 关键类与关键接口 DominatorTree：核心支配树实现，llvm/IR/Dominators.h DominatorTreeWrapperPass：将支配树封装为分析Pass DominatorTreeAnalysis：支配树分析 DominatorTree DT = getAnalysisDominatorTreeWrapperPass().getDomTree(); 作用： invoke callCallBaseInvokeInst 处理器硬件计数pt LBR PEBS","tags":[null]},{"title":"Kernel-compile","path":"/notebooks/compiler_kernel/Linux kernel/Kernel-compile.html","content":"内核编译#查看当前内核版本uname -a#源码获取sudo apt-get install linux-sourcecd /usr/srctar xvf linux-source-*.tar.bz2cd linux-source-*#补丁应用patch -p1 /path/to/patch.diff#`-p1`选项可能需要根据补丁文件的格式进行调整。make menuconfig #或者oldconfig,本质是配置/usr/src/configmake -j$(nproc)#根据 .config 配置文件编译内核、内核模块和其他必要的文件，生成内核镜像（vmlinuz）和其他相关的文件#编译过程中可能出现每包头文件或者其他错误，可能是版本原因，针对报错解决即可#内核安装#安装过程可能会出现头文件报错或者缺库、或安装完成后缺库，只需要apt安装对应的库就好，make过程中会自动hook#安装内核模块(在运行时可以加载或卸载的模块)#从编译输出的目录（如 lib/modules/kernel_version/）,并执行 depmod 来生成模块依赖关系sudo make modules_install#内核镜像（如 vmlinuz）、配置文件（如 config）、符号表文件（如 System.map）和 initrd 镜像安装到 /boot 目录sudo make install#更新启动引导程序sudo update-grup 关于配置选项相关操作，参考：:star::star:Linux 内核动手编译实用指南 KGDB配置编译选项 参考资料KGDB原理分析及远程挂载调试ARM64内核 内核启动参数 kgdboc=ttyS0,115200 kgdbwait kgdbtcp=192.168.1.2:1234 kgdboc=ttyS0,115200：设置串口调试（可选） kgdbwait：启动时等待调试器连接 kgdbtcp=192.168.1.2:1234：被调试主机的IP和端口 永久修改：/etc/default/grub下GRUB_CMDLINE_LINUX变量 proxmox-boot-toolproxmox-boot-tool，一个脚本，设置启动内核、增删内核等,使用设置grub内核启动参数的方式不一定生效的情况下可以使用这个工具。 proxmox-boot-tool git clone https://git.proxmox.com/git/proxmox-kernel-helper.git之后 make deb会在当前路径下生成对应的deb包，apt install安装后可以正常使用/usr/sbin/proxmox-boot-tool 参考链接：https://kernelnewbies.org/KernelBuild","tags":[null]},{"title":"Neoperf_study","path":"/notebooks/compiler_kernel/Linux kernel/Neoperf-study.html","content":"NeoPerf study 本文主要为学习论文《NeoMem: HardwareSoftware Co-Design for CXL-Native Memory Tiering》的工作，分为三个部分，用户态、内核以及FPGA部分，内核开源仓库地址为： PKUZHOUlinux 代码基于linux内核代码6.0开始修改。 由于初步探索linux内核代码，所以没有按照自顶向下的视角分析代码，而是基于git提交记录，借助AI与互联网搜索，平铺遇到的相关知识。版本不断更新…… a naive neoprof drivercommit ID 9bd35383 本次主要在driver目录下提交了一个驱动neoperf: 主要是实现了一些对外设的IO操作 Linux内核配置文件KconfigKconfig文件用于定义内核配置菜单，这些菜单可以在编译内核时启用或禁用特定的功能。 #Kconfig文件config NEOPROF#定义了一个名为`NEOPROF`的内核配置选项，将在内核配置菜单中创建选项\tbool Enable Neoprofiler #bool类型，是否启用 default n #默认不启用 在内核编译过程中可以在Drivers下查找到 Kconfig有其独特的语法，也是可以一层一层包裹下去：menu、source、endmenu等组成了编译选项配置过程中的树状菜单 Kconfig设置对应的编译变量后，makefile指导构建编译的过程中会利用这些变量，从而实现选择性的编译 neoperf.h主要新增了四个接口，对neoprof设备（此处指Type2-CXL设备）进行访问： /* * The following functions are used to access the neoprof device*/u64 get_nr_hotpages(void);//获取当前系统中的热页数量u64* get_hotpages(void);//获取热页u64 get_hotness_threshold(void);//获取热度阈值void set_hotness_threshold(u64 threshold);//设置热度阈值 neoperf.c驱动开发hello worldneoperf.c 以下部分参考驱动开发知识：https://www.cnblogs.com/downey-blog/p/10500828.html module_init(neoprof_init);module_exit(neoprof_exit);MODULE_LICENSE(GPL v2);MODULE_AUTHOR(PKUZHOU);MODULE_DESCRIPTION(Neoprofiler Linux driver); io地址映射相关知识需要理解 IO端口的编址方式： 包括IO指令的端口映射方式、MMIO的统一内存映射方式 一些常见的IO操作函数 void * ioremap(unsigned long phys_addr, unsigned long size, unsigned long flags); //memset_io\\memcpy_fromio\\readb 参考：https://blog.csdn.net/do2jiang/article/details/5450839 neomem migration skeletoncommit ID 26cabad18 新增 NeoMem模块: migrate.c主要是提供接口migrate_misplaced_page_no_vma 调用一些mm中的内存操作函数，进行页面隔离、迁移 neomem.h eomem.c主要就是启动 neomem模块（调用core文件中启动守护进程） late_initcall()在内核启动后期适当时间执行，理解module_init等init宏的顺序，在include/linux/init.h中 linux设备驱动加载的先后顺序 neomem_core.c 内存中的各种分配函数 kthread_run内核线程 FPGA端侧模块结构顶层模块： cxltyp3_memexp_ddr4_top-ed_top_wrapper_typ3 ddr内存参数调整cxl ip考虑了不同ddr的，包括是否支持DBI、内存通道数量等。采用宏的方式区分，设置不同的方式时，需要对ip内通过宏定义来确定相关的内存参数，同时也需要在顶层模块对相关参数进行修改。 .qbluyxirhyrk{zoom:50%;} .afhbswqndgtc{zoom:50%;} .gcccmhmdxspk{zoom:50%;} 或者通过更改ip文件，重新生成新的IP文件夹 .xqviikzffrmr{zoom:50%;} quartusset_global_assignment -name OPTIMIZATION_MODE AGGRESSIVE COMPILE TIME 烧录模式AS Jtag ps 三种烧录模式 Neomem todo代码存在一些可以完善的地方： CXL地址采用硬编码，可以引入设备树或者其他检测CXL物理地址的工具进行优化，参考","tags":[null,null,null,null,null]},{"title":"linux脚本备忘录","path":"/notebooks/compiler_kernel/Linux kernel/linux脚本备忘录.html","content":"Linux脚本备忘录安装系统后的环境准备添加新用户 adduser #封装命令，处理完添加用户的全部过程 su到新用户显示username@hostname~$:useradd #底层命令，什么都没有加,su到新用户显示$:# 生成 8 位强密码PASSWORD=$(openssl rand -base64 6 | cut -c1-8)echo Password: $PASSWORD 用户添加sudo组 usermod -aG sudo new_user#加完以后记得:newgrp sudo#作用有3：#1.切换到指定的组上下文#2.即时生效组更改#3.启动一个子 shell 配置sshd#服务端安装apt install ssh-server#配置sudo vim /etc/ssh/sshd_config#重启服务sudo service restart sshd VimVim配置推荐 - ma6174 wget 47.93.11.51:88/install_vim.shbash install_vim.sh zsh#安装zshsudo apt install zsh#修改默认shell为zshchsh -s /bin/zsh#安装oh-my-zshsh -c $(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)##如果不成功，请执行下面两条命令，成功了就不需要做下面两条wget 47.93.11.51:88/install_zsh.shbash install_zsh.sh#安装zsh-syntax-highlightinggit clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM:-~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting 历史记录推荐命令插件##命令自动推荐，根据历史记录git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM:-~/.oh-my-zsh/custom/plugins/zsh-autosuggestions 命令自动补全##命令自动补全插件mkdir ~/.oh-my-zsh/plugins/incrwget http://mimosa-pudica.net/src/incr-0.2.zsh -O ~/.oh-my-zsh/plugins/incr/incr.plugin.zsh##目录自动跳转插件sudo apt install autojump .zshrc配置文件配置#插件添加zsh-syntax-highlightingplugins=(git zsh-syntax-highlighting) #设置终端颜色，提示符，及上一条指令返回码提示autoload -U colors colorsPROMPT=%$fg[red]%%n%$reset_color%@%$fg[blue]%%m %$fg[yellow]%%1~ %$reset_color%%# RPROMPT=[%$fg[yellow]%%?%$reset_color%]# Useful support for interacting with Terminal.app or other terminal programs[ -r /etc/zshrc_$TERM_PROGRAM ] . /etc/zshrc_$TERM_PROGRAMsource ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions/zsh-autosuggestions.plugin.zshsource /usr/share/autojump/autojump.shsource ~/.oh-my-zsh/plugins/incr/incr*.zsh ctags#安装sudo apt install ctags #建立索引ctags -I __THROW -I __attribute_pure__ -I __nonnull -I __attribute__ --file-scope=yes --langmap=c:+.h --languages=c,c++ --links=yes --c-kinds=+p --c++-kinds=+p --fields=+iaS --extra=+q -f ~/.vim/systags /usr/include/* /usr/include/x86_64-linux-gnu/sys/* /usr/include/x86_64-linux-gnu/bits/* /usr/include/arpa/* .vimrc添加索引 set tags+=~/systags 安装glibc-doc 使用以下命令安装 sudo apt install glibc-doc 常见路径hostname :/etc/hostname host: /ect/hosts tomcat#安装JDK8sudo apt install default-jre -ysudo apt install openjdk-11-jre-headless -ysudo apt install openjdk-8-jre-headless -y #sudo wget https://downloads.apache.org/tomcat/tomcat-8/v8.5.65/bin/apache-tomcat-8.5.65.tar.gzsudo wget https://mirrors.bfsu.edu.cn/apache/tomcat/tomcat-8/v8.5.73/bin/apache-tomcat-8.5.73.tar.gztar zxf apache-tomcat-8.5.73.tar.gzsudo mv apache-tomcat-8.5.73 /usr/local/tomcat#建立软连接sudo ln -s /usr/local/tomcat/bin/* /usr/local/sbin/#启动startup.sh start #端口检查netstat -anput | grep 8080#启动命令startup.sh start #//启动shutdown.sh #//关闭catalina.sh stop #//启动catalina.sh start #//关闭#关闭防火墙sudo ufw disable #tomcat 参数配置vim /usr/local/tomcat/conf/server.xml #.......Connector port=8081 protocol=HTTP/1.1 #将之前8080端口改成8081端口connectionTimeout=20000 # redirectPort=8443 /#目录修 # Host name=localhost appBase=/opt/www #将网站根目录改到/opt/www # unpackWARs=true autoDeploy=true#更改网站家目录，这里的ROOT必须大写，更改完成后需要重启sudo mkdir /opt/www/ROOT -p mysqlmysql 8.0下载 wget https://repo.mysql.com//mysql-apt-config_0.8.20-1_all.deb #MySQL 设置#密码sudo mysql -uroot use mysql;update user set authentication_string=PASSWORD(自定义密码) where User=root;update user set plugin=mysql_native_password where User =root;flush privileges;quit; 对于Linux和windows下字符集不兼容的情况，需要替换 • 把文件中的所有的utf8mb4_0900_ai_ci替换为utf8_general_ci• 以及utf8mb4替换为utf8• 如上图所示的位置，上图只是一部分，注意全部替换。 数据库导出 mysqldump -uroot -p c:ShareYunAlbum。sql 数据库导入 use ShareYunAlbum source ~/ShqreYunAlbum.sql 卸载mysql sudo apt purge mysql-* -ysudo rm -rf /etc/mysql/ /var/lib/mysqlsudo apt autoremovesudo apt autorecleansudo apt-get remove mysql-common dpkg -l |grep ^rc|awk print $2 |sudo xargs dpkg -P","tags":[null]},{"title":"llama.cpp","path":"/notebooks/other/大模型/llama-cpp.html","content":"Llama.cpp源码浅析ggml 源码结构学习入口函数简单以llama.cli作为推理学习的入口。其入口函数main位置为：llama.cpp/tool/main.cpp/main() 关键数据结构内存管理Arena 分配器“批发内存，零售指针，整单清场” 批发内存： 零售指针 整场清除： mmap模型加载流程： 核心步骤： llama_model_loadre 核心对象. 之后加载模型架构arch\\超参数hparams\\词表vocab\\元数据信息、以及张量tensors","tags":[null]},{"title":"大模型入门","path":"/notebooks/other/大模型/大模型入门.html","content":"大模型基础概念入门Transformer自注意力（Self-Attention）Q\\K\\V Q：目前关系的问题，当前token; K:token的标签 V:包含的信息 Q*K得到谁更重要，之后再乘以V得到这些重要的人说了什么信息。 除以根号dk与softmax是数学策略。 前馈网络FFN简单而言就是增加维度-增加信息-降低维度。-增加非线性变化。 层数影响逐层抽象。浅层学习低级特征（词性、局部语法），深层捕捉高级语义 输入空间—Layer 1—语法空间—Layer 2—语义空间—…—推理空间 单层表示： Layer(x)LayerNorm(x+FFN(LayerNorm(x+Attention(x)))) 多层复合： Model(x)LayerN(LayerN−1(…Layer1(x))) Prefill Decoder Prefill（预填充）：处理输入的所有已知 tokens，计算它们的隐藏状态并填充 KV Cache。 Decoder（解码）：基于 KV Cache 逐个生成新 token，直到结束。 1.为什么要提前计算所有的tokens？ 2.怎么计算kv的？ 3.什么是 token 的隐藏状态 4.QKV权重矩阵是干嘛的？ 5.什么是PD分离 优化策略计算图优化与算子融合 投机采样 FlashAttention","tags":[null]},{"title":"力扣刷题笔记乱序","path":"/notebooks/Interview/力扣刷题笔记(乱序).html","content":"状态规划专题119杨辉三角2 .dtnbjdcwwtbn{zoom:50%;} 关键点 ： 空间复杂度O(row)","tags":[null]}]