[{"title":"CUDA算子优化-GEMM","path":"/2025/09/06/CUDA 算子优化系列/CUDA算子优化-GEMM/","content":"TongkaioCUDA_Kernel_Samples: CUDA 算子手撕与面试指南","tags":["CUDA.C++"],"categories":["CUDA"]},{"title":"CUDA算子优化-Reduce","path":"/2025/09/06/CUDA 算子优化系列/CUDA算子优化-Reduce/","content":"本文是对官方reduce优化的精简，方便个人复习,详细回顾参考知乎深入浅出系列 leetGPU 问题解决 展示 reduce的7种优化 V0_0 naive跨步相加，非全局内存访问， __global__ void reduce_naive(float *g_idata, float *g_odata, unsigned int n) unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx = n) return; // 直接在全局内存上进行跨步归约 for (unsigned int stride = 1; stride blockDim.x; stride *= 2) // 确保所有前一轮的写入对下一轮可见 if (threadIdx.x % (2 * stride) == 0) g_idata[idx] += g_idata[idx + stride]; __syncthreads(); // 一个线程块的结果写回全局内存 if (threadIdx.x == 0) g_odata[blockIdx.x] = g_idata[blockIdx.x * blockDim.x]; 整个过程都在读写缓慢的全局内存，延迟很高，O(N)次操作需要访问全局内存O(NlogN)次全局内存，效率低 V0 shared_memory.ihdbxsphncdg{zoom:50%;} __global__ void reduce0(int *g_idata, int *g_odata) extern __shared__ int sdata[]; // each thread loads one element from global to shared mem\tunsigned int tid = threadIdx.x;\tunsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\tsdata[tid] = g_idata[i]; __syncthreads();\t// do reduction in shared mem\tfor(unsigned int s=1; s blockDim.x; s *= 2) if (tid % (2*s) == 0) sdata[tid] += sdata[tid + s]; __syncthreads(); // write result for this block to global mem\tif (tid == 0) g_odata[blockIdx.x] = sdata[0]; 问题： 会造成线程束分化，同一个warps内执行的操作不一致 V1 线程束分化__global__ void reduce1(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*blockDim.x+threadIdx.x; unsigned int tid=threadIdx.x; sdata[tid]=d_in[i]; __syncthreads(); // do reduction in shared mem for(unsigned int s=1; sblockDim.x; s*=2) int index = 2*s*tid; if(index blockDim.x)//不同线程束执行不同 sdata[index]+=sdata[index+s]; __syncthreads(); // write result for this block to global mem if(tid==0)d_out[blockIdx.x]=sdata[tid]; 本质，将区别通过index转换迁移到线程束之间而不是线程束内部，以前是每个线程束中都会出现if else，限制是一个线程束中的程序都一起执行或者一起不执行（最后几轮除外） 继续假定block中存在256个thread，即拥有256328个warp。当进行第1次迭代时，0-3号warp的indexblockDim.x， 4-7号warp的indexblockDim.x。对于每个warp而言，都只是进入到一个分支内，所以并不会存在warp divergence的情况。当进行第2次迭代时，0、1号两个warp进入计算分支。当进行第3次迭代时，只有0号warp进入计算分支。当进行第4次迭代时，只有0号warp的前16个线程进入分支。此时开始产生warp divergence。通过这种方式，我们消除了前3次迭代的warp divergence。 来自知乎《深入浅出GPU优化系列》 V2 bank冲突0号和32号元素冲突 __global__ void reduce2(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*blockDim.x+threadIdx.x; unsigned int tid=threadIdx.x; sdata[tid]=d_in[i]; __syncthreads(); // do reduction in shared mem for(unsigned int s=blockDim.x/2; s0; s=1) if(tid s) sdata[tid]+=sdata[tid+s]; __syncthreads(); // write result for this block to global mem if(tid==0)d_out[blockIdx.x]=sdata[tid]; 让一开始的访存跨度最大，恰好在元素多的时候错开，一个warp中不会同时访问同一个bank 问题： 一半的线程是空闲的 V3 空闲线程优化__global__ void reduce3(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*(blockDim.x*2)+threadIdx.x; //核心，每个block处理两个block unsigned int tid=threadIdx.x; sdata[tid]=d_in[i] + d_in[i+blockDim.x]; __syncthreads(); // do reduction in shared mem for(unsigned int s=blockDim.x/2; s0; s=1) if(tid s) sdata[tid]+=sdata[tid+s]; __syncthreads(); // write result for this block to global mem if(tid==0)d_out[blockIdx.x]=sdata[tid]; V4 展开最后一次计算减少同步__device__ void warpReduce(volatile float* cache,int tid) //volatile 的作用：禁止编译器优化，每次访问都必须真正从共享内存里取值/写值,防止缓存到寄存器里，然后重复使用寄存器的值，而不是每次都从共享内存读取 //32到1是跨度s cache[tid]+=cache[tid+32]; cache[tid]+=cache[tid+16]; cache[tid]+=cache[tid+8]; cache[tid]+=cache[tid+4]; cache[tid]+=cache[tid+2]; cache[tid]+=cache[tid+1];__global__ void reduce4(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*(blockDim.x*2)+threadIdx.x; unsigned int tid=threadIdx.x; sdata[tid]=d_in[i] + d_in[i+blockDim.x]; __syncthreads(); // do reduction in shared mem for(unsigned int s=blockDim.x/2; s32; s=1) if(tid s) sdata[tid]+=sdata[tid+s]; __syncthreads(); // write result for this block to global mem if(tid32)warpReduce(sdata,tid); if(tid==0)d_out[blockIdx.x]=sdata[tid]; 一个SIMD单元工作时，避免__syncthreads同步 V5 暴力for循环展开其实个人无法理解，现代版本中感觉不需要了 template unsigned int blockSize__device__ void warpReduce(volatile float* cache,int tid) if(blockSize = 64)cache[tid]+=cache[tid+32]; if(blockSize = 32)cache[tid]+=cache[tid+16]; if(blockSize = 16)cache[tid]+=cache[tid+8]; if(blockSize = 8)cache[tid]+=cache[tid+4]; if(blockSize = 4)cache[tid]+=cache[tid+2]; if(blockSize = 2)cache[tid]+=cache[tid+1];template unsigned int blockSize__global__ void reduce5(float *d_in,float *d_out) __shared__ float sdata[THREAD_PER_BLOCK]; //each thread loads one element from global memory to shared mem unsigned int i=blockIdx.x*(blockDim.x*2)+threadIdx.x; unsigned int tid=threadIdx.x; sdata[tid]=d_in[i] + d_in[i+blockDim.x]; __syncthreads(); // do reduction in shared mem if(blockSize=512) if(tid256) sdata[tid]+=sdata[tid+256]; __syncthreads(); if(blockSize=256) if(tid128) sdata[tid]+=sdata[tid+128]; __syncthreads(); if(blockSize=128) if(tid64) sdata[tid]+=sdata[tid+64]; __syncthreads(); // write result for this block to global mem if(tid32)warpReduceblockSize(sdata,tid); if(tid==0)d_out[blockIdx.x]=sdata[tid]; V6版本和文章中讲的有些差距，本质一个线程处理多个数。 template unsigned int blockSize, int NUM_PER_THREAD__global__ void reduce6(float *d_in,float *d_out, unsigned int n) __shared__ float sdata[blockSize]; // each thread loads NUM_PER_THREAD element from global to shared mem unsigned int tid = threadIdx.x; unsigned int i = blockIdx.x * (blockSize * NUM_PER_THREAD) + threadIdx.x; sdata[tid] = 0; #pragma unroll for(int iter=0; iterNUM_PER_THREAD; iter++) sdata[tid] += d_in[i+iter*blockSize]; __syncthreads(); // do reduction in shared mem if (blockSize = 512) if (tid 256) sdata[tid] += sdata[tid + 256]; __syncthreads(); if (blockSize = 256) if (tid 128) sdata[tid] += sdata[tid + 128]; __syncthreads(); if (blockSize = 128) if (tid 64) sdata[tid] += sdata[tid + 64]; __syncthreads(); if (tid 32) warpReduceblockSize(sdata, tid); // write result for this block to global mem if (tid == 0) d_out[blockIdx.x] = sdata[0]; reduce6THREAD_PER_BLOCK, NUM_PER_THREADGrid,Block(d_a, d_out, N); V7 shuffle优化todo How_to_optimize_in_GPUreducereduce_v7_shuffle.cu at master · Liu-xiandongHow_to_optimize_in_GPU 采用了shuffle指令之后，warp内的线程可以直接对其他线程的寄存器进行访存。通过这种方式可以减少访存的延时。 #define THREAD_PER_BLOCK 256#define WARP_SIZE 32template unsigned int blockSize__device__ __forceinline__ float warpReduceSum(float sum) if (blockSize = 32)sum += __shfl_down_sync(0xffffffff, sum, 16); // 0-16, 1-17, 2-18, etc. if (blockSize = 16)sum += __shfl_down_sync(0xffffffff, sum, 8);// 0-8, 1-9, 2-10, etc. if (blockSize = 8)sum += __shfl_down_sync(0xffffffff, sum, 4);// 0-4, 1-5, 2-6, etc. if (blockSize = 4)sum += __shfl_down_sync(0xffffffff, sum, 2);// 0-2, 1-3, 4-6, 5-7, etc. if (blockSize = 2)sum += __shfl_down_sync(0xffffffff, sum, 1);// 0-1, 2-3, 4-5, etc. return sum;template unsigned int blockSize, int NUM_PER_THREAD__global__ void reduce7(float *d_in,float *d_out, unsigned int n) float sum = 0; // each thread loads one element from global to shared mem unsigned int tid = threadIdx.x; #pragma unroll //线程级局部规约 for(int iter=0; iterNUM_PER_THREAD; iter++) sum += d_in[i+iter*blockSize]; // Shared mem for partial sums (one per warp in the block) static __shared__ float warpLevelSums[WARP_SIZE]; const int laneId = threadIdx.x % WARP_SIZE; const int warpId = threadIdx.x / WARP_SIZE; sum = warpReduceSumblockSize(sum); if(laneId == 0 )warpLevelSums[warpId] = sum; __syncthreads(); // read from shared memory only if that warp existed sum = (threadIdx.x blockDim.x / WARP_SIZE) ? warpLevelSums[laneId] : 0; // Final reduce using first warp if (warpId == 0) sum = warpReduceSumblockSize/WARP_SIZE(sum); // write result for this block to global mem if (tid == 0) d_out[blockIdx.x] = sum;reduce7THREAD_PER_BLOCK, NUM_PER_THREADGrid,Block(d_a, d_out, N); 优化总结 使用共享内存 减少线程束分化（差异保留在线程束之间） 减少bank冲突；变换角标，错位（+1与跨度错位） 减少空闲线程，一个线程取两个元素 减少__syncthreads同步，一个SIMD内可以不使用 合理设置block数量 使用shuffle指令","tags":["CUDA.C++"],"categories":["CUDA"]},{"title":"CUDA算子优化-SoftMax","path":"/2025/09/06/CUDA 算子优化系列/CUDA算子优化-SoftMax/","content":"TongkaioCUDA_Kernel_Samples: CUDA 算子手撕与面试指南","tags":["CUDA.C++"],"categories":["CUDA"]},{"title":"GGML源码浅析(1) 基础数据结构、内存管理、后端管理","path":"/2025/08/01/GGML源码浅析/","content":"GGML 源码浅析（1）前言1.阅读路线 ​\t1.内存管理：不使用后端时（参见examplesimple-ctx）介绍ggml中的重要数据结构以及内存管理 ​\t2.后端的设计逻辑 3.基于gpt-2学习模型构建过程中权重与kv-cache管理 2.重点 ​\t1.关于ggml-context的相关数据结构，需要理解ggml-contxt中内存分配结构，可以参看其中给的几张图 ​\t2.通过输出结果反推构建计算图 ​\t3.后端设计中，一定用面向对象的方式来思考相关后端实现，其实就是用c实现了C++的面向对象，自己实现了虚函数表，私有成员变量（void*) ​\t4.一次操作的逻辑 大佬教程 深入理解GGML（一）模型和计算图 - 知乎 1.核心数据结构1.1ggml_contextstruct ggml_context size_t mem_size; void* mem_buffer; //cpu采用由poxsi_align分配的内存对齐内存，对于不同的后端，分配不同的内存 bool mem_buffer_owned;//外部分配还是属于本处，决定内存分配权？ bool no_alloc; bool no_alloc_save; // this is used to save the no_alloc state when using scratch buffers int n_objects; struct ggml_object * objects_begin;//ggml_object维护的是一个链表 struct ggml_object * objects_end; struct ggml_scratch scratch; struct ggml_scratch scratch_save;; struct ggml_object size_t offs; size_t size; struct ggml_object * next; enum ggml_object_type type; char padding[4]; ; ggml_context是最核心的数据结构，所有的张量、计算图都依赖于这个数据结构: mem_size表示ggml_init时分配一块多大的内存，后续的张量都从这块内存中分配空间，避免反复的malloc mem_buffer_owned表示这个mem_buffer是外部传进来的还是自己分配的，决定分配权，避免误释放 。 no_malloc是表示张量分配内存的时候是真的分配内存还是仅仅用于占位 no_alloc_save使用了scratch会强制开启内存，所以需要暂存一下no_malloc scratch是一块临时内存，用于存放中间的临时结果、缓存 scratch_save暂存scatch的状态 n_objects以及两个指针维护了一个对象双向链表，记录所有已创建的对象，可以看到这个ggml_object主要维护了偏移offs和size,容易理解实际数据就是分配在ggml_context的buffer中，而要找到这些数据就通过object链表来进行查找与分配。 ggml_object这个结构体本本身也是分配在ggml_context.mem_buffer上的，之后跟着对应的数据。 ggml-object-ggml_tensor-tensor_data 这一部分的实现具体查看ggml_new_object static struct ggml_object * ggml_new_object(struct ggml_context * ctx, enum ggml_object_type type, size_t size) // always insert objects at the end of the contexts memory pool struct ggml_object * obj_cur = ctx-objects_end; const size_t cur_offs = obj_cur == NULL ? 0 : obj_cur-offs; const size_t cur_size = obj_cur == NULL ? 0 : obj_cur-size; const size_t cur_end = cur_offs + cur_size; // align to GGML_MEM_ALIGN size_t size_needed = GGML_PAD(size, GGML_MEM_ALIGN); char * const mem_buffer = ctx-mem_buffer; struct ggml_object * const obj_new = (struct ggml_object *)(mem_buffer + cur_end); if (cur_end + size_needed + GGML_OBJECT_SIZE ctx-mem_size) GGML_PRINT(%s: not enough space in the contexts memory pool (needed %zu, available %zu) , __func__, cur_end + size_needed, ctx-mem_size); assert(false); return NULL; *obj_new = (struct ggml_object) .offs = cur_end + GGML_OBJECT_SIZE, .size = size_needed, .next = NULL, .type = type, ; GGML_ASSERT_ALIGNED(mem_buffer + obj_new-offs); if (obj_cur != NULL) obj_cur-next = obj_new; else // this is the first object in this context ctx-objects_begin = obj_new; ctx-objects_end = obj_new; //printf(%s: inserted new object at %zu, size = %zu , __func__, cur_end, obj_new-size); return obj_new; 实际使用过程中，可以看到所有的操作都挂靠到ggml_context上 内存分配： struct ggml_init_params params = .mem_size = 64 * 1024 * 1024, .mem_buffer = malloc(mem_size), .no_alloc = false;struct ggml_context * ctx = ggml_init(params); 张量分配： struct ggml_tensor * a = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 10); ​\t可以查看ggml.c/ggml_new_tensor_impl()具体实现，最终分配的tensor的data,指针指向的数据还是ggml_contxt.mem_buf中的对应偏移量。 计算操作： struct ggml_tensor * c = ggml_add(ctx, a, b); 执行计算图： struct ggml_cgraph graph = ggml_build_forward(c);ggml_graph_compute_with_ctx(ctx, graph, n_threads); 下边小红书博主**TransormerX**绘制的这几张图 清晰 美观的展示了内存分配情况： .rukpuhzkixwe{zoom:80%;} .wbojeilyatoh{zoom:67%;} .slxvfjefcmmk{zoom:50%;} 除了图中绘制的tensor是按照这样的内存分布以外，其他类型graph、workbuffer等类型的object都是采用这样的内存排布，即：object结构体-对应类型结构体（tensor\\graph)-相应data 比如图的存储就是object-ggml_cgraph(结构体本身)-节点叶子等指针数组 WORK_BUFFER就是object–work_data （直接被cplan中的指针所指） 1.2ggml_state整个ggml程序运行过程中有一个全局的g_state. static struct ggml_state g_state;#define GGML_MAX_CONTEXTS 64struct ggml_state struct ggml_context_container contexts[GGML_MAX_CONTEXTS]; struct ggml_numa_nodes numa;;struct ggml_context_container bool used; struct ggml_context context;;struct ggml_numa_nodes enum ggml_numa_strategy numa_strategy; struct ggml_numa_node nodes[GGML_NUMA_MAX_NODES]; uint32_t n_nodes; uint32_t total_cpus; // hardware threads on system uint32_t current_node; // node on which main process is execting#if defined(__gnu_linux__) cpu_set_t cpuset; // cpuset from numactl#else uint32_t cpuset; // no NUMA support outside of Linux at this time. Use a portable datatype#endif; ggml_state中包含了一个ggml_context（额外加一个used标识而已）数组。 在第一次调用ggml_init时，会对g_state进行初始化。 //位于ggml.c/ggml_initif (is_first_call) //.....初始化激活函数表 //初始化g_state g_state = (struct ggml_state) /*.contexts =*/ 0 , /*.numa =*/ .n_nodes = 0, .total_cpus = 0, , ; for (int i = 0; i GGML_MAX_CONTEXTS; ++i) g_state.contexts[i].used = false; 于是可以理解到，程序运行时，会在全局静态内存区分配一个g_state,包含了一个g_context数组，每次ggml_init的时候就去g_state中找一个没有使用的g_context,获得其指针后进行每个字段的初始化以及填充，之后就依靠这个ggml_context进行一次一次。 目前个人看代码觉得每次模型执行只需要一个ggml_conterxt，但是给了一个64个ggml_context组成的数组，是因为可能需要支持模型的并行（这个观点参考GPT） 1.3ggml_tensor// n-dimensional tensor struct ggml_tensor enum ggml_type type; //数据类型 GGML_DEPRECATED(enum ggml_backend_type backend, use the buffer type to find the storage location of the tensor); struct ggml_backend_buffer * buffer;//一个表示数据实际存储的内存后端，多态实现，一个重要的结构体 int64_t ne[GGML_MAX_DIMS]; // number of elements//每一维的元素个数 size_t nb[GGML_MAX_DIMS]; // stride in bytes: //每一位走到下一个元素的字节数 // nb[0] = ggml_type_size(type) // nb[1] = nb[0] * (ne[0] / ggml_blck_size(type)) + padding // nb[i] = nb[i-1] * ne[i-1] // compute data enum ggml_op op;//这个tensor是通过什么操作计算得到的 // op params - allocated as int32_t for alignment int32_t op_params[GGML_MAX_OP_PARAMS / sizeof(int32_t)]; //存储当前op的参数op = CONV2D 时，可能存储 kernel size、stride int32_t flags; struct ggml_tensor * grad; //如果当前张量支持反向传播，则存储梯度张量 struct ggml_tensor * src[GGML_MAX_SRC]; //当前张量是某个张量的运算结果，则记录输入张量 // source tensor and offset for views struct ggml_tensor * view_src; size_t view_offs; void * data;//实际张量的数据指针，通常指向ggml_context char name[GGML_MAX_NAME]; void * extra; // extra things e.g. for ggml-cuda.cu // char padding[4]; ; 这里有一个比较重要的概念是view,view不会信分配内存。一个简单的例子：如果一个短向量是一个长向量的子向量，可以理解为短向量是长向量的子向量，也就是长向量的视图，分配短向量时，可以利用长向量已经分配的内存，从而避免了内存的分配。 1.4 ggml_cgraphstruct ggml_cgraph int size; int n_nodes;//节点 int n_leafs;//叶子节点数 struct ggml_tensor ** nodes;//中间节点数 struct ggml_tensor ** grads; struct ggml_tensor ** leafs;//叶子节点 struct ggml_hash_set visited_hash_set;//反向构建中 hash去重 enum ggml_cgraph_eval_order order; ; 2.核心操作2.1模型加载gguf结构与解析gguf定义了模型的权重保存方式，以下为gguf的结构 主要可以分为以下4部分： 1.header 包含模式，tensor数量、kv元数据数、版本等 2.模型元数据（KV表示） 3.每个tensor的信息（offset等，不包含tensor的值） 4.tensor的值 文件ggml.c中，定义了gguf相关的结构体，其中gguf_context对应于一个文件，包含header、kv、tensor_info等 union gguf_value uint8_t uint8; int8_t int8; uint16_t uint16; int16_t int16; uint32_t uint32; int32_t int32; float float32; uint64_t uint64; int64_t int64; double float64; bool bool_; struct gguf_str str; struct enum gguf_type type; uint64_t n; // GGUFv2 void * data; arr;;struct gguf_kv struct gguf_str key; enum gguf_type type; union gguf_value value;;struct gguf_header char magic[4]; uint32_t version; uint64_t n_tensors; // GGUFv2 uint64_t n_kv; // GGUFv2;struct gguf_tensor_info struct gguf_str name; uint32_t n_dims; uint64_t ne[GGML_MAX_DIMS]; enum ggml_type type; uint64_t offset; // offset from start of `data`, must be a multiple of `ALIGNMENT` // for writing API const void * data; size_t size;;struct gguf_context struct gguf_header header; struct gguf_kv * kv; struct gguf_tensor_info * infos; size_t alignment; size_t offset; // offset of `data` from beginning of file size_t size; // size of `data` in bytes //uint8_t * padding; void * data;; ggml.c/gguf_init_from_file实现了从文件加载模型到结构体中 2.2计算图（前向图）构建以simple-ctx为例说明计算图构建流程 struct ggml_cgraph * build_graph(const simple_model model) struct ggml_cgraph * gf = ggml_new_graph(model.ctx); // result = a*b^T struct ggml_tensor * result = ggml_mul_mat(model.ctx, model.a, model.b); ggml_build_forward_expand(gf, result); return gf;struct ggml_tensor * ggml_mul_mat( struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b) GGML_ASSERT(ggml_can_mul_mat(a, b)); GGML_ASSERT(!ggml_is_transposed(a)); bool is_node = false; if (a-grad || b-grad) is_node = true; const int64_t ne[4] = a-ne[1], b-ne[1], b-ne[2], b-ne[3] ; struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, 4, ne); result-op = GGML_OP_MUL_MAT; result-grad = is_node ? ggml_dup_tensor(ctx, result) : NULL; result-src[0] = a; result-src[1] = b; return result; ggml_new_graph会在ggml-contxt中相关的内存区域分配ggml_cgraph的内存，进行初始化； 而后进行实际的算子构建，这个过程每一步得到的输出张量会记录对应的输入、以及操作类型，参照上边的ggml_tensor的数据结构 最后核心的函数是ggml_build_forward_expand，这个过程根据输出的tensor，反向添加graph中的计算节点。 即流程为手动设置最终输出tensor的过程，利用tensor中的变量反向构建静态输出图，成功在graph中添加不同的节点，完成graph的维护 具体构建过程见：ggml.c/ggml_visit_parents简言之就是后序遍历+hash去重，填充cgraph中的相关指针 static void ggml_visit_parents(struct ggml_cgraph * cgraph, struct ggml_tensor * node) if (node-grad == NULL) // this usually happens when we generate intermediate nodes from constants in the backward pass // it can also happen during forward pass, if the user performs computations with constants if (node-op != GGML_OP_NONE) //GGML_PRINT_DEBUG(%s: warning: node %p has no grad, but op %d , __func__, (void *) node, node-op); // check if already visited if (ggml_hash_insert(cgraph-visited_hash_set, node) == GGML_HASHSET_ALREADY_EXISTS) return; for (int i = 0; i GGML_MAX_SRC; ++i) const int k = (cgraph-order == GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT) ? i : (cgraph-order == GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT) ? (GGML_MAX_SRC-1-i) : /* unknown order, just fall back to using i*/ i; if (node-src[k]) ggml_visit_parents(cgraph, node-src[k]); if (node-op == GGML_OP_NONE node-grad == NULL) // reached a leaf node, not part of the gradient graph (e.g. a constant) GGML_ASSERT(cgraph-n_leafs cgraph-size); if (strlen(node-name) == 0) ggml_format_name(node, leaf_%d, cgraph-n_leafs); cgraph-leafs[cgraph-n_leafs] = node; cgraph-n_leafs++; else GGML_ASSERT(cgraph-n_nodes cgraph-size); if (strlen(node-name) == 0) ggml_format_name(node, node_%d, cgraph-n_nodes); cgraph-nodes[cgraph-n_nodes] = node; if (cgraph-grads) cgraph-grads[cgraph-n_nodes] = node-grad; cgraph-n_nodes++; 后向图的构建过程更加复杂一些，会同时保存相关的梯度信息。 后向图的构建依赖前向图，后向图靠拷贝了前向计算图， 2.3计算核心函数 enum ggml_status ggml_graph_compute_with_ctx(struct ggml_context * ctx, struct ggml_cgraph * cgraph, int n_threads) struct ggml_cplan cplan = ggml_graph_plan(cgraph, n_threads, NULL); struct ggml_object * obj = ggml_new_object(ctx, GGML_OBJECT_TYPE_WORK_BUFFER, cplan.work_size); cplan.work_data = (uint8_t *)ctx-mem_buffer + obj-offs; return ggml_graph_compute(cgraph, cplan); 主要步骤为， 1.构建执行计划ggml_graph_plan,生成执行顺序和任务计划、所有算子中的预计分配内存大小、线程数量。线程分配数量由ggml_get_n_tasks中的一个switch case决定。其中有一段work_size += CACHE_LINE_SIZE * n_threads;的代码是避免多个线程访问同一个缓存和，造成伪共享。一些可以被多线程执行的算子计算预计内存分配的时候还会乘以线程数，保证不会导致额外的内存同步开销。 2.ggml_new_object生成任务缓冲区,ggml_context中 。 3.执行计算图ggml_graph_compute ggml_graph_compute 执行ggml_graph_compute时，首先初始化或者设置ggml_threadpool中的相关参数，包括cplan等 只有调用ggml_graph_compute_thread进行多线程处理，启用omp进行多线程处理，否则单线程 其中遍历cgraph中的每个算子节点，通过ggml_compute_forward转发到不同的算子进行计算，每个算子内部区分不同的精读，转发到不同的精度处理函数，最后不同的精度计算算子函数 通过ith th考虑线程分配，内存存取 将线程划分计算的权利交给了算子自身来执行。 例如以f16_add为例,通过ith划分举证的行和列，每个线程计算不同的小矩阵，实现并行化。 static void ggml_compute_forward_add_f16_f16( const struct ggml_compute_params * params, struct ggml_tensor * dst) const struct ggml_tensor * src0 = dst-src[0]; const struct ggml_tensor * src1 = dst-src[1]; GGML_ASSERT(ggml_are_same_shape(src0, src1) ggml_are_same_shape(src0, dst)); const int ith = params-ith; const int nth = params-nth; const int nr = ggml_nrows(src0); GGML_TENSOR_BINARY_OP_LOCALS GGML_ASSERT(src0-type == GGML_TYPE_F16); GGML_ASSERT(src1-type == GGML_TYPE_F16); GGML_ASSERT(dst-type == GGML_TYPE_F16); GGML_ASSERT( nb0 == sizeof(ggml_fp16_t)); GGML_ASSERT(nb00 == sizeof(ggml_fp16_t)); // rows per thread const int dr = (nr + nth - 1)/nth; // row range for this thread const int ir0 = dr*ith; const int ir1 = MIN(ir0 + dr, nr); if (nb10 == sizeof(ggml_fp16_t)) for (int ir = ir0; ir ir1; ++ir) // src0, src1 and dst are same shape = same indices const int i3 = ir/(ne2*ne1); const int i2 = (ir - i3*ne2*ne1)/ne1; const int i1 = (ir - i3*ne2*ne1 - i2*ne1); ggml_fp16_t * dst_ptr = (ggml_fp16_t *) ((char *) dst-data + i3*nb3 + i2*nb2 + i1*nb1); ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0-data + i3*nb03 + i2*nb02 + i1*nb01); ggml_fp16_t * src1_ptr = (ggml_fp16_t *) ((char *) src1-data + i3*nb13 + i2*nb12 + i1*nb11); for (int i = 0; i ne0; i++) dst_ptr[i] = GGML_FP32_TO_FP16(GGML_FP16_TO_FP32(src0_ptr[i]) + GGML_FP16_TO_FP32(src1_ptr[i])); else // src1 is not contiguous GGML_ABORT(fatal error); 3.后端实现快速入门技巧： 用面向对象思想来阅读！ 3.1相关数据结构ggml_backend查看examplesimple代码时，后端的模型中增加了一个ggml_backend结构。 ggml_backend可以理解为一个设备管理器 struct ggml_backend ggml_guid_t guid;//用于唯一标识 backend 实例，便于注册、查找或调试。 struct ggml_backend_i iface;//定义后端行为的函数虚表，包含后端行为的函数指针 ggml_backend_context_t context;//void *，具体内容依赖后端实现; ggml_backend_bufferstruct ggml_backend_buffer struct ggml_backend_buffer_i iface; ggml_backend_buffer_type_t buft; ggml_backend_buffer_context_t context; size_t size; enum ggml_backend_buffer_usage usage; ; 又包含了ggml_backend_buffer_type struct ggml_backend_buffer_type struct ggml_backend_buffer_type_i iface; ggml_backend_buffer_type_context_t context; ; 这里结构体比较多，看起来比较复杂，用这篇博客中的图片描述是这样的 .kdzykbyphfii{zoom: 80%;} 但是如果用面向对象的思想来理解，一切都很简单。 最开始的初衷是有一个后端基类，不同类型的后端基于此派生，而后端包含有不同的buffer，因此创建了后端buffer一组基类，并作为后端基类的成员变量，相同的方式有了后端buffer-type类 不同的后端有不同的成员变量，也就是ctx，通过void*实现 3.2CPU后端示例讲解load model加载模型之前先根据不同的宏启用不同的后端初始化函数， 之后创建对应的ggml_contxt， 创建向量时，首先会将向量存储在CPU， 之后通过ggml_backend_alloc_ctx_tensors 创建对应的后端buffer,然后通过ggml_backend_tensor_set将向量从CPU内存搬到后端内存 void load_model(simple_model model, float * a, float * b, int rows_A, int cols_A, int rows_B, int cols_B) // initialize the backend#ifdef GGML_USE_CUDA fprintf(stderr, %s: using CUDA backend , __func__); model.backend = ggml_backend_cuda_init(0); // init device 0 if (!model.backend) fprintf(stderr, %s: ggml_backend_cuda_init() failed , __func__); #endif#ifdef GGML_USE_METAL fprintf(stderr, %s: using Metal backend , __func__); ggml_backend_metal_log_set_callback(ggml_log_callback_default, nullptr); model.backend = ggml_backend_metal_init(); if (!model.backend) fprintf(stderr, %s: ggml_backend_metal_init() failed , __func__); #endif // if there arent GPU Backends fallback to CPU backend if (!model.backend) model.backend = ggml_backend_cpu_init(); int num_tensors = 2; struct ggml_init_params params /*.mem_size =*/ ggml_tensor_overhead() * num_tensors, /*.mem_buffer =*/ NULL, /*.no_alloc =*/ true, ; // create context model.ctx = ggml_init(params); // create tensors model.a = ggml_new_tensor_2d(model.ctx, GGML_TYPE_F32, cols_A, rows_A); model.b = ggml_new_tensor_2d(model.ctx, GGML_TYPE_F32, cols_B, rows_B); // create a backend buffer (backend memory) and alloc the tensors from the context model.buffer = ggml_backend_alloc_ctx_tensors(model.ctx, model.backend); // load data from cpu memory to backend buffer ggml_backend_tensor_set(model.a, a, 0, ggml_nbytes(model.a)); ggml_backend_tensor_set(model.b, b, 0, ggml_nbytes(model.b)); ggml_backend_cpu_init ggml_backend_cpu_init XXX_backend_cpu_init负责创建对应的ggml_backend_t结构体 可以看到结构体中的ctx是每个后端有一个自己的结构体 虚函数表interface是全局定义了每个后端的表，如cpu_backend_i ggml_backend_t ggml_backend_cpu_init(void) struct ggml_backend_cpu_context * ctx = malloc(sizeof(struct ggml_backend_cpu_context)); if (ctx == NULL) return NULL; ctx-n_threads = GGML_DEFAULT_N_THREADS; ctx-threadpool = NULL; ctx-work_data = NULL; ctx-work_size = 0; ctx-abort_callback = NULL; ctx-abort_callback_data = NULL; ggml_backend_t cpu_backend = malloc(sizeof(struct ggml_backend)); if (cpu_backend == NULL) free(ctx); return NULL; *cpu_backend = (struct ggml_backend) /* .guid = */ ggml_backend_cpu_guid(), /* .interface = */ cpu_backend_i, /* .context = */ ctx ; return cpu_backend;static struct ggml_backend_i cpu_backend_i = /* .get_name = */ ggml_backend_cpu_name, /* .free = */ ggml_backend_cpu_free, /* .get_default_buffer_type = */ ggml_backend_cpu_get_default_buffer_type, /* .set_tensor_async = */ NULL, /* .get_tensor_async = */ NULL, /* .cpy_tensor_async = */ NULL, /* .synchronize = */ NULL, /* .graph_plan_create = */ ggml_backend_cpu_graph_plan_create, /* .graph_plan_free = */ ggml_backend_cpu_graph_plan_free, /* .graph_plan_update = */ NULL, /* .graph_plan_compute = */ ggml_backend_cpu_graph_plan_compute, /* .graph_compute = */ ggml_backend_cpu_graph_compute, /* .supports_op = */ ggml_backend_cpu_supports_op, /* .supports_buft = */ ggml_backend_cpu_supports_buft, /* .offload_op = */ NULL, /* .event_new = */ NULL, /* .event_free = */ NULL, /* .event_record = */ NULL, /* .event_wait = */ NULL, /* .event_synchronize = */ NULL,; ggml_backend_alloc_ctx_tensors 核心代码alloc_tensor_range本质就是构造函数 ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors(struct ggml_context * ctx, ggml_backend_t backend) return ggml_backend_alloc_ctx_tensors_from_buft(ctx, ggml_backend_get_default_buffer_type(backend));ggml_backend_buffer_t ggml_backend_alloc_ctx_tensors_from_buft(struct ggml_context * ctx, ggml_backend_buffer_type_t buft) GGML_ASSERT(ggml_get_no_alloc(ctx) == true); size_t alignment = ggml_backend_buft_get_alignment(buft); size_t max_size = ggml_backend_buft_get_max_size(buft); ggml_backend_buffer_t * buffers = NULL; size_t n_buffers = 0; size_t cur_buf_size = 0; struct ggml_tensor * first = ggml_get_first_tensor(ctx); for (struct ggml_tensor * t = first; t != NULL; t = ggml_get_next_tensor(ctx, t)) size_t this_size = 0; if (t-data == NULL t-view_src == NULL) this_size = GGML_PAD(ggml_backend_buft_get_alloc_size(buft, t), alignment); if (this_size max_size) fprintf(stderr, %s: tensor %s is too large to fit in a %s buffer (tensor size: %zu, max buffer size: %zu) , __func__, t-name, ggml_backend_buft_name(buft), this_size, max_size); for (size_t i = 0; i n_buffers; i++) ggml_backend_buffer_free(buffers[i]); free(buffers); return NULL; if ((cur_buf_size + this_size) max_size) // allocate tensors in the current buffer if (!alloc_tensor_range(ctx, first, t, buft, cur_buf_size, buffers, n_buffers)) return NULL; first = t; cur_buf_size = this_size; else cur_buf_size += this_size; // allocate remaining tensors if (cur_buf_size 0) if (!alloc_tensor_range(ctx, first, NULL, buft, cur_buf_size, buffers, n_buffers)) return NULL; if (n_buffers == 0) #ifndef NDEBUG fprintf(stderr, %s: all tensors in the context are already allocated , __func__);#endif return NULL; ggml_backend_buffer_t buffer; if (n_buffers == 1) buffer = buffers[0]; else buffer = ggml_backend_multi_buffer_alloc_buffer(buffers, n_buffers); free(buffers); return buffer; 总结1.ggml中分明很多地方都是面向对象的思想写的，但是为什么会用C语言写呢？ 更加稳定的ABI，不和编译器强相关 零依赖，轻量化 没有运行时，手动控制内存更方便","tags":["LLM/ggml"],"categories":["源码解析/大模型"]},{"title":"几种程序接口重定向、插桩方式比较","path":"/2025/07/29/几种程序接口重定向、插桩方式比较/","content":"最近在实验中需要分析程序中的堆变量内存分配情况，一开始自己的实现是采用llvm IR Pass修改的方式，后来在OSDI的论文中发现相关的方法采用的是LD_PRELOAD的方式实现，一开始认为这种方法会更加简单，于是进行了实现，结果发现各有特点。 1. LLVM IR 方式2. LD_PRELOAD方式代码 LD_PRELOAD的全进程级别：LD_PRELOAD是基于全进程级别的动态链接符号重定向。 启动加载时加载指定的.so文件，然后后续所有的调用都会使用so文件中提供的实现，这包括引用程序代码、第三方库、libc 比如：重定向了一个malloc，记录程序中的malloc地址然后使用LD_PRELOAD的方式进行重定向。 #include stdio.h#include stdlib.hint main() void* p = malloc(64); // printf(p: %p , p); free(p); return 0; 理论以上代码值调用一个malloc,但是拦截重定向以后发现有3个malloc,不注释代码中的printf函数，还会多出一个malloc。 这样的全进程级别特性可以采集到lib c本身的特性，但是也可能会对程序分析不必要的麻烦，比如我本身只想分析应用程序级别的事务，实现程序代码中的过滤可以通过调用栈过滤等方式实现。","tags":["LD_PRELOAD","LLVM","PIN"]},{"title":"论文中常见内存性能分析workloads","path":"/2025/07/27/论文中常见内存性能分析workloads/","content":"一、分类说明整理所读到论文中经常使用的内存分析工作负载。 按照特点可以分为延迟敏感型、带宽密集型；按照作用可以分为AI、HPC、Database等 博客用于内存性能评估的workload中整理了常见的workloads,但是主要还是重在基本介绍，没有对其访存特征等镜像介绍。本文借鉴这篇博客，自己分析、运行相关workloads。 二、LLM inference1.llama.cpp轻量化的大模型推理框架、适用于嵌入型系统、边缘节点上进行大模型推理。 访存特征： 模型权重加载阶段：采用大块连续内存、或者可选mmap()映射模型参数。具有较好的空间局部性、较差的时间局部性(一次加载一次使用) 推理阶段：主要是KV-Cache需要大量内存访问，读写频繁、更新频繁、各个层中存在一些张量操作也需要访存。高读密集型 内存需求： ​\t同其他大模型推理需求相似，内存需求量主要来自模型权重与KV-cache。模型权重内存使用量与精读有关、KV-Cache与依赖层数有关。一个qwen-7b在llama.cpp的内存占用量约为14~16B 其他： ​\tllama.cpp的内存管理采用统一预分配内存池，使用offset二次分配与访问，最后集中释放的方式进行内存管理。其ggml内存中有对内存的一次性malloc（ggml_init）ggml_new_tensor、ggml_new_tensor_1d进行内存内存二次分配、ggml_free进行内存释放。Arena 分配器（“批发内存，零售指针，整单清场”）","tags":["内存性能分析workloads"],"categories":["内存性能分析workloads"]},{"title":"Qt源码阅读与设计模式","path":"/2025/07/18/Qt源码阅读与设计模式/","content":"todo","tags":["Qt","设计模式"],"categories":["Qt"]},{"path":"/about/index.html","content":"🧑‍💻 About me Hello, welcome to my blog. I’m Zane Jiang. I graduated with a bachelor’s degree in Computer Science from Nanjing Normal University(NNU), and I am currently pursuing a master’s degree at the College of Computer Science, Chongqing University(CQU). I have worked as a C++ software development engineer for one year in a company specializing in LED control systems. My current areas of interest include OS,LLVM, CXL, Qt, and more. This blog is used to document some of my reflections on life, work, and study notes. 📫 Contact Email: 2129056867@qq.com GitHub"},{"title":"C++算法刷题常用API","path":"/notebooks/Interview/C++算法刷题常用API.html","content":"仅供快速复习，最佳方案还是遇到了查dash string 常见操作 构造、长度容量、访问略 输入输出 string s;cin s; // 读取一个单词（遇到空格停止）getline(cin, s); // 读取整行（包括空格） 修改： 追加与连接 + append push_back 插入与删除 s.insert(5,””) s.erase(5,6)从5的位置删除6个字符s.replace(1,3,”xxx”) 子串操作 //提取子串string s = Hello World;string sub = s.substr(6, 5); // 从位置6开始，取5个字符 - Worldstring sub2 = s.substr(6); // 从位置6到结尾 - World//查找size_t pos = s.find(World); // 6 - 第一次出现的位置pos = s.find(Hello, 1); // 12 - 从位置1开始查找// 反向查找pos = s.rfind(Hello); // 12 - 从后往前找// 查找字符pos = s.find_first_of(aeiou); // 1 - 第一个元音字母 epos = s.find_last_of(aeiou); // 15 - 最后一个元音字母 opos = s.find_first_not_of(Helo ); // 6 - 第一个不是这些字符的 W 比较 string s1 = apple, s2 = banana;if (s1 == s2) /* 相等 */ if (s1 != s2) /* 不相等 */ if (s1 s2) /* s1 在字典序中小于 s2 */ if (s1.compare(s2) 0) /* 同上，返回负数、0或正数 */ 数值转换 // 字符串转数值string num_str = 123;int i = stoi(num_str); // 转 intdouble d = stod(3.14); // 转 doublelong l = stol(1000000); // 转 long// 数值转字符串string s1 = to_string(123); // 123string s2 = to_string(3.14159); // 3.14159"},{"title":"CPP杂记","path":"/notebooks/Interview/CPP杂记.html","content":"全局静态变量、函数内静态变量、attribute((destructor))析构顺序构造析构顺序的不确定性 以及静态函数获取的单例。 C++ 标准规定：同一个编译单元（同一个 cpp 文件）内，静态全局对象的析构顺序与构造顺序相反。 但不同编译单元（不同 cpp 文件so）之间的析构顺序是未定义的。 局部 static（即函数内 static）对象的析构顺序与其定义顺序有关，但也只在同一编译单元内有保证。 若一个变量仅在单个文件中可见，则建议将这个变量声明为静态全局变量，static修饰的静态全局变量仅在当前文件中可见。 如果一个全局变量只被单个函数使用,将其改为该函数的静态局部变量可以进一步限制变量的作用域,提高代码的内聚性,降低耦合度。静态局部变量具有全局寿命但局部作用域的特点, 静态全局变量是存储在**静态数据区的,**而不是栈区,因此静态全局变量的大小不会导致栈溢出。栈溢出通常是由于函数调用层次过深或局部变量过大导致的。 类的内存占用 1.32位系统中虚函数指针为4字节，64位为8字节 2.只需要考虑虚函数指针，虚函数表不计入某个类的资源 3.char占一字节，但是需要考虑内存调用 4.如果有虚继承，则多一个虚基类指针。 5.空类占一个字节（用于标识） 指针好题 int arr[5]{1,2,3,4,5};在这个数组的定义中，通常的理解arr是数组的地址即数组首元素的地址，进一步理解arr是一个int型的指针常量，常量+1地址偏移sizeof(int)，所以arr+1是首元素下一个元素的地址；考虑到这一层就不难理解**arr*的含义，arr是对arr取地址，结果也是个地址，只是这个地址的类型是指向有5个int类型数据的数组的指针常量，这个常量+1地址偏移5sizeof(int)。 各级指针算各级的： 主要就是理解 和 * 的“升级降级”； 链接：https://www.nowcoder.com/exam/test/89156461/submission?examPageSource=Intelligentpid=62380309testCallback=https%3A%2F%2Fwww.nowcoder.com%2Fexam%2Fintelligent%3FquestionJobId%3D10%26subTabName%3Dintelligent_page%26tagId%3D21000testclass%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91 Copy and Swap传统做法operator= class MyString private: char* data; // 动态分配的字符串public: // 赋值运算符重载 MyString operator=(const MyString other) // 检查自赋值 if (this == other) return *this; // 释放当前对象的资源 delete[] data; // 复制数据 data = new char[std::strlen(other.data) + 1]; std::strcpy(data, other.data); // 返回当前对象的引用 return *this; ; Copy and Swap MyString operator=(const MyString other) // 检查自赋值 if (this == other) return *this; MyString tmpother; std::swap(data,other.data); return *this; 优势： 异常安全：传统方法new抛出异常时，对象出于无效状态。data已经被删除，但是分配失败 强异常安全性: 如果一个操作因为异常而失败，程序的状态会回滚到操作之前的样子，就像这个操作从来没执行过一样 代码复用：复用拷贝构造函数 自动资源管理：自动释放tmp资源 C++11 写法 MyString operator=(MyString other) //值传递 //传入左值：拷贝构造 //传入右值：移动构造 std::swap(data,other.data); return *this; 移动构造还是拷贝构造？ 左值：叫得出名字 右值：叫不出名字（临时变量，std::move） 左值用拷贝构造，右值用移动构造（偷） 模板模板特化、偏特化模板特化是为特定的模板参数提供特殊实现的技术。当通用模板不适合某些特定类型时，可以为其提供定制版本。 所有模板参数都指定具体类型就叫模板全特化 偏特化只对部分模板参数进行特化,函数模板不支持偏特化，只有类模板支持 函数模板特化 #include iostream#include cstring// 通用模板templatetypename Tvoid print(T value) std::cout General: value std::endl;// 全特化 - 为const char*类型templatevoid printconst char*(const char* value) std::cout C-string: \\ value \\ std::endl;// 全特化 - 为int类型templatevoid printint(int value) std::cout Integer: value (0x std::hex value ) std::endl;int main() print(3.14); // 调用通用版本 print(Hello); // 调用const char*特化 print(42); // 调用int特化 return 0; 类模板特化 #include iostream// 通用模板templatetypename Tclass TypeInfo public: static const char* name() return Unknown; ;// 全特化 - int类型templateclass TypeInfoint public: static const char* name() return int; ;// 全特化 - double类型templateclass TypeInfodouble public: static const char* name() return double; ;// 全特化 - const char*类型templateclass TypeInfoconst char* public: static const char* name() return const char*; ;int main() std::cout TypeInfofloat::name() std::endl; // Unknown std::cout TypeInfoint::name() std::endl; // int std::cout TypeInfoconst char*::name() std::endl; // const char* return 0; 偏特化 #include iostream// 通用模板 - 两个类型参数templatetypename T, typename Uclass Pair public: void print() std::cout General PairT, U std::endl; ;// 偏特化 - 两个类型相同templatetypename Tclass PairT, T public: void print() std::cout Specialized PairT, T std::endl; ;// 偏特化 - 第二个参数为inttemplatetypename Tclass PairT, int public: void print() std::cout Specialized PairT, int std::endl; ;int main() Pairdouble, char p1; p1.print(); // General PairT, U Pairfloat, float p2; p2.print(); // Specialized PairT, T Pairstd::string, int p3; p3.print(); // Specialized PairT, int return 0;//指针类型进行偏特化// 通用模板templatetypename Tclass Wrapper public: static const char* type() return Value; ;// 偏特化 - 指针类型templatetypename Tclass WrapperT* public: static const char* type() return Pointer; ;// 偏特化 - 指向指针的指针templatetypename Tclass WrapperT** public: static const char* type() return Pointer-to-Pointer; ;int main() std::cout Wrapperint::type() std::endl; // Value std::cout Wrapperint*::type() std::endl; // Pointer std::cout Wrapperint**::type() std::endl; // Pointer-to-Pointer return 0; 可变参数模板基本用法： templatetypename... Args // Args是模板参数包void function(Args... args) // args是函数参数包 // 处理参数... #include iostream// 基础情况：0个参数时返回0templatetypename TT sum(T value) return value;// 递归求和templatetypename T, typename... ArgsT sum(T first, Args... args) return first + sum(args...);// 计算参数个数templatetypename... Argsstd::size_t count(Args... args) return sizeof...(args); // sizeof... 操作符获取参数包大小int main() std::cout sum(1, 2, 3, 4, 5) std::endl; // 15 std::cout sum(1.5, 2.5, 3.5) std::endl; // 7.5 std::cout count(1, a, hello, 3.14) std::endl; // 4 return 0; 非类型模板参数(常量模板)cuda中用的多，用于编译器常量优化 template int Nint sumArray(int (arr)[N]) int sum = 0; for (int i = 0; i N; i++) sum += arr[i]; return sum;int a[5] = 1,2,3,4,5;int total = sumArray(a); // N=5，编译期确定 类型转换运算符static_cast 静态转换用于在编译时已知的，有逻辑关联的类型之间的转换 场景 //基本数据类型之间的转换int i = 10;double d = static_castdouble(i); // int - double//派生类指针/引用 - 基类指针/引用（向上转换，Upcasting）。这是安全的，并且是隐式转换的显式写法。class Base ;class Derived : public Base ;Derived derived;Base* basePtr = static_castBase*(derived); // 安全//基类指针/引用 - 派生类指针/引用（向下转换，Downcasting）。不安全Base* basePtr = new Derived(); // 实际上指向一个Derived对象Derived* derivedPtr = static_castDerived*(basePtr); // 可以，但有风险//任何具有转换构造函数的类型转换class MyClass public: MyClass(int x) // 转换构造函数;int num = 5;MyClass obj = static_castMyClass(num); dynamic_cast动态转换用于安全的在继承参差结构中进行向下转换或者交叉转换，依赖运行时类型信息RTTI 用法： 将基类指针或引用安全的转换为派生类的指针或者引用，必须用于多态类型（至少含有一个虚函数） ​\t转换成功会返回目标类型的指针，转换失败返回null或者std::bad_cast class Base virtual void foo() ; // 多态基类（有虚函数）class Derived : public Base ;Base* basePtr = new Derived(); // 正确指向派生类// 安全向下转换Derived* derivedPtr = dynamic_castDerived*(basePtr);if (derivedPtr != nullptr) // 转换成功，可以使用derivedPtr else // 转换失败Base* basePtr2 = new Base(); // 指向基类本身Derived* derivedPtr2 = dynamic_castDerived*(basePtr2);// derivedPtr2 将是 nullptr！// 引用转换try Derived derivedRef = dynamic_castDerived(*basePtr); catch (const std::bad_cast e) std::cout 转换失败: e.what() std::endl; const_cast 常量转换用于修改类型的 const 或 volatile 属性。这是唯一能“去掉” const 属性的转换操作符 主要使用方式就是去掉const属性，以便接受一个非const参数但是不会修改还旧API void printString(char* str) // 一个旧的、不修改str的函数 std::cout str std::endl;const char* myStr = Hello, World!;// printString(myStr); // 错误：不能将const char* 传递给char*printString(const_castchar*(myStr)); // 可行，但有风险 reinterpret_cast 重新解释转换进行低级的、底层的、“重新解释”比特模式的转换。它可以将一个指针转换为任何其他类型的指针，甚至是一个整数。 不进行任何格式检查或安全性检查，可移植性差 场景 //在函数指针类型之间进行转换。//在指针和足够大的整数类型之间进行转换（如 void* 转 uintptr_t）。int* ip = new int(65);// 将int指针毫无关系地转换为char指针，然后打印其指向的值char* cp = reinterpret_castchar*(ip);std::cout *cp; // 输出 A (因为65是A的ASCII码)","tags":[null]},{"title":"C++11新特性","path":"/notebooks/Interview/C++11新特性.html","content":"很好的面试资料： 整理了一年的Linux C++武林秘籍，你早晚用得到（C++进阶必看） - 知乎 转载并简单整理自知乎 程序喵大人 1.左值引用与完美转发等基本概念: 左值、右值 ：放在等号左边，可以取地址并且有名字 的就叫左值，反之叫右值 int a = b + c ;int a = 4; 纯右值:用于初始化一个对象。 运算表达式产生的临时变量、不和对象关联的原始字面量、返回非引用的函数调用、lambda表达式、this指针 将亡值：代表一个“即将结束生命周期”的对象， int main() std::string str = Hello World; // str 是一个左值 // std::move(str) 将左值 str 转换为一个将亡值 // 它在告诉编译器：“str 即将消亡，请移动它而不是拷贝它” std::string new_str = std::move(str); // 此时，str 的资源（动态分配的字符数组）被“移动”到了 new_str 中 // str 本身仍然存在（是一个合法的对象），但处于“有效但未指定状态” // 通常是一个空字符串，但你不能依赖这一点，只能对它进行重新赋值或销毁 std::cout str after move: \\ str \\ std::endl; // 可能是 std::cout new_str: \\ new_str \\ std::endl; // Hello World return 0;//std::move(str) 就是一个将亡值。它不是一个新创建的值（不是纯右值），而是即将失效的现有对象 str 的另一种表现形式//主要产生方法：//1. std::move(a);A a;auto c = std::move(a); // std::move(a) 是一个将亡值//2.static_castA(a)A a;auto d = static_castA(a); // static_castA(a) 是一个将亡值 左值、右值引用: 右值引用。是一种特殊的引用类型，主要设计用来绑定到右值（特别是将亡值），并表明所引用的对象资源可以被安全地”移动”或”窃取”。 int a=5;int b=a;//b是左值引用b=4;int c = 10;//error，10无法取地址，无法进行引用const int d = 10;//ok，因为是常引用，引用常量数字，这个常量数字会存储在内存中，可以取地址int a=4;int b=a;//error,a是左值int c=std::move(a);//ok 移动语义浅拷贝容易导致资源重复释放等问题，深拷贝会导致额外的开销。 因此引入了移动语义，也就是实现了所有权管理，偷掉一些将亡值的资源 C++中所有的STL都实现了移动语义，方便使用。 实现移动语义的关键组件 1.右值引用（T) 2.移动构造函数 class MyString private: char* m_data; size_t m_size; public: // 移动构造函数 MyString(MyString other) noexcept : m_data(other.m_data), m_size(other.m_size) // 偷资源 // 将源对象置于有效但空的状态 other.m_data = nullptr; other.m_size = 0; ; 3.移动赋值运算符 class MyString public: // 移动赋值运算符 MyString operator=(MyString other) noexcept if (this != other) delete[] m_data; // 释放当前资源 // 偷资源 m_data = other.m_data; m_size = other.m_size; // 置空源对象 other.m_data = nullptr; other.m_size = 0; return *this; ;//赋值和构造的区别：赋值是替换，构造是创建加赋值 编译器优化：RVONRVO现代编译器会进行返回值优化（RVO）和命名返回值优化（NRVO），有时甚至比移动语义更高效 返回值优化的时机： return的值类型与函数的返回值类型相同 return的是一个局部对象 MyString create_string() return MyString(Hello); // RVO：直接在目标位置构造，无拷贝无移动MyString create_string_nrvo() MyString result(Hello); return result; // NRVO：直接在目标位置构造result 完美转发完美转发指的是在函数模板中，将参数以原始的值类别（左值或右值）和constvolatile限定符完全不变地转发给另一个函数。 简单来说：保持参数的所有特性不变地传递下去。 为什么需要完美转发？在没有完美转发之前，泛型编程中会遇到参数类别丢失的问题： void wrapper(Foo arg) callee(arg); void wrapper(const Foo arg) callee(arg); void wrapper(Foo arg) callee(std::move(arg)); 使用模板可以简化 templatetypename Tvoid wrapper(T arg) callee(arg); 但问题是： 如果传的是右值，T 会推导成 非引用类型，导致右值变成左值，例如调用wrapper(42);时模板会翻译成warpper(int);会用右值初始化为一个新的局部变量arg，变成了左值。 constvolatile 属性也可能丢失。 万能引用与引用折叠 templatetypename Tvoid wrapper(T arg); 这里的T 当传入右值时：T 推导为 U，于是参数类型为 U → 右值引用； 当传入左值时：T 推导为 U，于是参数类型为 U → 引用折叠 → U。 引用折叠规则： → → → → 所以 T 在模板中既能绑定左值，也能绑定右值。 std::forward在 wrapper 中直接写： callee(arg); 会把所有参数当成左值传递，右值的性质丢失。 需要用 std::forwardT(arg) 来“条件转发”： 如果 T 推导为 U，则 std::forwardT(arg) 返回 U。 如果 T 推导为 U，则 std::forwardT(arg) 返回 U。 这样右值保持右值，左值保持左值，实现完美转发。 典型写法#include iostream#include utilityvoid callee(int x) std::cout callee(int) ;void callee(const int x) std::cout callee(const int) ;void callee(int x) std::cout callee(int) ;templatetypename Tvoid wrapper(T arg) callee(std::forwardT(arg)); // 完美转发int main() int a = 42; const int b = 100;\t//思考输出？ wrapper(a); // callee(int) wrapper(b); // callee(const int) wrapper(10); // callee(int) 典型应用场景1.壳函数 例如日志、装饰器模式、函数调用计时器 templatetypename F, typename... Argsvoid log_and_call(F f, Args... args) std::cout Calling function... ; std::forwardF(f)(std::forwardArgs(args)...); 2.容器构造器 //vector::emplace_back, map::emplace templatetypename... Argsvoid emplace_back(Args... args) new (storage[size++]) T(std::forwardArgs(args)...); // push_back：需要显式移动 names.push_back(std::move(temp)); // 移动语义 // emplace_back：完美转发参数，直接构造 names.emplace_back(David); // 直接在vector中构造，更高效！ 2.智能指针C++11 标准库在 memory 头文件中提供了三种主要智能指针： std::unique_ptr std::shared_ptr std::weak_ptr unique_ptrstd::unique_ptr是一个独占型的智能指针，它不允许其它智能指针共享其内部指针，也不允许unique_ptr的拷贝和赋值。 #include iostream#include memorystruct Foo Foo() std::cout Foo ctor ; ~Foo() std::cout Foo dtor ; ;int main() std::unique_ptrFoo p1(new Foo()); // 独占所有权 // std::unique_ptrFoo p2 = p1; // 错误，不能拷贝 std::unique_ptrFoo p3 = std::move(p1); // 可以转移所有权 if (!p1) std::cout p1 is empty ; 其内部就是对指针包了一层、然后禁用其拷贝构造、赋值函数： templatetypename T, typename Deleter = std::default_deleteTclass unique_ptr private: T* ptr; // 原始指针 Deleter del; // 删除器，默认调用 deletepublic: explicit unique_ptr(T* p = nullptr) : ptr(p) ~unique_ptr() if (ptr) del(ptr); // 禁止拷贝 unique_ptr(const unique_ptr) = delete; unique_ptr operator=(const unique_ptr) = delete; // 支持移动 unique_ptr(unique_ptr other) noexcept : ptr(other.ptr) other.ptr = nullptr; unique_ptr operator=(unique_ptr other) noexcept if (this != other) reset(); ptr = other.ptr; other.ptr = nullptr; return *this; T* get() const return ptr; T operator*() const return *ptr; T* operator-() const return ptr; void reset(T* p = nullptr) if (ptr) del(ptr); ptr = p; ; shared_ptr shared_ptr 使用了引用计数，每一个shared_ptr的拷贝都指向相同的内存，每次拷贝都会触发引用计数+1， 每次生命周期结束析构的时候引用计数-1，在最后一个shared_ptr析构的时候，内存才会释放。 注意点： 可以自定义删除器，在引用计数为0的时候自动调用删除器来释放对象的内存： std::shared_ptrptr(newint,[](int*p)deletep;);std::shared_ptrptr(newint,[](int*p)deletep;); 不要用一个裸指针初始化多个shared_ptr，会出现double_free导致程序崩溃 通过shared_from_this()返回this指针，不要把this指针作为shared_ptr返回出来，因为this指针本质就 是裸指针，通过this返回可能会导致重复析构，不能把this指针交给智能指针管理。 classA shared_ptrAGetSelf() returnshared_from_this(); //returnshared_ptrA(this);错误，会导致doublefree ; 尽量使用make_shared，少用new。 不要delete get()返回来的裸指针。 要避免循环引用，循环引用导致内存永远不会被释放，造成内存泄漏。 #include iostream#include memorystruct Node std::shared_ptrNode next; std::shared_ptrNode prev; ~Node() std::cout Node destroyed ; ;int main() auto n1 = std::make_sharedNode(); auto n2 = std::make_sharedNode(); n1-next = n2; n2-prev = n1; std::cout main end ; main函数解释时，栈上的 n1 被销毁 - n1对象 计数 -1 1 栈上的 n2 被销毁 - n2对象 计数 -1 1 两者的引用计数都不为0 ，可能导致内存泄漏 实现： templatetypename Tclass shared_ptr private: T* ptr; // 指向资源 ControlBlock* ctrl; // 控制块（包含引用计数） struct ControlBlock size_t strong_count; // 强引用计数（shared_ptr 数量） size_t weak_count; // 弱引用计数（weak_ptr 数量） ControlBlock() : strong_count(1), weak_count(0) ;public: explicit shared_ptr(T* p = nullptr) ptr = p; if (p) ctrl = new ControlBlock(); else ctrl = nullptr; ~shared_ptr() release(); void release() if (ctrl) if (--ctrl-strong_count == 0) delete ptr; // 最后一个强引用销毁资源 if (ctrl-weak_count == 0) delete ctrl; // 控制块也销毁 // 拷贝构造：增加 strong_count shared_ptr(const shared_ptr other) ptr = other.ptr; ctrl = other.ctrl; if (ctrl) ctrl-strong_count++; // 移动构造：转移所有权 shared_ptr(shared_ptr other) noexcept ptr = other.ptr; ctrl = other.ctrl; other.ptr = nullptr; other.ctrl = nullptr; // 计数接口 size_t use_count() const return ctrl ? ctrl-strong_count : 0; ; weak_ptr解决循环引用可能出现内存泄漏的问题 弱引用不参与资源的管理 #include iostream#include memorystruct Node std::shared_ptrNode next; std::weak_ptrNode prev; // 改成 weak_ptr ~Node() std::cout Node destroyed ; ;int main() auto n1 = std::make_sharedNode(); auto n2 = std::make_sharedNode(); n1-next = n2; n2-prev = n1; // weak_ptr，不增加 strong_count std::cout main end ; 在使用 weak_ptr 访问对象时，必须检查其有效性。 3.函数式编程 std::fuction与lambdalambda表达式，提供匿名函数的方式，快速定义和使用函数对象 要注意悬空引用 std::functionvoid() createCallback() int local_var = 42; // 危险！捕获了局部变量 local_var 的引用 return []() std::cout local_var; ; // local_var 在这里被销毁int main() auto cb = createCallback(); cb(); // 未定义行为！打印的是已销毁栈上的垃圾值。 for(int id = enCoef9_Rr ; id = enCoef9_Bb ;id++) connect(m_spinBoxs[id],QOverloaddouble::of(QDoubleSpinBox::valueChanged),this,[](double value) OnCoefMatrixSlot(id,value);//id 恒等于 0,未定义行为 ); 默认情况下，以值方式 [=] 捕获的变量在 Lambda 体中是 const 的。如果你想修改它们，必须在参数列表后加上 mutable 关键字。 int main() int count = 0; // 错误：无法在非 mutable lambda 中修改按值捕获的变量 // auto f = [count]() count++; ; // 正确：使用 mutable auto f = [count]() mutable count++; std::cout count; // 输出的是副本，外部的 count 不变 ; f(); // 输出 1 std::cout count; // 输出 0 （外部变量未被修改） std::fuction通用的函数包装器，为各种可调用实体（普通函数、函数指针、lambda、std::bind 表达式、函数对象等）提供了一个统一的类型。这使得我们可以像使用普通变量一样存储和传递函数，极大地增加了代码的灵活性。 std::bind它可以用来绑定一个可调用对象的部分参数，重新排列参数顺序，或者设置默认参数，从而生成一个新的可调用对象 #include iostream#include functionalusing namespace std::placeholders; // 对于 _1, _2, _3...void print_sum(int a, int b, int c) std::cout a + b + c std::endl;void print_coordinates(int x, int y, int z) std::cout ( x , y , z ) ;class MyClass public: void member_func(int x, const std::string msg) std::cout Member func: x , msg std::endl; ;int main() // 1. 绑定参数：将 print_sum 的第三个参数固定为 10 auto bind_func1 = std::bind(print_sum, _1, _2, 10); // _1 是第一个参数，_2 是第二个参数 bind_func1(5, 3); // 等价于 print_sum(5, 3, 10); 输出 18 // 2. 重排序参数：改变参数顺序 auto bind_func2 = std::bind(print_coordinates, _3, _1, _2); // 新顺序：第三、第一、第二 bind_func2(10, 20, 30); // 等价于 print_coordinates(30, 10, 20); 输出 (30, 10, 20) // 3. 绑定成员函数 MyClass obj; // 绑定成员函数需要传递一个对象实例的指针或引用（这里用 obj） // _1 将作为成员函数的第一个参数 (int x) auto bind_member = std::bind(MyClass::member_func, obj, _1, Hello); bind_member(42); // 等价于 obj.member_func(42, Hello); // 4. 与 std::function 结合使用 std::functionvoid(int, int) func = std::bind(print_sum, _1, _2, 100); func(50, 25); // 等价于 print_sum(50, 25, 100); 输出 175 return 0; 4.线程C++11 是 C++ 标准中第一次正式引入跨平台的线程库支持的版本，它将并发和多线程编程纳入了语言和标准库，使得开发者不再依赖 pthread、Windows API 等平台特定的接口。 c++11新特性之线程相关所有知识点 - 简书 基础实现：std::thread用于创建和管理线程。 构造时接收一个可调用对象（函数、lambda、函数对象等），作为线程的入口。 提供 join()（阻塞等待子线程结束）和 detach()（分离线程，让其后台运行）两种线程生命周期管理方式。 禁止拷贝（避免二义性），支持移动语义 #include iostream#include threadvoid worker(int id) std::cout Thread id is running ;int main() std::thread t(worker, 1); t.join(); // 等待线程结束 std::mutexstd::mutex m; std::lock_guardstd::mutex lock(m); // 临界区std::unique_lockstd::mutex lock(m);lock.unlock(); // 可提前解锁lock.lock(); // 再次加锁unique_lock开销比lock_guard更大 std::condition_variable用于线程之间同步,必须与互斥量一起使用。 wait(std::unique_lockstd::mutex lock, Predicate pred)：阻塞当前线程，直到被唤醒且条件谓词 pred 为 true。它会自动释放锁，并在被唤醒后重新获取锁。 notify_one()：唤醒一个等待中的线程（如果有）。 notify_all()：唤醒所有等待中的线程。 典型的生产者消费者写法： #include iostream#include thread#include mutex#include condition_variable#include queuestd::queueint data_queue;std::mutex mtx;std::condition_variable cv;constexpr int MAX_ITEMS = 10;void producer() for (int i = 0; i MAX_ITEMS; ++i) std::this_thread::sleep_for(std::chrono::milliseconds(100)); std::lock_guardstd::mutex lock(mtx); data_queue.push(i); std::cout Produced: i std::endl; // lock 在这里析构解锁 cv.notify_one(); // 通知一个消费者 void consumer() while (true) std::unique_lockstd::mutex lock(mtx); // wait 会释放 lock，并在被唤醒后重新获取 lock // 如果 lambda 返回 false，继续等待；返回 true，则继续执行 cv.wait(lock, [] return !data_queue.empty(); ); int value = data_queue.front(); data_queue.pop(); lock.unlock(); // 可以提前手动解锁，减少锁的持有时间 std::cout Consumed: value std::endl; if (value == MAX_ITEMS - 1) break; int main() std::thread prod(producer); std::thread cons(consumer); prod.join(); cons.join(); std::future 和 std::promise - 异步结果std::future表示一个未来会产生的值，用get()获取结果,可能会阻塞 std::promise提供结果的生产者，和future成对出现，也就是说一个是生产者，一个是消费者 #include iostream#include thread#include futurevoid worker(std::promiseint p) int result = 42; p.set_value(result); // 把结果传递出去int main() std::promiseint p; std::futureint f = p.get_future(); std::thread t(worker, std::move(p)); std::cout Result from worker: f.get() std::endl; t.join(); #include iostream#include thread#include futurevoid worker(std::promiseint p) try throw std::runtime_error(something went wrong); catch (...) p.set_exception(std::current_exception()); // 传递异常 int main() std::promiseint p; std::futureint f = p.get_future(); std::thread t(worker, std::move(p)); try std::cout f.get() std::endl; // 这里会抛出异常 catch (const std::exception e) std::cout Caught exception: e.what() std::endl; t.join(); std::packaged_task#include future#include iostreamint heavy_work(int x) // 模拟繁重计算 return x * x;int main() // 将函数 heavy_work 包装成一个任务 std::packaged_taskint(int) task(heavy_work); // 获取与任务结果关联的 future std::futureint fut = task.get_future(); // 在线程中运行任务（任务对象不可复制，必须移动） std::thread t(std::move(task), 10); t.detach(); // 可以分离，用 future 来获取结果 std::cout Result: fut.get() std::endl; // 输出 100 return 0; std::async#include iostream#include future#include chronoint compute() // 模拟耗时操作 std::this_thread::sleep_for(std::chrono::seconds(2)); return 42;int main() // 异步启动 compute 函数 std::futureint fut = std::async(std::launch::async, compute); // 在主线程做其他事情... std::cout Doing other work... ; // 当需要结果时，get() 会阻塞直到结果就绪 int result = fut.get(); std::cout The answer is: result std::endl; return 0; std::atomicstd::atomic 是一个模板类（如 std::atomicint, std::atomicbool），它包装了一个类型，并提供了一系列保证原子操作的成员函数。原子操作意味着该操作从任何线程的视角看，都是不可分割的 #include atomicstd::atomicint counter(0);// 线程 1 (Writer)int new_value = compute_expensive_value();counter.store(new_value, std::memory_order_release); // 原子写// 线程 2 (Reader)int current_value = counter.load(std::memory_order_acquire); // 原子读//std::atomic允许根据场景选择不同的同步强度，在保证正确性的前提下追求极致性能。//内存顺序通过 std::memory_order 枚举来指定，作为参数传递给 load, store, fetch_* 等操作。counter.fetch_add(1, std::memory_order_relaxed); 关于内存序，这是一个比较复杂的问题，X86\\Riscv的不同强弱内存序也有不同定义与实现。 std::atomic_flag 是专为实现自旋锁而设计的最简单的原子布尔类型 //自旋锁是一种忙等待（Busy-Waiting） 锁。当一个线程尝试获取一个已经被其他线程持有的自旋锁时，它不会立即进入睡眠状态（像互斥锁那样），而是会在一个循环中不断地检查锁是否已经被释放（即“自旋”），直到最终获取到锁为止。#include atomicclass Spinlock private: // ATOMIC_FLAG_INIT 确保标志初始为“清除”（false）状态 std::atomic_flag lock_flag = ATOMIC_FLAG_INIT;public: void lock() // test_and_set() 是原子操作： // 1. 读取当前值 // 2. 无论当前值是什么，都将其设置为 true // 3. 返回它读取到的**旧值** while (lock_flag.test_and_set(std::memory_order_acquire)) // 如果旧值是 true，说明锁已被占用，循环继续自旋 // 如果旧值是 false，说明成功获取锁，循环结束 // 可选：在自旋等待时提示CPU降低功耗或切换超线程 // __builtin_ia32_pause(); // GCC/Clang intrinsic for x86 // std::this_thread::yield(); // 如果等待时间可能较长，可主动让出时间片 void unlock() // 将标志清除（设为false），释放锁 lock_flag.clear(std::memory_order_release); ;// 使用示例Spinlock my_lock;int shared_data = 0;void critical_section() my_lock.lock(); // 获取锁，若失败则自旋等待 shared_data++; // 安全地修改共享数据 // ...其他操作 my_lock.unlock(); // 释放锁 自旋锁与互斥锁的比较 特性 自旋锁 (Spinlock) 互斥锁 (Mutex, e.g., std::mutex) 等待机制 忙等待 (Busy-Waiting)。线程在CPU上循环检查，不放弃CPU时间片。 阻塞等待 (Blocking-Wait)。线程被操作系统挂起，放入等待队列，放弃CPU时间片。 开销 获取释放锁的开销极小（主要是原子CPU指令）。但等待期间消耗CPU周期。 获取释放锁的开销较大（需要进入操作系统内核进行线程调度）。但等待期间不消耗CPU。 适用场景 1. 临界区代码非常短（执行速度快）。 2. 等待时间极短。 3. 不希望发生线程上下文切换（因其开销可能比短暂等待更大）。 例如： 内核编程、无锁数据结构、性能关键的底层代码。 1. 临界区代码执行时间较长。 2. 等待时间可能较长或不可预测。 3. 适用于绝大多数应用程序级别的并发。 例如： 文件操作、数据库访问、复杂的计算过程。 缺点 浪费CPU资源。如果锁被长时间持有，自旋线程会空转CPU，导致性能下降（“烧CPU”）。 上下文切换开销。线程切换需要保存和恢复寄存器状态，开销较大。 短期持有用自旋，长期持有用互斥 5.类型推导auto让编译器在编译期根据初始化表达式自动推导出变量的类型 decltype查询一个表达式（而非初始化器）的类型。它返回该表达式的精确类型，包括引用和 const 限定符。","tags":[null]},{"title":"Qt技能点整理","path":"/notebooks/Interview/Qt技能点整理.html","content":"Qt与QML知识总结Qt中的设计模式中间滑动的slider 2. doubleSpinBox3. iSet新建项目的两种模式ListWidget、选中状态设置为图标模式4.QTreeWidget 框架、表头、样式表、代理 Qt QTreeWidget树形控件用法详解_qtreewidget用法_睿科知识云的博客-CSDN博客 2. Qt大模块1. Qt DP指针2. 视图模型机制3. 视图框架4. QStyle 高度自定义实现5. 元对象与信号槽机制6.构建系统7.QInvokeMethod8.国际化3. Qt细节1. 基础控件2. Undo操作3. svg操作4. Qt不在对象树的工具汇总5.Qt中有有用的宏：Q_LIKELY 6.paint在install中绘制 4. Qt锦上添花1. 动画知识2. 拖拽3. 插件机制4.模块化构建5. Qt中的设计模式1. 单例模式2. 责任链模式3. 接口模式4. 适配器模式5.观察者模式， 线程观察、时间压缩 6.全局信号单例转发类 MAINOPERATIONVIEW_EXPORT bool MainOperationView_Init(IISetWidget ** ppWidget, QWidget * parent) if (ppWidget == nullptr || parent == nullptr) return false; *ppWidget = new CMainOperationView(parent); return true; 6. C++知识c++11新特性，所有知识点都在这了！ - 知乎 (zhihu.com) this_thread ++ lambda的坑 for(int id = enCoef9_Rr ; id = enCoef9_Bb ;id++) connect(m_spinBoxs[id],QOverloaddouble::of(QDoubleSpinBox::valueChanged),this,[](double value) OnCoefMatrixSlot(id,value); ); id 恒等于 0 多继承与QOBject private 虚继承 class CBaseprivate:\tvirtual void virtualPrivateFuntion() std::coutbase virtualPrivateFuntion;\t；class CSub :public CBase\tprivate:\tvirtual void virtualPrivateFuntion() override std::coutsub virtualPrivateFuntion;\tint main()\tCBase* pObject = new CSub();\tpObject-virtualPrivateFuntion();//结果：sub virtualPrivateFuntion，//结论：子类继承父类的private virtual可以重写 可变参数、变参模板 函数包装器 std::shared_ptrreset、make_shared 右值引用C++反射元编程Metaprogram is a program about a program. 7.杂项QStatusBar插件机制解耦快捷键ISet7.0 接口设计 MVC8.轮子1.单例2.工作线程封装3.stl 迭代器模式和适配器模式报错整理： QMenu 没有添加Action时，不能直接visible或这exec； setGeometry: Unable to set geometry 的一种解决办法是重写sizehint，而不是使用setFixedSize； 工具使用AddressSanitizer（不适用与MinGW）[AddressSanitizer 定位嵌入式cc++内存错误 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/436177229#:~:text=AddressSanitizer （又名 ASan）是 C%2FC%2B%2B 的内存错误检测器。 AddressSanitizer 由 google,的一部分，而从 4.8 版开始逐渐成为 GCC 的一部分。 这也意味着如果交叉编译器版本低于 4.8 ，是无法使用的。) Qt 拾遗 008 在 Qt 中使用 Address Sanitizer - 简书 (jianshu.com) 在Qt中使用gcc 4.8.0的地址消毒剂(Address Sanitizer) MTuner软件【精选】基于MTuner软件进行qt的mingw编译程序的内存泄漏检测_mtuner怎么使用_yantuguiguziPGJ的博客-CSDN博客","tags":[null]},{"title":"ai_hpc_cuda","path":"/notebooks/Interview/ai_hpc_cuda.html","content":"NvidiaGPUCUDA相关：1.描述一下SM的结构，在写kernel的时候共享内存大小和寄存器文件数量需要注意吗？ SM是NVIDIA GPU的核心计算单元,包含 CUDA core，最核心的基本计算单元，处理整形和单精度浮点运算。 寄存器文件、 Warp Scheduler线程数调度器， 共享内存 L1cache 等 写kernel时共享内存大小和寄存器文件数量直接影响SM的活跃线程束数量（Occupancy）—-即SM上同时可执行的线程束数与最大可支持线程束数的比率。 共享内存大小： 注意：必须注意。共享内存是按块分配的有限资源。 影响：每个块申请的共享内存越大，一个SM上能同时驻留的线程块就越少，会降低占用率（Occupancy），可能影响性能。 寄存器数量： 注意：必须注意。寄存器是按线程分配的有限资源。 影响： 每个线程使用的寄存器越多，一个SM上能同时驻留的线程就越少，同样会降低占用率。 寄存器使用过多会导致寄存器溢出（Register Spilling），编译器被迫将变量存储到慢速的全局内存中，严重损害性能。 2.共享内存和寄存器分别应该存放哪些数据，其用量与SM上活跃的线程块的关系。共享内存 从全局内存预加载的数据块、中间计算结果（如规约运算的局部和） 作用：协作、缓存、通信 寄存器 线程私有的局部数据 作用：私有性、高性能 关系 SM上的活跃线程块数量同时受共享内存总量和寄存器总量的硬性约束，最终的实际数量是以下三个计算结果中的最小值： SM支持的最大线程块数（架构限制）。 SM共享内存总量 / 每块申请的共享内存大小。 SM寄存器总量 / (每线程寄存器数量 * 每块线程数)。 3.bank冲突是什么？描述具体结构，如何解决？Bank冲突为了提供高带宽，共享内存被物理上划分为若干个（通常是32个，与Warp大小对应）同样大小的、能同时被访问的内存模块，这些模块称为 Bank。 理想情况（无冲突）： 如果一个Warp中的32个线程分别访问32个不同Bank中的地址（或者访问同一个Bank中的完全相同的一个地址，即广播），那么所有这些访问都可以在一个时钟周期内一次性完成。 冲突情况： 如果一个Warp中的多个线程访问了同一个Bank中的不同地址，就会发生Bank冲突。硬件必须将这些冲突的访问拆分成多个没有冲突的周期依次执行。有n个线程访问同一个Bank的不同地址，就需要n个时钟周期来完成原本一个周期就能完成的工作。 Bank编号Bank编号 (地址字节偏移量 4字节) % 32 解决核心思路是：改变数据在共享内存中的布局或访问模式，确保一个Warp内的线程访问不同的Bank。 方法一：内存填充（Memory Padding） 问题： 在操作二维数组（例如矩阵）的Tile时，如果数组的宽度是Bank数量（32）的整数倍，那么同一行中相邻的元素会位于不同的Bank，但同一列中相邻的元素会因为固定的步长而落在同一个Bank里。当线程按列读取时，就会导致严重的Bank冲突。 解决方案： 在声明共享内存数组时，人为地给每一行增加一些多余的“填充”元素，使实际的行长（Pitch）不再是32的整数倍。 示例：一个32x32的Tile cpp // 可能产生Bank冲突的声明__shared__ float tile[32][32];// 使用填充避免Bank冲突的声明（例如，多加1个元素）__shared__ float tile_padded[32][33]; // 33不再是32的因数 这样，原来在同一列上的元素 tile[0][0], tile[1][0], tile[2][0]… 现在变成了 tile_padded[0][0], tile_padded[1][0], tile_padded[2][0]…。计算它们的Bank编号： (0 / 4) % 32 = 0 ( (1 * 33 * 4) / 4 ) % 32 = (33) % 32 = 1 ( (2 * 33 * 4) / 4 ) % 32 = (66) % 32 = 2它们被巧妙地分散到了不同的Bank中，从而避免了冲突。 方法二：改变访问模式或算法设计核函数时，尽量让一个Warp内的线程访问连续的共享内存地址。因为连续地址通常映射到不同的Bank，这是最友好的访问模式。 方法三：使用不同的广播机制如果确实需要让多个线程读取同一个值，应尽量确保它们访问的是完全相同的地址，这会触发广播机制，在一个周期内完成操作，而不是产生冲突。 4.说一下分支冲突（线程束分化），如果warp内有冲突，部分符合if条件，部分符合else条件，是否需要等待？分支冲突发生在同一个Warp内部的线程执行了不同的控制流路径时。例如，部分线程满足if条件，而另一部分线程满足else条件。 串行化执行： GPU的Warp调度器会让Warp先执行所有走if路径的线程。此时，那些本该走else路径的线程在这个阶段是被禁用（masked out） 的，它们不会执行任何操作，但必须等待。 再次执行： 当if路径执行完毕后，调度器会再让Warp执行所有走else路径的线程。同样，此时走if路径的线程被禁用并等待。 汇合后继续： 当所有不同的控制流路径都执行完毕后，Warp内的所有线程才会在汇合点（reconvergence point） 重新同步，并继续一起执行后续的相同指令。 因此，分支冲突的性能代价是：执行时间变成了所有不同路径执行时间的总和，而不是其中最长路径的时间。 怎样避免线程束分化 核心思想：同一个Warp内的数据具有相同的特性，从而执行相同的指令路径。 预处理数据，使同一Warp数据特征一致 谓词执行 用条件赋值 ?: 替代短小的 if-else 5.用过TensorCore吗？了解TensorCore的原理吗？Tensor Core是一种专为执行矩阵乘累加运算而设计的专用硬件单元，从Volta架构开始引入。其核心原理可以概括为： 1. 计算模式：D A * B + C它的核心是执行一个固定的计算操作：接收两个小矩阵A和B，与一个累加矩阵C相乘后相加，得到结果矩阵D。 A, B, C, D 都是特定维度的小矩阵（如 16x16, 8x32, 32x8 等）。 计算A * B是完整精度的，其结果与C相加后，再以目标精度（如FP32FP16INT8）存储到D。 2. 混合精度计算：这是其实现性能突破的关键。它使用低精度输入来实现高吞吐量和低功耗，但使用高精度进行累加以保持数值稳定性。 常见模式： FP16 输入 (A, B) + FP16 或 FP32 的累加器 (C) - FP16 或 FP32 输出 (D)。 其他模式： 也支持INT8、INT4、BF16等输入精度，以及TF32（在Ampere及以后架构中）。 3. 极高的吞吐量：每个Tensor Core每个时钟周期可以执行大量的乘加运算（FMA operations）。例如，一个V100的SM中的每个Tensor Core每周期可以执行一个 8x4 * 4x8 - 8x8 的MMA操作，这相当于 64 次乘加运算（128 FLOPS） 每周期。这与传统的CUDA Core（每周期1次乘加）相比，吞吐量提升了两个数量级。 4. 编程模型：Warp-Level OperationTensor Core的操作是在线程束级别进行的。一个Warp内的线程需要协作来共同加载一个大的输入矩阵的各个小块（Tile）到寄存器中，然后调用一条指令（如wmma::mma_sync）来让Tensor Core硬件执行整个小矩阵的运算。 为什么用float4向量来存取数据？有什么好处？ 为什么用双缓冲优化？了解cuda流和cuda graph吗？ 除了MPI，有知道现在用的更多的GPU通信库吗？ 在Nsight Computing中，经常关注的与内存相关的指标。有关注L1 Cache命中率吗？ GPU指令集优化方面了解吗？有做过PTX相关的优化吗？ GEMM是计算密集型还是访存密集型算子？ 知道cutlass中如何对GEMM进行优化的吗？ 训练推理了解Transformer吗？底层是什么结构？cuda中如何优化？说一下你对大模型的理解。cuda中如何写Softmax？某个参数过大如何解决？Dropout和BatchNorm在训练和推理时有什么区别？说一下你了解的无监督学习算法。知道Faster Transformer吗？有了解如何实现的吗？Paged Attention有了解吗？知道TensorRT吗？部署过推理模型吗？","tags":[null,null]},{"title":"侯捷C++内存管理机制","path":"/notebooks/Interview/侯捷C++内存管理机制.html","content":"C++内存调用 第一节 C++ primitives","tags":[null]},{"title":"力扣刷题笔记乱序","path":"/notebooks/Interview/力扣刷题笔记(乱序).html","content":"状态规划专题119杨辉三角2 .bygbnffplspg{zoom:50%;} 关键点 ： 空间复杂度O(row)","tags":[null]},{"title":"面经常见问题：C++篇","path":"/notebooks/Interview/面经常见问题：C++篇.html","content":"问题 三种多态的实现方式及其区别 STL 容器在 for 循环中使用 erase 删除元素会导致什么问题？ 什么是Perfec t Forwarding？为什么要使用它，它解决了什么问题？写一下 完美转发和移动语义的实践场景 虚函数表的存储位置和内存结构 单继承、多继承、虚继承的vptr是怎样的？对应的C++内存模型是怎样的 虚函数能否是模板函数？为什么？ 析构函数声明为虚函数的原因是什么？ C++类默认生成的成员函数有哪些？ 左值和右值的区别与应用场景 Name Mangling的作用及其带来的兼容性问题 动态库和静态库的优缺点对比 参考回答 三种多态的实现方式及其区别 静态多态和动态多态：函数重载、运算符重载、泛型编程与模板（特别的CRTP）（编译期多态）；虚函数 动态多态 STL 容器在 for 循环中使用 erase 删除元素会导致什么问题？ std::vectorint vec = 1, 2, 3, 4, 5;// 错误方式：迭代器失效、变成野指针for (auto it = vec.begin(); it != vec.end(); ++it) if (*it % 2 == 0) vec.erase(it); // it 失效，后续 ++it 未定义行为 // 正确方式1：利用返回值for (auto it = vec.begin(); it != vec.end(); ) if (*it % 2 == 0) it = vec.erase(it); // erase返回下一个有效迭代器 else ++it; // 正确方式2：remove-erase惯用法vec.erase(std::remove_if(vec.begin(), vec.end(), [](int x) return x % 2 == 0; ), vec.end()); 什么是Perfec t Forwarding？为什么要使用它，它解决了什么问题？写一下在函数模板中，保持参数的原有类型和特性（const）进行转发。没有完美转发时，泛型编程会遇到参数类别丢失的问题。传入右值时，T会被推导成非右值，const属性也会消失； templatetypename Tvoid wrapper(T arg)\tcallee(arg);\tcallee（std::forwardT (arg)）; 完美转发和移动语义的实践场景 日志函数\\ 装饰器模式、容器构造器 templatetypename Tvoid log_and_call(T arg)\tstd::coutaaa;\tcall(std::forwardT arg); 虚函数表的存储位置和内存结构 vtable 只读数据段、vptr对象内部 单继承、多继承、虚继承的vptr是怎样的？对应的C++内存模型是怎样的 虚函数能否是模板函数？为什么？ 虚函数不能是模板函数。虚函数表要求在编译器固定布局，模板函数实例化在编译器按需生成。时机冲突 析构函数声明为虚函数的原因是什么？ 当一个派生类对象通过 基类指针 删除时，如果基类的析构函数 不是虚的，将只会调用基类析构函数，而派生类部分不会被析构，造成 资源泄漏 或 未定义行为。 C++类默认生成的成员函数有哪些？ 默认构造、拷贝、析构、移动、拷贝赋值与移动赋值 左值和右值的区别与应用场景 Name Mangling的作用及其带来的兼容性问题 动态库和静态库的优缺点对比 RAII设计模式的实际应用案例"},{"title":"面经常见问题：操作系统与体系结构篇","path":"/notebooks/Interview/面经常见问题：操作系统与体系结构篇.html","content":"问题 请描述从用户态调用 read 函数读取文件数据，到获取数据，中间完整的内核处理过程是怎样的？ Linux 内核的内存管理机制是如何工作的？为什么要引入虚拟内存概念？ 你如何进行Linux内核的裁剪？具体步骤和考量是什么？ 简要说明Linux文件系统的底层原理和管理机制。 编译器（Compiler）和连接器（Linker）在程序构建过程中分别起什么作用？ NUMA和非NUMA架构的区别？优势 什么是缓存一致性协议，如何实现缓存一致性协议？ 三个层级cache延迟时间分别大约多少？ 进程间通信的几种主要方式 new和malloc的底层机制差异 虚拟地址到物理地址的转换过程 大页表技术的适用场景 思路1. 请描述从用户态调用 read 函数读取文件数据，到获取数据，中间完整的内核处理过程是怎样的？2. Linux 内核的内存管理机制是如何工作的？为什么要引入虚拟内存概念？3. 你如何进行Linux内核的裁剪？具体步骤和考量是什么？4. 简要说明Linux文件系统的底层原理和管理机制。5. 编译器（Compiler）和连接器（Linker）在程序构建过程中分别起什么作用？6. NUMA和非NUMA架构的区别？优势7. 什么是缓存一致性协议，如何实现缓存一致性协议？ 最著名、最基础的缓存一致性协议是MESI，也称为伊利诺伊协议。它通过为每个缓存行（Cache Line，缓存的基本单位）维护一个状态位，并通过核心之间的通信来维护状态。MESI是四个状态的缩写： M - 修改（Modified） 状态：该缓存行是“脏的”，即它的数据已被当前核心修改，与主内存中的数据不一致。这是该数据唯一的最新副本。 责任：当这个缓存行被替换时，必须将其写回主内存。 对应行：其他核心的缓存中没有该数据的副本。 E - 独占（Exclusive） 状态：该缓存行是“干净的”，其数据与主内存一致。但只有当前核心拥有这份副本。 特权：当前核心可以随时对其进行修改，而无需通知其他核心。一旦修改，状态会变为 M。 对应行：其他核心的缓存中没有该数据的副本。 S - 共享（Shared） 状态：该缓存行是“干净的”，数据与主内存一致。 限制：可能有多个核心的缓存中都存在该数据的副本。因此，当前核心不能直接修改它，必须先与其他核心沟通。 对应行：其他核心的缓存中可能有该数据的副本。 I - 无效（Invalid） 状态：该缓存行是无效的，不能使用。它要么是空的，要么里面的数据是过时的。 要求：如果核心要读取或写入一个处于 I 状态的缓存行，它必须先从其他核心或主内存中获取最新数据。 假设有两个核心，Core A 和 Core B。 初始：内存地址 X 的值为 10。没有核心缓存它。 Core A 读取 X： Core A 缓存未命中，在总线上发送 Bus Read。 无人响应，从内存读取值 10。 Core A 的缓存行状态变为 E（独占）。 Core B 读取 X： Core B 缓存未命中，在总线上发送 Bus Read。 Core A “嗅探”到这个请求，知道自己有副本。 Core A 将数据提供给 Core B，并将自己的状态从 E 变为 S。 Core B 接收到数据，状态设置为 S。 Core A 要写入 X（比如改为 20）： Core A 发现自己的状态是 S，不能直接写。 它在总线上发送 Bus Upgrade 或 Bus Read Exclusive 消息。 Core B “嗅探”到这个消息，知道自己副本要失效了，将自己的缓存行状态变为 I。 Core A 收到确认后，执行写入操作，并将自己的状态从 S 变为 M。此时，Core A 的缓存是最新值 20，Core B 的缓存已无效，内存中的数据（10）也已过时。 Core B 再次读取 X： Core B 缓存行状态是 I，未命中。发送 Bus Read。 Core A “嗅探”到请求，发现自己状态是 M（已修改）。 Core A 将数据（20）写回总线（或先写回内存再提供），并将自己的状态变为 S。 Core B 接收到最新数据 20，状态变为 S。 总线窥探（Snooping）：上述机制依赖于所有核心监听总线上的所有消息，这在大规模多核系统中可能成为瓶颈。 目录协议（Directory Protocol）：为了解决窥探的扩展性问题，更高级的系统使用“目录”。目录作为一个中心化的组件，记录每个缓存行被哪些核心缓存。当一个核心需要修改数据时，它只需查询目录，然后由目录直接通知那些持有副本的核心使其失效，而不是广播到所有核心。这在NUMA架构中非常常见。 存储缓冲区（Store Buffer）：为了不让核心在写操作时总是等待其他核心的响应（例如，等待 Bus Upgrade 的确认），CPU引入了存储缓冲区。核心可以先把写操作放入存储缓冲区，然后继续执行后续指令。这虽然提升了性能，但也引入了更复杂的内存一致性模型问题（如需要内存屏障指令）。 8. 三个层级cache延迟时间分别大约多少？9. 进程间通信的几种主要方式10. new和malloc的底层机制差异11. 虚拟地址到物理地址的转换过程12. 大页表技术的适用场景"},{"title":"面经常见问题：手撕篇","path":"/notebooks/Interview/面经常见问题：手撕篇.html","content":"问题 实现一个 shared_ptr，需要注意哪些关键点？（引用计数、拷贝构造、赋值操作符、thread safe等） 无序数组中找到出现次数超过 n2 的元素，要求常数空间复杂度 参考回答 实现一个 shared_ptr，需要注意哪些关键点？（引用计数、拷贝构造、赋值操作符、thread safe等） templatetypename Tclass SharedPtr private: T* ptr_; int* count_; std::mutex* mutex_; void release() bool should_delete = false; std::lock_guardstd::mutex lock(*mutex_); if (--(*count_) == 0) should_delete = true; if (should_delete) delete ptr_; delete count_; delete mutex_; public: // 构造函数 explicit SharedPtr(T* ptr = nullptr) : ptr_(ptr), count_(new int(1)), mutex_(new std::mutex) // 拷贝构造函数 SharedPtr(const SharedPtr other) : ptr_(other.ptr_), count_(other.count_), mutex_(other.mutex_) std::lock_guardstd::mutex lock(*mutex_); ++(*count_); // 赋值操作符 SharedPtr operator=(const SharedPtr other) if (this != other) release(); // 释放当前资源 ptr_ = other.ptr_; count_ = other.count_; mutex_ = other.mutex_; std::lock_guardstd::mutex lock(*mutex_); ++(*count_); return *this; // 移动语义 SharedPtr(SharedPtr other) noexcept : ptr_(other.ptr_), count_(other.count_), mutex_(other.mutex_) other.ptr_ = nullptr; other.count_ = nullptr; other.mutex_ = nullptr; ~SharedPtr() release(); T operator*() return *ptr_; T* operator-() return ptr_; T* get() return ptr_; int use_count() return count_ ? *count_ : 0; ;","tags":[null]},{"title":"面经常见问题：调试性能调优篇","path":"/notebooks/Interview/面经常见问题：调试与性能调优篇.html","content":"问题 程序通常会在哪些情况下发生 Core Dump？你是否有过排查经历？ 如果程序发生 Core Dump，你通常会如何定位和分析问题？ 你是否有过性能调优的经历？请讲讲你的思路和常用工具。 如何使用 GDB 调试多线程程序？（例如查看所有线程 backtrace、切换线程等） 思路"},{"title":"面经常见问题：网络篇","path":"/notebooks/Interview/面经常见问题：网络篇.html","content":"问题ARP协议的工作流程TCP拥塞控制从URL输入到页面渲染的完整链路 参考思路"},{"title":"ASPLOS-25 PIM Is All You Need A CXL-Enabled GPU-Free System  for Large Language Model Inference","path":"/notebooks/paper/ASPLOS-25-PIM Is All You Need_ A CXL-Enabled GPU-Free System  for Large Language Model Inference.html","content":"背景PIM和PNMPIM 的计算单元与存储单元结合紧密，处于内存芯片内部。在传统 PIM 方法里，计算单元被放置在主存储器（DRAM）中，与存储单元在物理上紧密相连。像美光的混合存储立方体（HMC），在 DRAM 层的堆栈下设置逻辑层，期望在逻辑层实现自定义逻辑；NM 的计算单元在物理上靠近内存阵列，但与 PIM 相比结合程度没那么紧密。其处理单元可部署在不同位置，形成多种架构形态。常见的位置有 3D 堆叠内存的逻辑层，如三星 HBM-PIM 在 HBM2 内存堆栈的每个存储体中集成可编程计算单元（PCU）；PIM内存密度低，容量小；PNM带宽没有优势。 GPU推理特点带宽在内存不足时会趋近饱和；decoder-only与传统大模型相比GPU算力利用率更低 PIM具有极大的内部带宽 PIM具有极低的内存密度言外之意存储容量不够 为什么需要Hierarchical PIM-PNM架构？有两种操作：\t1.在near-bank使用通用处理PU；2.使用专用的near-bank PUs进行DSA计算，同时在PIM中进行了其他操作。 CENT硬件架构一个CPU连接了CXL Switch 连接32个CXL device。每个device包含了CXL控制器；每个控制器包含一个PNM和16个chips，每个chips包含了两个PIM。CXL switch支持CPU到CXL通信、以及CXL之间的点对点通信； CXL-based Network ArchitectureCENT 以CXL 3.0为技术底座，复用 PCIe 6.0 物理层实现高速互连，同时通过协议定制满足 LLM 推理的低延迟与高带宽需求。复用了物理层。CXL于host之间采用x16通道链路，理论带宽128GBs；CXL于switch之间采用x4通道，理论带宽32GBs；使用了CXL.io. CXL.mem;未使用CXL,cache; Inter-Device Communication（设备间）Figure 5展示了单个 CXL设备的内部结构。CXL设备之间的通信主要使用了IDCC和Shared Buffer设备之间通信采用了三个通信原语： SEND_CXL ：“非阻塞” 指令，需明确指定目标设备 ID（DVid）、源设备与目标设备的共享缓冲区地址；RECV_CXL： 与SEND_CXL 相反，RECV_CXL 是 “阻塞式” 指令，且无需指定源设备 ID；SEND和RECV构成一次完整的CXL写事务；事务流程： 源设备执行 SEND_CXL，将共享缓冲区（Rs）中的数据封装为 CXL 帧（含 DVid、Rd 地址），通过 CXL 端口发送给交换机； CXL 交换机解析帧头的 DVid，将数据转发到目标设备的 CXL 端口； 目标设备通过 RECV_CXL 监测到端口数据，按 Rd 地址写入本地共享缓冲区，并向源设备返回 “无数据响应（NDR）”； 源设备收到 NDR 后，确认数据传输完成（非阻塞指令的 “完成通知”）。 BCAST_CXL ：非阻塞指令，通过 8 位 DVcount 参数指定 “需广播数据的后续 CXL 设备数量”。 单个CXL设备通过一个请求写数据到多个设备，基于PBR(基于端口的路由机制)的filt的预留字段实现，设置了一个标记广播位。有一个确认机制，广播出去以后需要目标设备接收数据并返回NDR后才会执行，避免数据丢失 CXL端口的实现如图Figure6:主要就是发送队列和接收队列的实现 相应术语：L2R；(local 2 remote).读写事务对应的各种包 Hierarchical PIM-PNM Architecture如图5.也是一个取指译码的过程，指令先被放到一个指令缓冲区，然后译码器执行并分发到PNM和PIM。（标准的读写事务被直接分发给PIM处理，其他的则转化为微操作分发到PIM和PNM） PIMPIM接收微操作然后转化为相关的DRAM指令，Figure 7a 展示每个PIM channel结构。每个bank有32MB容量和一个near-bank PU； 值得注意的是，near-bank中实现了一个16 MAC reduction tree（16路MAC规约树），专门用来做乘加运算。如GEMV，本质是向量元素与矩阵元素做乘法，然后累加求和。接收16组16bit*16bit的输入数据，然后并行执行16次乘法，最后输入一个32bit的累加结果。这些输入来自临近的bank或者是全局的buffer，这一点有点类似CUDA编程。 全局缓冲区可以向通道内不得16个PU共享256bit的数据，实现数据复用，降低内存访问延迟。类似CUDA中的全局内存。每个PU内部有一个32bit的累加寄存器存储MAC 的结果，这个寄存器的结果访问可以由定制的ISA访问。激活函数利用DRAM中的查找表LUT和线性插值实现；避免了复杂的电路。 同时介绍了DRAM于PU的同步，DRAM存储体每2ns向PU输出1次16bit的输入，PU每2ns完成一次16路规约树计算，最大化PIM通道的计算-内存访问overlap；然后介绍了一些通过不同PIMchannel的指令如ACTab等指令。 PNM单元PNM如Figure7b所示。包含（1）.32个累加器，输入从Shared Buffer中取得，执行残差连接等；（2）32个规约树，每一路从shared buffer中获得一个256bit的值（16组BF），执行多元素求和；（3）指数加速器；（4）2发射超标量流水线RISCV核心，执行一些不常见操作； Intra-Device Communication设备内部通信，WR_SBK、RD_SBK等实现DRAM banks与Shared buffer 之间的移动; ISA 总结基于这些数据移动指令和部件，实现了一些对外暴露的相关ISA操作，见Table2 模型映射 （软件架构）大模型高参数以及PIM的低存储密度，所以并行化策略很重要。这一节主要讲如何利用这些硬件，重点是如何利用进行模型并行化。 Pipeline-Parallel Mapping (PP)云服务器提供商服务数万到数百万用户的查询，吞吐量（单位时间内处理的查询数）至关重要。传统GPU存储因为KV cache占用内存大导致吞吐量难以提升。 以Transfomer块拆分为流水线阶段。不同查询在流水线的不同阶段同时执行。根据负载均衡进行块在CXL之间的划分依据，避免单个块内的通行，避免拆分流水线放到不同的CXL设备的PIM channels中； 上一个块执行完传到下一个块进行，实现了请求并行，每次传的块是16KB。受限于Global Buffer的大小，不支持批处理请求（一次请求多个请求）； 言外之意流水线并行就是每个CXL设备处理每个请求的一部分，然后放给下一个CXL设备继续处理，但是每次只能处理一个请求，不能同时处理多个请求。 Tensor-Parallel Mapping (TP)所有资源同时处理一个解码器块，有效降低延迟。 Figure 9展示了分配方案， 具体每个层是放在一个CXL设备还是多个CXL设备是按照通信开销来决定：计算密集且通信少的层拆分并行，通信密集或轻量的层单设备执行。 Hybrid Tensor-Pipeline Parallel Mapping将每个 Transformer 解码器分配给多个连续的 CXL 设备。例如，在 32 个设备的情况下，将每个解码器映射到 324 8 个设备上，这样就实现了 TP8 和 PP4。其中，张量并行是将单个张量（如权重矩阵）按维度拆分，不同设备计算张量的部分子块，通过通信拼接结果。流水线并行则是将模型按层拆分，不同设备按 “流水线” 顺序执行，前一设备的输出作为后一设备的输入 Transformer Block Mapping一个层是如何在CXL设备上执行的：并且详细讲解了GEMV、向量点积、元素级乘法等操作的对应优化。 End-to-End Model Mappingprefill直接在CXL上按照上述方法映射Transformer，执行后top-k采样在CPU上执行 Programming Model为了更加简单的使用，又进行了一次封装，只需要配置并行策略等，自研的编译器会进行转换。保证执行的效率。 名义上的编译器，其实就是二次封装，更像是一个算子库，封装了常见的算子。 实验方法论及结果进行了并行策略对比、GPU与CENT跨平台性能对比、场景适配性测试、敏感性测试吞吐量提高了，成本降低了，能耗降低了 个人评价1.PIM没有真实设备实现，但是其硬件结构设计、到ISA到多种并行方式从底层到上层垂直打通，最大优势是利用了PIM的带宽。 2.并行策略丰富，常见的并行策略都有所考虑。 3.实验部分没有仔细看，是采用模拟器模拟执行，CXL采用这种模拟框架在后续实验中也许可以使用。（类似的模拟器方法包括摩尔线程的SimuMax)结合摩尔线程的这个模拟库修改可以实现GPU-CXL协同计算的一些环境模拟，应该可以发掘一些场景，CPU+GPU不行或者全CXL不行，需要GPU+CXL或者CPU+GPU+CXL。有些人可能比较容易想到这样协同的点，但是实现起来很困难，借助这个模拟器可以实现。","tags":[null,null]},{"title":"ASPLOS`25 Systematic CXL Memory Characterization and Performance Analysis at Scale","path":"/notebooks/paper/ASPLOS-25-Systematic-CXL-Memory-Characterization-and-Performance-Analysis-at-Scale.html","content":"Systematic CXL Memory Characterization and Performance Analysis at Scale Jinshu Liu Virginia Tech https://github.com/MoatLab/Melody 1.IntroductionCurrently, there is a significant gap in research that explores detailed CXL characteristics and their impact on memory-intensive workloads at scale, in depth, and across the full spectrum of sub-μs latencies. In particular, how do CXL devices differ in detailed performance characteristics beyond average latency and bandwidth metrics? How (much) does CXL’s long (and longer) latency affect CPU efficiency and workload performance? What are the underlying causes and how do we analyze it? Exsiting works focus on coarse-grained analysis and overlook several critical aspects: (i) CXL performance stability (i.e., tail latencies); (ii) CPU tolerance to prolonged CXL latencies across various workloads, and the architectural implications of CXL; and (iii) the lack of systematic approach to dissect workload performance and CPU inefficiency under CXL. So: introduce Melody, a comprehensive framework for detailed CXL performance characterization. The first analysis of CXL characteristics beyond average latency and bandwidth across 4 real CXL devices. An extensive evaluation of CXL’s performance implications across diverse workloads. A systematic approach for workload performance analysis under CXL. contributions(in my view): ​\t1.MELODY,a framwork to measure CXL perfomence. ​\t2.An in-depth study of CXL tail latencies (like caption). ​\t3.Root-cause analysis approach 2.BackgroundHow CPU backend and CXL MC process Load and Store request? .sloifirtlpig{zoom:200%;} Request types: The CPU issues two types of load requests: Demand and Prefetch. Demand loads are memory reads that CPU requests from (CXL) MC only when it is needed for computation. Prefetch reads are predictive reads directed by prefetchers, e.g., “L1PF” and “L2PF” in Figure 2a. Stores are first queued in the “store buffer.” Each store request triggers a Read-for-ownership (RFO) for cache coherence from CXLDRAM, followed by a Write upon cache eviction. MC ： Memory requests to the CXL MC are encapsulated(compress) in a specific packet format, known as Flits , for transmission over CXLPCIe. Upon arrival, the CXL controller (“CXL Ctrl”) parses the request and places it in the request queue. The request scheduler then selects the next request to process based on the scheduling policy and other factors such as thermal management for low latency, high bandwidth, and reliability. Requests are then passed to the command scheduler, which issues appropriate low-level DDR commands to the DRAM chips. 3.CXL Device Characterization3.1 Testbed Concern: work load: cloud workloads (in-memory caching and databases such as Redis [13] and VoltDB [21], CloudSuite [1], and Phoronix [12]), graph processing (GAPBS [22], PBBS [19]), data analytics (Spark [30]), MLAI (GPT-2 [5], MLPerf [14], Llama [9]), SPEC CPU 2017 [18], and PARSEC [24]. 3.2 CXL latency stability and its relationship with bandwidth Terms distinction: Loaded latencies: memory access latency under high utilization Idle latency: occurs when the system experiences minimal load 这一部分实现了一个MIO，通过多次指针追踪记录一次rdtsc时间戳来计算average latency，并采用MLC来验证MIO。测试了一些tail latency 与bandwidth之间的关系，结果均可以想到。 一个测量内存压力的方法：将指针追踪访问线程和32个AVX访存线程一起bind到一个numa nod(co-locate) CXL latency vs. bandwidth under various readwrite ratios. Local DRAM achieves the highest bandwidth under a read-only workload, whereas NUMA and all CXL devices (except CXL-C) achieve minimal bandwidth in read-only scenarios. This is because NUMA and CXL links are bidirectional, allowing them to sustain higher bandwidth under mixed readwrite workloads CXL devices demonstrate significant variability Impact of CPU prefetchers on (tail) latency. Prefetching does not fully mitigate CXL-induced tail latencies. Reasoning. 本节中测出的结果发现尾延迟等性能差距很大，这样的结论其实作用不大。但是性能差异大可以作为其他性能研究的挑战和动机 1.CXL协议传输层与连接层的实现本身引入了性能开销 2.MC 控制器实现本身 4 Workload Characterization 讨论了一些工作负载的延迟敏感性等，此前论文已经有过 5 Spa for CXL Slowdown Analysis5.2 Challenges and Limitations of State-of-the-ArtChallenge:1.Identifying the underlying CPU eventsmetrics that can correlate to the slowdowns is challenging. It is even more challenging to establish a precise correlation between workload performance and architecture-level performance metrics, Why not TMA? TMA does not provide a differential analysis to interpret pipeline differences resulting from varying backend memory (i.e., CXL vs. local DRAM). TMA is unable to precisely correlate architecture level metrics with workload slowdowns. 5.3 Spa: A Bottom-Up Approach DRAM (Demand Load) Slowdown:These misses denote demand read misses, excluding RFO and prefetch requests. Store Slowdown :Incoming store requests queued in the store buffer are dequeued upon completion. Some writes issue RFO requests before execution. If the store buffer fills up, these RFOs would hinder load efficiency, causing CPU stalls. Cache Slowdown:On SKX, most cache slowdown occurs in L2 due to a significant rise in stall cycles for L1 load misses with CXL. Conversely, on SPREMR, LLC experiences the bulk of slowdown, with a notable increase in stall cycles for L2 load misses with CXL. key finding: This reduces L2 prefetcher’s coverage of both demand reads and L1 prefetch. L1 prefetches would either miss entirely in L2 or at best, they would hit on a pending L2 prefetch in L2. Consequently, CXL also negatively impacts L1 prefetcher’s timeliness.Loads that would have otherwise hit in the cache if L1 prefetches were timely, now are delayed. Consequently, overall prefetch efficiency suffers and stalls on caches increase. 由于CXL的长延迟，L2预取的信息时效性降低，当L1需要相应数据的时候，L2还没有预取回来，导致L1认为miss，于是访问L2,再次发出请求。原本可以命中的Load请求变得不命中。 intel没有计数器直接观测L1Pf-L2-hit与miss的情况，可以通过一些其他的计数器间接的观测情况。 发现：L2PF-L3-miss减少，L1PF-L3-miss增多，L2PFL3-hit不变，因此推导出：L2预取器低效预取，L1预取增多。 5.5 5.6 Workload Slowdown Diversity Period-based Slowdown AnalysisAn approach to convert time-based sampling data into a period-based slowdown analysis. 5.7 Spa Use CasesPerformance tuning. For example, to mitigate the slowdown bursts observed in 605.mcf (Figure 16b), we first identify memory accesses during bursty periods (e.g., exceeding 10%) using binary instrumentation via Intel Pin. Next, we pinpoint the source code responsible for high slowdowns using addr2line. Our analysis reveals that two performance-critical objects, each 2GB in size, are contributing to the slowdown. 作者提到的两个case，一个是用来做性能优化，一个是作为性能指标来进行分层，这两个点其实都是和后一篇论文有联系，做铺垫。 不足与机会： 作者在验证cache slow down的主要原因的 方法是： To validate this, we disable all the hardware prefetchers (L1 and L2) and measure workload slowdowns. With prefetchers off, we found virtually no stall cycles on cache。这样的方法并不深入，为什么降低？这些值得深挖，但是需要一些硬件探索。 关于预取，可以增加预取器的深度(也就是再多预取几个周期)直接解决这个问题。 5.7中讲到了一个关于SPA的使用案例，通过剖析SPA中的slowdown来分析slowdown，然后把slowdown严重的变量放置到CXL，这样的方法我觉得十分适用。","tags":[null]},{"title":"ASPLOS·23 '25 TPP","path":"/notebooks/paper/ASPLOS23： TPP.html","content":"TPP：面向 CXL 使能的分层内存透明页放置技术摘要超大规模应用对内存需求的持续增长，使得内存成为数据中心总体支出的重要组成部分。CXL（Compute Express Link）等一致性接口的出现，为内存扩展提供了有效解决方案，使主内存能够整合多种特性各异的内存技术。本文通过分析 Meta 服务器集群中各类数据中心应用的内存使用模式，证实了将冷页卸载到低速内存层的可行性。然而，若缺乏高效的内存管理机制，这类分层内存系统会严重降低性能。 为此，本文提出一种面向 CXL 内存的操作系统级透明页放置机制（TPP）。TPP 采用轻量级方法识别热页与冷页，并将其放置到合适的内存层，支持主动将本地内存中的页面降级到 CXL 内存，为新页面分配预留空间（新分配页面通常与请求处理相关，具有短生命周期和高热特性）。同时，TPP 能快速将滞留在低速 CXL 内存中的性能关键热页提升到高速本地内存，且最小化采样开销与不必要的页面迁移。TPP 无需应用特定知识，可作为内核版本全局部署。 在支持 CXL 1.1 的新型 x86 CPU 生产服务器集群中，通过多种内存敏感型工作负载对 TPP 进行评估。结果表明，TPP 使分层内存系统性能接近 “全本地内存” 的理想基准（性能差距 1%），相比现有 Linux 系统性能提升 18%，相比 NUMA Balancing、AutoTiering 等现有方案性能提升 5%-17%。目前，大部分 TPP 补丁已整合到 Linux v5.18 版本，剩余补丁正处于讨论阶段。 1 引言数据中心应用对内存需求的激增，叠加 DRAM 成本上升与技术缩放挑战，使得内存成为超大规模数据中心基础设施支出的重要部分。非 DRAM 内存技术为构建分层内存子系统、以更低每 GB 成本扩展内存容量提供了可能，但这类技术通常延迟远高于主内存，若页面在内存层级中放置不当，会严重影响性能。此外，有效利用这些技术需提前掌握应用行为并进行精细调优，这在应用种类繁多且快速迭代的超大规模环境中，资源消耗极高。 CXL（Compute Express Link）通过提供介于 DRAM 与非 DRAM 之间的延迟水平、类 DRAM 带宽和缓存行粒度访问语义，缓解了上述问题。CXL 协议支持新的内存总线接口将内存连接到 CPU，从软件视角看，CXL 内存表现为无 CPU 的 NUMA 节点，其内存特性（带宽、容量、技术类型等）独立于直接连接 CPU 的本地内存，为内存子系统设计提供灵活性与细粒度控制。同时，CXL 内存的类主内存特性为透明页放置创造了条件。然而，Linux 内存管理机制基于同构 CPU 连接 DRAM 设计，在 CXL 内存系统中表现不佳 —— 由于不同内存层访问延迟存在差异，应用性能高度依赖高速内存服务的内存访问比例。 为验证分层内存的收益，需先分析现有数据中心应用的内存访问行为，包括特定时间段内内存的热、温、冷状态占比及页面生命周期长短。现有基于空闲页跟踪（IPT）的特性分析工具存在明显缺陷：需修改内核（生产环境中通常不可行）、持续的访问位采样与分析会带来过高 CPU 和内存开销（难以支持大规模工作集），且无法区分应用对不同类型页面（匿名页、文件页缓存、共享内存等）的敏感度差异。为此，本文开发轻量级用户态工具 Chameleon，利用 CPU 的精确事件采样（PEBS）机制分析应用内存访问行为，生成不同页面类型的内存使用热力图，为多内存层应用性能预测提供依据。 通过 Chameleon 对生产环境中不同服务领域的大规模内存密集型应用进行分析，得出以下关键发现：（1）应用工作集中存在大量可卸载到低速内存层且不影响性能的温页与冷页；（2）匿名页（用于程序栈、堆、mmap 调用）中热页占比显著高于文件备份页；（3）页面访问模式在较长时间内（分钟至小时级）保持稳定，为内核层页放置决策提供充足时间窗口；（4）页面分配与回收会导致物理页热度快速变化，静态页分配策略会严重降低性能。 基于上述发现，本文设计操作系统级透明页放置机制 TPP，实现热页在高速内存层、冷页在低速内存层的高效放置。TPP 包含三大核心组件：（a）轻量级回收机制，将冷页降级到低速 CXL 节点；（b）解耦多 NUMA 系统的分配与回收逻辑，在高速层预留空闲页空间；（c）反应式页提升机制，识别 CXL 内存中的热页并提升到本地内存。此外，TPP 支持基于页面类型的分配策略 —— 优先将敏感匿名页分配到本地内存，文件缓存分配到 CXL 内存，该策略能为特定访问模式的应用提供更优初始状态，加速性能收敛。 在支持 CXL 1.1 的系统中，对占服务器集群主要份额的四类生产工作负载进行测试。结果显示，TPP 使分层内存系统性能接近全本地内存水平，即使本地 DRAM 仅为系统总内存的 20%，部分工作负载仍能保持这一性能。TPP 将有效热内存全部迁移到本地内存，相比默认 Linux 系统性能提升最高 18%，相比 NUMA Balancing、AutoTiering 等主流分层内存方案性能提升 5%-17%。 本文的主要贡献如下： 开发轻量级用户态内存特性分析工具 Chameleon，用于理解工作负载内存消耗行为并评估超大规模数据中心分层内存的应用前景，已开源； 提出面向分层内存系统的高效内存管理机制 TPP，已开源，大部分代码已整合到 Linux 内核 v5.18，剩余代码正处于上游讨论阶段； 在支持 CXL 的分层内存系统中，通过真实生产工作负载对 TPP 进行长期评估，证实 TPP 使分层内存性能接近全本地内存水平，相比默认 Linux 提升最高 18%，相比 NUMA Balancing、AutoTiering 提升 5%-17%； 首次实现并评估可直接部署于超大规模数据中心的端到端 CXL 内存系统。 2 研究背景与动机2.1 数据中心应用内存需求增长为构建低延迟服务，内存计算已成为数据中心应用的常态，导致服务器集群内存需求快速增长。随着 CPU 与 DRAM 技术迭代，内存在机架级功耗与总拥有成本（TCO）中的占比持续上升（如图 3 所示），内存成本与功耗优化成为数据中心降本增效的关键。 2.2 同构服务器设计的扩展性挑战当前服务器架构中，内存子系统设计完全依赖 CPU 支持的内存技术，存在诸多限制：（1）内存控制器仅支持单一代际内存技术，无法混合搭配不同成本、带宽与延迟特性的内存；（2）内存容量需按 2 的幂次扩展，难以实现细粒度容量配置；（3）每代 DRAM 的带宽 - 容量组合有限，为获取更高带宽常需过度配置内存容量。这种 CPU 与内存的强耦合限制了高效内存层级的设计，导致计算、网络、内存资源利用率低下。此外，现有支持内存扩展的总线接口多为厂商专有，跨 CPU 兼容性差，且延迟高、缺乏一致性，难以满足超大规模数据中心需求。 2.3 CXL 对分层内存系统的支撑作用CXL 是基于 PCIe 的开放行业互联标准，支持主机处理器与设备（加速器、内存缓冲器、智能 IO 设备等）之间的高速低延迟通信，同时扩展内存容量与带宽。CXL 提供字节可寻址内存空间，支持通过标准内存分配 API 进行透明分配，实现缓存行粒度访问与硬件维护的一致性。随着 PCIe 5.0 的普及，CPU 与 CXL 互联带宽将接近双路服务器跨插槽互联带宽，CXL 内存访问延迟与 NUMA 远程访问延迟相当（仅比普通 DRAM 高 50-100 纳秒）。这种类 NUMA 特性与类主内存访问语义，使 CXL 内存成为数据中心分层内存系统中低速层的理想选择。 目前，主流芯片厂商均在开发并集成 CXL 解决方案，支持 CXL 所需的工具、驱动与操作系统修改已开源，避免了对单一供应商的依赖。CXL 突破了传统内存子系统的限制，允许根据工作负载需求灵活设计内存子系统的带宽、容量与成本组合，实现计算与内存资源的独立扩展，提升闲置资源利用率。 2.4 CXL 分层内存系统的应用前景数据中心工作负载极少持续使用全部内存，应用常分配大量内存但访问频率低。本文对 Meta 生产服务器集群中四类典型应用的分析显示，任意两分钟内，应用已分配内存中 55%-80% 处于空闲状态。将这些冷页迁移到低速内存层，可为主内存释放空间以容纳更多热页，提升应用性能，同时通过 “小容量高速本地内存 + 大容量低成本 CXL 内存” 的灵活服务器设计，降低总拥有成本（TCO）。 为简化表述，本文将直接连接 CPU 的内存称为 “本地内存”，CXL 连接的内存称为 “CXL 内存”（CXL 内存可采用 DRAM、NVM、LPDRAM 等任意技术）。 3 数据中心应用内存特性分析为明确分层内存系统在超大规模应用中的适用场景，本文开发轻量级用户态内存访问行为分析工具 Chameleon。该工具需满足以下需求：可在现有生产服务器上部署，不干扰运行中应用，无需修改内核；性能开销低，不影响生产应用行为；能分析应用内存访问的关键特征（热 温 冷页占比、页面在各热度层的停留时间、访问频率等）。实际使用中，仅需在少量服务器上运行数小时，即可完成对某类应用的特性分析。 3.1 Chameleon 工具设计Chameleon 包含 Collector（收集器）与 Worker（分析器）两个组件，分别运行于独立线程： Collector（收集器）：利用现代 CPU 的 PEBS 机制，采集与内存访问相关的硬件性能事件，包括需求加载的末级缓存（LLC）缺失事件（MEM_LOAD_RETIRED.L3_MISS）与需求存储的 TLB 缺失事件（MEM_INST_RETIRED.STLB_MISS_STORES，受硬件限制，无精确的存储 LLC 缺失事件）。采样记录包含内存访问事件对应的进程 ID（PID）与虚拟内存地址。 采样率需在准确性与性能开销间权衡：高频采样提升准确性，但会增加应用线程开销与 Worker 线程 CPU 消耗。在 Meta 集群中，默认配置为每 200 个事件采样 1 次，实现开销与准确性的平衡。 为提升灵活性，Collector 将所有 CPU 核心划分为多个组，每次仅对一个或多个核心组启用采样，每 5 秒（默认 mini_interval）轮换核心组。这种 “轮循采样” 机制可进一步调节开销与准确性，还支持为不同核心组配置不同采样事件（如对延迟敏感应用采样半数核心，对存储密集型应用分核心组分别采样加载与存储事件）。 Collector 将采样数据写入两个哈希表之一，每 1 分钟（默认 interval）唤醒 Worker 处理当前哈希表数据，并切换到另一个哈希表存储下一时段采样数据。 Worker（分析器）：运行于独立线程，读取页面访问信息并生成内存访问行为分析结果。将采样记录中的地址视为虚拟页访问（页面大小由操作系统定义，支持 4KB 普通页、2MB1GB 大页等）。为同时分析虚拟地址空间与物理地址空间的特性，Worker 会将采样虚拟页映射到对应的物理页；若应用工作集极大（如 TB 级），可禁用物理地址转换，仅分析虚拟地址空间访问模式。 对于每个页面，Worker 用 64 位位图记录其在各时间间隔内的活跃状态（活跃则置位），每个时间间隔结束后位图左移 1 位以记录新间隔状态。若需捕捉页面访问频率，可配置每个时间间隔占用多位，但会缩短历史记录长度。生成统计结果并报告后，Worker 进入休眠状态。 在生产环境中，Chameleon 仅在 CPU 与内存使用率低于 80% 的服务器上运行，未观察到对服务性能的影响，CPU 开销控制在单个核心的 3%-5% 以内。仅在内存带宽敏感且 CPU 核心饱和的合成工作负载中，因 Chameleon 与应用竞争 CPU 资源，性能损失约 7%。 3.2 生产工作负载概述利用 Chameleon 对 Meta 生产服务器集群中四类长期运行、处理真实流量的内存密集型应用进行分析，这些应用覆盖不同服务领域，占服务器集群份额较大，具有代表性： Web 服务：运行虚拟机处理 Web 请求，包括基于 HHVM 的 Web1 和基于 Python 的 Web2； 缓存服务（Cache）：分布式内存对象缓存服务，位于 Web 层与数据库层之间，提供低延迟数据查询； 数据仓库（Data Warehouse）：统一计算引擎，在计算集群上并行处理数据，协调执行长时间运行的复杂批处理查询； 广告服务（Ads）：计算密集型工作负载，读取内存数据并执行机器学习计算。 3.3 页面热度特征数据中心应用的已分配内存中，大量页面在数分钟内保持冷态（如图 7 所示）。Web、Cache、Ads 服务虽占用系统 95%-98% 的内存容量，但在任意两分钟内，平均仅使用 22%-80% 的已分配内存；数据仓库作为计算密集型工作负载，虽占用服务器几乎全部内存，但两分钟内仅 20% 的被访问内存为热页。 结论：数据中心应用的被访问内存中，大量页面在分钟级时间内保持冷态，若页放置机制能将这些冷页迁移到低速内存层，分层内存系统将显著受益。 3.4 不同页面类型的热度差异应用根据逻辑与执行需求使用不同类型页面，但匿名页（anon）的热页占比显著高于文件页（file）（如图 8 所示）。以 Web 服务为例，两分钟内匿名页热页占比为 35%-60%，而文件页热页占比仅为 3%-14%。 缓存服务使用 tmpfs 实现高速内存查询，匿名页主要用于查询处理，文件页占热内存比例较高，但 Cache1 的匿名页两分钟内访问占比为 40%，文件页仅为 25%；Cache2 在两分钟内匿名页与文件页访问占比接近，但一分钟内匿名页热页占比（43%）仍高于文件页（30%）。 数据仓库与广告服务的匿名页用于计算，文件页用于存储中间计算结果，因此热内存几乎全部为匿名页，文件页多为冷页。 结论：短时间间隔内，匿名页中热页占比显著高于文件页，页放置机制需考虑页面类型差异。 3.5 页面类型使用的时间稳定性Web 服务启动时，需将虚拟机二进制文件与字节码加载到内存，此时文件缓存占内存比例较高；随着运行时间增加，匿名页占比逐渐上升，文件缓存被回收以释放空间（如图 9a 所示）。 缓存服务主要使用文件缓存进行内存查询，文件页占已分配内存的 70%-82%；若匿名页需求增长，文件页会被回收以容纳新匿名页（如图 9b、9c 所示）。 数据仓库的匿名页占已分配内存的 85%，文件页占 15%，且两类页面占比在运行期间保持稳定（如图 9d 所示）。 结论：尽管匿名页与文件页占比可能随时间变化，但应用的页面类型使用模式总体稳定，页放置机制需结合页面类型动态调整策略。 3.6 页面类型对性能的影响内存密集型应用的吞吐量随内存利用率提升而增加，但不同工作负载对页面类型的敏感度存在差异（如图 10 所示）。例如，Web 服务吞吐量随匿名页利用率提升而显著增长；Cache1 的匿名页使用量在生命周期内固定，因此匿名页与文件页利用率对吞吐量无明显影响；Cache2 的高吞吐量对应较高的匿名页利用率；数据仓库的文件页使用量固定，匿名页利用率达到峰值时吞吐量最高。 结论：工作负载对不同页面类型的敏感度存在差异，且随时间变化，页放置机制需动态适配这种敏感度差异。 3.7 冷页重访问时间特征冷页可能在后续时间被重新访问，不同工作负载的冷页重访问时间差异显著（如图 11 所示）。Web 服务的冷页中，80% 在十分钟内被重新访问，表明 Web 服务常复用早期分配的页面；缓存服务的冷页也有大量在十分钟内重访问，随机卸载冷页会影响性能；数据仓库的热文件页中，仅 20% 在十分钟内被重新访问，其余为新分配页面。 结论：不同工作负载的冷页重访问时间差异大，页放置机制需主动将重访问的冷页（变为热页）迁移到高速内存层，避免高访问延迟。 综上，数据中心应用存在大量冷页且访问模式稳定，分层内存系统具有广阔应用前景，而高效的页放置机制是发挥分层内存优势的关键。 4 TPP 设计原则随着 CXL 技术的普及，超大规模数据中心开始采用 CXL 使能的异构分层内存系统，不同内存层性能特性差异显著。为优化这类系统性能，需设计透明页放置机制（TPP），实现不同热度页面在对应内存层的高效放置。设计 TPP 需解决三个核心问题：TPP 的实现层级、页面热度检测方法、CXL 内存的抽象方式。本节将阐述这些设计选择的依据与权衡。 4.1 实现层级选择透明页放置机制可在用户态或内核态实现： 用户态实现：可基于 Chameleon 类工具检测页面热度，通过move_pages()等用户态 API 实现 NUMA 迁移。但该方式存在明显缺陷：用户态与内核态上下文切换带来额外开销；需在用户态管理页面历史信息，增加处理开销；页面信息存储的内存开销随工作集规模增长而扩大，难以支持大规模场景。因此，用户态实现仅适用于短时间、小范围的性能分析，无法满足全生产集群的持续运行需求。 内核态实现：内核态实现无需上下文切换，可直接利用内核现有内存管理组件（如 LRU 链表），开销更低、复杂度更小、性能更优。因此，TPP 选择在内核态实现。 4.2 页面热度检测方法页面热度检测有多种候选技术，需结合可行性、开销与兼容性综合选择： PEBS（精确事件采样）：PEBS 可在硬件层面采集内存访问事件，但存在明显局限：不同 CPU 厂商的 PEBS 计数器不兼容，难以实现跨硬件平台的通用内核级热度检测；CPU 支持的性能计数器数量有限，且通常需暴露给用户态；即使优化后，PEBS 采样在高压力工作负载中仍存在较高开销，不适合作为 TPP 的常驻组件。 页面中毒（Page Poisoning）：通过对内存区域中的部分页面进行采样与 “中毒” 跟踪访问事件，是检测热页 冷页的常用方法。基于 IPT（空闲页跟踪）的页面中毒技术需频繁清除页面访问位并刷新 TLB，导致严重性能下降；Thermostat 通过 2MB 大页粒度采样优化性能，但仅适用于大页场景，而 TPP 需支持任意页面大小（生产环境中应用常使用 2MB1GB 大页存储代码、静态数据等热数据，需避免其被降级到 CXL 内存）。 NUMA Balancing（AutoNUMA）：NUMA Balancing 支持任意页面大小，通过采样页面访问生成次要页错误来判断页面热度。但对高频访问页面频繁触发页错误会带来过高开销。TPP 对该技术进行优化：仅将次要页错误作为 CXL 内存的热度检测机制（CXL 内存主要存储温页与冷页，采样开销低）；本地内存的冷页检测则复用 Linux 现有 LRU 老化机制（轻量级且高效）。 实验证实，结合 LRU 与 NUMA Balancing 的热度检测方法可有效识别 CXL 内存中的热页，且开销接近零。 4.3 CXL 内存的抽象方式CXL 内存可通过现有内存交换机制（如 zswap）作为交换空间存储冷页，但该方式存在关键缺陷：会丧失 CXL 的核心优势 —— 缓存行粒度的加载 存储访问语义。交换空间抽象下，访问交换页面需触发主要页错误并读取整个页面，导致有效延迟远超 200 纳秒，使 CXL 内存失去竞争力。 因此，TPP 选择将 CXL 内存视为常规内存而非交换空间，应用可通过加载 存储指令直接访问 CXL 内存中的温页与冷页，避免页错误开销。需注意的是，基于交换的内存管理机制（如 TMO）与 TPP 并非竞争关系，而是互补：TMO 运行于用户态，通过反馈驱动回收内存；TPP 运行于内核态，优化本地内存与 CXL 内存间的页放置，二者可协同工作。 5 TPP 机制设计高效的页放置机制需实现：将冷页卸载到 CXL 内存，将 CXL 内存中的热页提升到本地内存；支持异构内存技术，适应 CXL 内存的无 CPU NUMA 节点特性；避免分配因回收缓慢而停滞，同时考虑应用对页面类型的敏感度。基于此，TPP 设计包含四大核心模块：轻量级 CXL 降级机制、分配 - 回收解耦、CXL 热页提升、页面类型感知分配。 5.1 轻量级 CXL 降级机制Linux 默认将页面分配到进程运行 CPU 的本地内存节点，当本地内存节点满时，通过交换（swap）回收页面。在 NUMA 系统中，若本地内存回收缓慢，新分配页面会被分配到 CXL 节点，导致性能下降。 TPP 的降级机制优化：本地内存触发回收时，不执行交换，而是将回收候选页面加入独立降级列表，异步迁移到 CXL 节点（迁移速度远快于交换）。降级候选页面通过 Linux 现有 LRU 机制选择，优先选择非活跃页面，降低热页误降级风险。若 CXL 节点内存不足导致迁移失败，再 fallback 到默认交换回收机制；CXL 节点本身仍使用默认交换回收（CXL 节点性能敏感度低）。 若系统存在多个 CXL 节点，根据 CPU 到各 CXL 节点的距离选择降级目标（距离越近延迟越低）。该静态距离策略虽简单，但在实验中表现出良好有效性。 5.2 分配与回收解耦Linux 为每个内存节点的内存域维护三个水位线（min、low、high）：当空闲页低于 low 水位线时，触发回收；直到空闲页回升到 high 水位线，才允许新分配。在高分配速率场景下，回收速度难以跟上分配速度，导致本地内存频繁停止分配，新页面被分配到 CXL 节点，性能下降。 TPP 通过解耦 “回收停止条件” 与 “分配允许条件”，在本地节点主动维持空闲页余量： 回收逻辑：本地节点的回收持续运行，直到空闲页达到 “降级水位线”（demotion_watermark）； 分配逻辑：只要空闲页达到 “分配水位线”（allocation_watermark），即可允许新分配。 其中，降级水位线始终高于分配水位线与 low 水位线，确保本地节点预留充足空闲页。该设计的优势在于：（1）突发分配（通常与请求处理相关，短生命周期、高热）可直接分配到本地内存；（2）本地内存有充足空间接收从 CXL 节点提升的热页。 为适应不同应用特性，TPP 提供用户态 sysctl 接口（/proc/sys/vm/demote_scale_factor），控制本地节点触发回收的空闲页阈值，默认值为 2%（即空闲页低于本地节点容量的 2% 时启动回收）。管理员可结合工作负载监控工具动态调整该值：若应用分配需求高且冷页占比大，可提高阈值以增强回收积极性；若应用热页占比超过本地节点容量，需降低阈值避免热页频繁迁移。 5.3 CXL 热页提升机制本地内存压力可能导致新页面被分配到 CXL 节点，且降级到 CXL 节点的页面可能重新变为热页。若缺乏提升机制，这些热页会长期滞留 CXL 节点，严重影响性能。TPP 通过增强 Linux NUMA Balancing 实现高效热页提升。 5.3.1 面向 CXL 的 NUMA Balancing 优化原生 NUMA Balancing 会采样所有内存节点的页面，对远程访问页面进行提升，但该机制在 CXL 系统中存在冗余：（1）本地内存的热页无需采样，采样会增加不必要的页错误开销；（2）无需将本地内存热页提升到其他节点。因此，TPP 将 NUMA Balancing 采样范围限制为 CXL 节点，仅分析 CXL 节点页面的访问情况。 提升 CXL 节点热页时，TPP 忽略本地节点的分配水位线检查，通过增加本地节点内存压力，触发冷页回收以容纳提升的热页。若系统存在多个本地节点，优先选择进程当前运行的本地节点；若应用跨多个本地节点，选择内存压力最低的节点作为提升目标。 5.3.2 避免 “乒乓效应” 的热页识别原生 NUMA Balancing 在检测到页面访问时立即提升，未考虑页面活跃状态，导致低频访问页面被误提升到本地节点。这些页面随后可能因本地内存压力被降级回 CXL 节点，形成 “乒乓效应”，浪费带宽与 CPU 资源。 TPP 通过页面 LRU 状态判断页面活跃度，避免误提升： 若 CXL 节点的采样页面处于非活跃 LRU 链表，不立即提升（可能为低频访问页）； 仅当页面处于活跃 LRU 链表时，才将其视为提升候选。 但 CXL 节点通常内存压力较低，回收不频繁，非活跃 LRU 链表中的页面可能无法自动迁移到活跃链表，导致热页被遗漏。为此，TPP 补充优化：当采样到非活跃 LRU 链表中的页面访问时，立即将页面标记为 “已访问” 并迁移到活跃 LRU 链表；若该页面在后续采样中再次被访问（此时已处于活跃链表），则执行提升。 该设计为页面提升增加 “滞后性”，结合 Linux 为匿名页与文件页维护独立 LRU 链表的特性，使不同类型页面按自身活跃度独立调整提升速率，加速热页在内存层间的收敛。 5.4 页面类型感知分配上述页放置机制对页面类型无差别处理，但部分应用可通过类型感知分配进一步优化性能。例如，应用预热阶段常进行大量文件 IO，生成的文件缓存多为冷页，若这些文件缓存占据本地内存，会导致匿名页被分配到 CXL 节点，后续需频繁提升，增加迁移开销。 TPP 支持可选的 “页面类型感知分配” 策略： 匿名页：优先分配到本地内存（匿名页通常为热页，对性能敏感）； 文件缓存（含 tmpfs）：优先分配到 CXL 节点（文件缓存多为冷页，对延迟敏感度低）。 当该策略启用时，应用生命周期内生成的文件缓存初始分配到 CXL 节点；若文件缓存变为热页并被 NUMA Balancing 采样到，再提升到本地内存。该策略使小容量本地内存 + 大容量 CXL 内存的系统能高效支持文件缓存密集型应用，同时保持高性能。 5.5 TPP 可观测性设计为评估 TPP 有效性并排查生产环境问题，TPP 引入多组统计计数器，通过/proc/vmstat接口暴露给用户态，主要包括： 降级统计：成功降级的匿名页与文件页数量、降级失败次数及原因； 提升统计：CXL 节点页面采样数、提升尝试次数、成功提升的匿名页与文件页数量； 乒乓效应跟踪：通过页面标志位（PG_demoted）标记降级页面，统计降级后被提升的页面数量（该值过高表明存在乒乓效应）； 提升失败分类：按失败原因（本地节点内存不足、页面引用异常、系统级内存紧张等）统计提升失败次数，辅助定位问题。 6 评估本节通过生产环境工作负载对 TPP 进行全面评估，验证其在 CXL 分层内存系统中的性能表现，主要回答三个问题：（1）TPP 在内存层间页面分布与性能提升方面的有效性；（2）TPP 各组件的贡献；（3）TPP 与现有主流方案的性能对比。 6.1 实验环境 硬件平台：采用支持 CXL 1.1 的预生产 x86 CPU，搭配 FPGA-based CXL 内存扩展卡，CXL 内存以无 CPU NUMA 节点形式呈现。当前 FPGA 卡的延迟比目标产品高约 250ns，用于功能验证；主流 x86 CPU 厂商确认，最终产品的 CXL 内存访问延迟将接近双路服务器的远程 NUMA 访问延迟。 系统配置：基于双路服务器构建模拟 CXL 系统，配置 1 个含活跃 CPU 核心的本地内存节点与 1 个 CXL 节点；基准环境（全本地内存）通过禁用一个 CPU 插槽的内存与核心，仅保留单个本地节点实现。 内存配置：采用两种内存容量比例：（1）2:1（本地内存：CXL 内存）：模拟当前生产环境，本地内存可容纳全部热页；（2）1:4：模拟内存受限场景，本地内存仅能容纳部分热页，测试 TPP 的极限性能。 工作负载：选择 Web1、Cache1、Cache2、Data Warehouse 四类生产工作负载，覆盖不同服务领域，均处理真实流量。 对比方案：默认 Linux 系统、NUMA Balancing、AutoTiering、TMO（Transparent Memory Offloading）。 性能指标：应用吞吐量（核心指标）、本地内存访问占比（底层支撑指标），实验中均关闭磁盘交换，系统内存充足。 6.2 TPP 有效性评估6.2.1 默认生产环境（2:1 配置） Web1：Web1 启动时需加载大量文件到内存，默认 Linux 的回收速度比 TPP 慢 44 倍，导致本地内存很快被文件缓存占满，新匿名页被分配到 CXL 节点且无法提升。默认 Linux 的本地内存访问占比仅 22%，吞吐量比基准低 16.5%。TPP 通过主动降级冷文件页，释放本地内存空间，使 92% 的匿名页分配到本地内存，本地访问占比提升至 90%，吞吐量仅比基准低 0.5%（如图 14a 所示）。 Cache1：Cache1 的匿名页与文件页占比稳定，匿名页初始分配到本地内存。默认 Linux 中，仅 8% 的热页滞留 CXL 节点，性能接近基准（吞吐量损失 3%）；TPP 通过提升所有滞留热页，使本地访问占比进一步提高，吞吐量损失降至 0.1%（如图 14b 所示）。 Cache2：Cache2 的匿名页虽多分配到本地内存，但两分钟内仅 75% 的匿名页为热页。TPP 可识别并降级冷匿名页，释放空间以提升热文件页，本地访问占比从默认 Linux 的 78% 提升至 91%，吞吐量损失从 2% 降至 0.4%（如图 14c 所示）。 Data Warehouse：数据仓库的文件页多为冷页，仅 13 匿名页为热页，默认 Linux 已能将大部分热页保留在本地内存，因此 TPP 与默认 Linux 性能接近（吞吐量损失均为 0.5%-0.7%）。但 TPP 通过优化匿名页与文件页分布，使本地匿名页占比从 67% 提升至 94%，本地访问占比提高 4%（如图 14d 所示）。 6.2.2 大规模 CXL 扩展（1:4 配置）该配置模拟 “小本地内存 + 大 CXL 内存” 的低成本场景，仅对 Cache 服务进行评估（Web 与数据仓库在该配置下不具生产实用性，但 TPP 仍能使其性能接近基准）。 Cache1：默认 Linux 中，文件页占据几乎全部本地内存，85% 的匿名页滞留 CXL 节点，吞吐量比基准低 14%。TPP 通过高效提升，将 97% 的 CXL 匿名热页（一分钟内访问）迁移到本地内存，同时降级冷文件页，使本地访问占比稳定在 85%，吞吐量损失仅 0.5%（如图 15a 所示）。 Cache2：默认 Linux 的吞吐量损失达 18%，本地匿名页占比仅 14%。TPP 将 80% 的 CXL 匿名热页提升到本地内存，尽管 41% 的内存访问仍来自 CXL 节点（文件缓存），但吞吐量损失降至 5%（如图 15b 所示）。 6.2.3 不同 CXL 延迟下的 TPP 表现为验证 TPP 对 CXL 延迟变化的适应性，在 2:1 配置下，为 Cache2 设置不同 CXL 延迟（220ns-300ns）。结果显示（如图 16 所示）： 无论 CXL 延迟如何，TPP 仅允许 4%-5% 的热页留在 CXL 节点，而默认 Linux 的 CXL 热页占比为 22%-25%； 默认 Linux 的平均内存访问延迟比 TPP 高 7 倍，吞吐量损失是 TPP 的 2.2-2.8 倍。 这表明 TPP 的热页识别与提升机制不受 CXL 延迟变化影响，始终能将热页高效迁移到本地内存。 6.3 TPP 组件贡献评估以 Cache1 的 1:4 配置为例，分析 TPP 各核心组件的作用。 6.3.1 分配 - 回收解耦的影响若关闭该特性，本地节点的回收触发延迟，高内存压力下，TPP 无法及时为提升页面预留空间，新分配页面也只能被分配到 CXL 节点： 分配速率：无 decoupling 时，本地节点分配速率受回收速率限制，波动较大；有 decoupling 时，本地节点分配速率在 95 百分位提升 1.6 倍（如图 17a 所示）； 提升速率：无 decoupling 时，本地节点内存频繁不足，提升几乎停滞；有 decoupling 时，提升速率稳定在 50KBs，峰值达 1.2MBs（99 百分位），确保 CXL 热页及时迁移，吞吐量损失从 12% 降至 0.5%（如图 17b 所示）。 6.3.2 活跃 LRU 热页识别的影响仅将活跃 LRU 页面作为提升候选，可有效减少不必要的提升： 提升速率降低 11 倍，降级后再提升的页面数量减少 50%； 尽管降级速率降低 4%，但本地节点空闲页利用率提升，提升成功率提高 48%； 本地内存访问占比提高 4%，吞吐量提升 2.4%； 性能收敛时间仅增加 5 分钟，对整体性能影响可忽略（如图 18 所示）。 6.3.3 页面类型感知分配的影响启用该策略后，Web 与 Cache 服务在小本地内存配置下仍能接近全本地性能（如表 2 所示）： Web1（2:1）：本地访问占比 97%，吞吐量损失 0.5%； Cache1（1:4）：本地访问占比 85%，吞吐量损失 0.2%； Cache2（1:4）：本地访问占比 72%，吞吐量损失 1.5%。 该策略通过初始分配优化，减少后续迁移开销，为特定应用提供更优性能基线。 6.4 与现有方案的性能对比6.4.1 TPP vs. NUMA Balancing AutoTiering Web1（2:1 配置）： NUMA Balancing：回收速度比 TPP 慢 42 倍，提升速率慢 11 倍，本地访问占比仅 20%，吞吐量损失 17.2%，且因冗余采样，CPU 开销比 TPP 高 2%； AutoTiering：虽回收速度较快，但分配 - 回收耦合导致预留缓存很快耗尽，CXL 访问占比达 70%，吞吐量损失 13%； TPP：吞吐量损失仅 0.5%，显著优于两种方案（如图 19a 所示）。 Cache1（1:4 配置）： NUMA Balancing：本地内存压力下停止提升，本地访问占比仅 46%，吞吐量损失 10%； AutoTiering：无法在 1:4 配置下运行（预热后查询阶段频繁崩溃）；在 2:1 配置下，TPP 的本地访问占比比 AutoTiering 高 10%，吞吐量提升 7%（如图 19b 所示）。 6.4.2 TPP vs. TMOTMO 通过监控应用资源 stall 情况，基于压力 stall 信息（PSI）将冷页卸载到交换空间，但该机制在 CXL 系统中存在局限： 内存利用率：TMO 仅能利用 CXL 内存的 45%（Web1）、61%（Cache1）、7%（Data Warehouse），而 TPP 能利用 83%、92%、87%； 性能开销：TMO 的交换机制需页错误触发页面加载，导致频繁 stall，而 TPP 无此开销。 TPP 与 TMO 可协同工作：TMO 通过交换释放系统级内存，为 TPP 的页面迁移提供更多空间，降低迁移失败率。例如，Web1（2:1 配置）中，TPP+TMO 的迁移失败率从 20 页 秒降至 5 页 秒，CXL 访问占比从 3.1% 降至 2.7%（如表 3 所示）；同时，TPP 将 TMO 的交换转化为 “降级 - 交换” 两阶段过程，减少 TMO 的进程 stall（从 70% 降至 40%），内存节省提升 3%（如表 4 所示）。 7 讨论与未来工作TPP 为第一代 CXL 分层内存系统的生产部署提供了可行方案，但随着技术发展，仍有以下方向值得探索： 7.1 多租户云环境的分层内存在多租户云环境中，TPP 需支持不同租户对内存层的竞争性共享。若本地内存占比高，现有机制可满足需求；但当租户具有不同优先级与 QoS 需求时，TPP 的无差别策略可能导致性能次优。未来需在 TPP 基础上集成 QoS 感知的内存管理，根据租户优先级分配本地内存资源，确保高优先级租户的性能保障。 7.2 面向带宽扩展的分配策略对于内存带宽密集型应用，CPU-DRAM 带宽常成为瓶颈。CXL 的额外带宽可通过将部分带宽敏感、延迟不敏感的页面分配到 CXL 内存，实现带宽扩展。未来需研究如何识别这类页面，确定最优分配比例，甚至探索硬件支持以提升识别精度，实现 “延迟 - 带宽” 双维度的页放置优化。 7.3 硬件辅助的页放置优化硬件特性可进一步提升 TPP 性能：（1）CXL ASIC 上的内存侧缓存与预取器，可降低 CXL 内存的有效延迟；（2）硬件支持的页面迁移，可减少 CPU 参与的迁移开销。当前 TPP 的迁移带宽为 4-16MBs（1-4K 页 秒），远低于 CXL 链路带宽，CPU 开销可忽略；但对于 “极小本地内存 + 极大 CXL 内存” 的极端配置，硬件辅助迁移将成为关键优化点。 8 相关工作8.1 分层内存系统随着低延迟非 DRAM 技术的发展，异构内存系统成为研究热点，已有大量工作探索利用 NVM 扩展主内存。CXL 技术的出现，为分层内存系统提供了介于 DRAM 与 NVM 之间的中间层，实现灵活且高性能的服务器设计，主流厂商均在推进 CXL 分层内存系统的研发与部署。 8.2 分层内存的页放置技术现有页放置技术可分为三类： 硬件辅助：依赖特定硬件特性实现页热度检测与迁移，但跨平台兼容性差，难以大规模部署； 应用引导：需应用修改代码或提供内存访问信息，增加开发成本，无法适配现有应用； 透明页放置：通过分析物理或虚拟地址空间的访问模式识别热页，但常因 TLB 失效、中断等导致高开销。 TPP 与现有透明页放置技术的核心差异在于：（1）复用 Linux 现有 LRU 机制与 NUMA Balancing，无需额外硬件支持，开销低；（2）解耦分配与回收逻辑，确保本地内存预留充足空间；（3）结合页面类型感知，优化初始分配与迁移策略。 基于交换的内存管理（如 TMO）将 CXL 内存视为交换空间，需页错误触发页面加载，而 TPP 将 CXL 内存视为常规内存，支持直接访问，避免页错误开销。AutoTiering 等方案虽也采用背景迁移与 NUMA Balancing 优化，但缺乏分配 - 回收解耦与页面类型感知，在高压力场景下性能劣于 TPP。 8.3 内存解聚内存解聚技术将远程主机的内存作为共享资源池，主要基于 RDMA 网络，延迟远高于 CXL（微秒级 vs 纳秒级），其内存管理机制与 TPP 正交 —— 可将 CXL 内存与远程解聚内存作为不同内存层，分别通过 TPP 与解聚管理方案优化。 9 结论本文通过轻量级用户态工具 Chameleon，分析数据中心应用的内存使用模式，证实了 CXL 分层内存系统的应用前景。基于分析结果，设计操作系统级透明页放置机制 TPP，实现热页在本地内存、冷页在 CXL 内存的高效放置。TPP 无需应用特定知识，通过轻量级降级、分配 - 回收解耦、精准热页提升与页面类型感知，最小化性能损失与开销。 在支持 CXL 的生产服务器集群中，通过多种工作负载评估表明：TPP 使分层内存系统性能接近全本地内存（差距 1%），相比默认 Linux 提升 18%，相比 NUMA Balancing、AutoTiering 提升 5%-17%。大部分 TPP 代码已整合到 Linux 内核，为超大规模数据中心的 CXL 分层内存部署提供实用解决方案。","tags":[null]},{"title":"CYY-RV64.zip","path":"/notebooks/paper/CYY-RV64.zip.html","content":"学习一下CYY师兄的工作，part1 https://www.rv64.zip/ BackgroundMotivation1.理想的RISC-V板子应当包含一组标准的指令拓展RVA23U64,但是目前的生态下，不同硬件支持的拓展与标准并不一致； 2.编译器和CPU没有针对新拓展进行优化的话，盲目打开新拓展反而会导致程序性能下降 Existing solution1.target_clones attributes 改代码，维护成本高 间接调用开销： When using target_clones or target_version, the compiler will use GNU IFUNC to dispatch the function call to the correct version at runtime. This introduces an overhead of an indirect function call, and also refuses some optimizations such as inlining. When compiling without -fno-plt or with -fno-pic, things will be worse since it requires 2 level call to the function (the first level is PLT call). 前置知识： PLT（Procedure Linkage Table) PLT 是动态链接（共享库）中用于实现 延迟绑定（Lazy Binding） 的核心结构，解决程序调用共享库函数时的跳转问题。 首次调用函数时： 程序跳转到 PLT 表中的对应条目（如 printf@plt）。 PLT 条目包含一条跳转指令，默认指向 动态链接器（_dl_runtime_resolve）。 动态链接器解析函数真实地址，并回填到 GOT（Global Offset Table）。 后续调用时： PLT 直接通过 GOT 跳转到真实函数地址（无需再次解析）。 -fno-plt 选项可绕过 PLT，直接通过 GOT 调用 IFUCN（Indirect Function） 运行时动态选择函数的具体实现,通过函数指针跳转，比直接调用多一次寻址，地址运行时确定，无法内联 定义时通过 __ attribute__((ifunc(resolver)))标记函数，提供一个解析器函数。 static void* my_func_resolver() if (__builtin_cpu_supports(avx2)) return my_func_avx2; else return my_func_default;void my_func() __attribute__((ifunc(my_func_resolver))); 最佳案例：GLib通过IFUNC为memcpy提供了多个实现。 Solution1.Decoupled function clone table​\t相当于将target_clones attributes从函数层提升到了文件层，不需要修改源代码。patch 2.Automatic function clone table generation​\t根据perf结果选择最佳的result 3.针对直接调用提了一些编译器端和CPU端的支持 期待CYY师兄的正式论文,后续继续学习 Other1.英语表达读起来好舒服 2.类似的方案在box64之类的二进制翻译场景下也大有用处。","tags":[null]},{"title":"ISCA`25 LIA A Single-GPU LLM Inference Acceleration with Cooperative AMX-Enabled CPU-GPU Computation and CXL Offloading","path":"/notebooks/paper/ISCA-25-LIA-A-Single-GPU-LLM-Inference-Acceleration-with-Cooperative-AMX-Enabled-CPU-GPU-Computation-and-CXL-Offloading.html","content":"Abstract​\t单GPU的内存容量限制了大模型推理，使得使用成本高昂的多GPU部署或者在慢速PCIE传输导致性能受限的CPU-GPU部署十分必要。在这个工作中，我们首先benchmark了最新的带有AMX的Intel CPU，包括4th SPR架构和 6th GNR架构的至强处理器，证明矩阵乘法的带宽达到了20TFLOPS和40TFLOPS，都比得上一些最新的GPU.\t这些发现解锁了更加广泛的CPU计算卸载，减少CPU-GPU传输，与之前代际的CPU相比缓解了带宽瓶颈。 补充：算力分析1.TFLOPS(Trillions of Floating-Point Operations Per Second):每秒万亿次浮点数运算 20~40TFLOPS为中等算力水平 2.算力场景中，更常使用吞吐量(Throughput)TFLOPS而非理论峰值算力，Throughput 更贴近实际任务的性能表现，Throughput 也是优化 LLM 推理性能的核心目标 3.常见GPU 算力： 4090 83TFLOPS | A100 312 FLOPS 基于以上发现，我们设计了LIA，一个单GPU大模型推理加速框架，协同AMX使能的CPU-GPU计算以及CXL卸载，LIA系统的卸载了计算到CPU上，优化了延迟和带宽。这个框架同样介绍了一个内存卸载策略，这个策略无缝的集成了便宜的CXL-DDR内存增强了带宽驱动型任务的性能表现。在有一张H100的SPR系统上，LIA对比之前的单GPU推理框架，达到了5.1到19倍的延迟降低以及3.7到5.1倍的带宽。并且，LIA部署了CXL卸载，产生了一个额外的1.5倍带宽提升（对比纯DDR方案）和1.8倍的最大batch size 提升。 1 Introduction​\tLLM在许多领域释放巨大的潜力，然而，这种前所未有的能力伴随着巨大的花费（指不断扩大的参数规模）。最近的大模型都设计了巨大的参数量，并且 似乎举例参数上限稳定线还十分遥远。这些参数量的增加引发了一个巨大的技术挑战：在一张GPU内保存模型参数和中间值（比如KV-cache和激活值）变得infeasible。即使最新的GPU例如H100达到了94GB的板载HBM内存，但是依旧难以面对推理时的内存需求。针对单GPU的能力限制，最近的一些工作已经转向了多GPU部署，这利用了模型的并行性。然而这些方法在非常昂贵的，同时操作起来十分复杂。例如，部署一个175bilion的参数的OPT模型至少需要5张H100GPU，总花费超过150000刀。因此，通过增加GPU的数量是一种在一些高性价比的经济场景中不是一个可行的方案。许多压缩技术，例如量化、剪枝、以及模型蒸馏（distillation)已经被提出来减轻大模型可拓展性的负担，尽管这些方法减轻了内存需求，然而他们通常以损失模型精度为代价，并且仍然需要多GPU。一个可选择的方向是系统级别的卸载，将模型参数存储在更大的CPU内存中然后按需传输到GPU中。然而，这些方法面临他自己的内存瓶颈，原因是PCIE的带宽有限（H100 的PCIE带宽是64GB，PCIe5.0),这见满了CPU-GPU的传输，导致了大量的推理延迟。 ​\t为了减轻大量数据的传输开销，一些方法已经提出选择一些层在CPU上进行计算，然而，这些CPU、GPU协同的框架的效率被CPU计算带宽限制，FlexGen和FastDecode这两个工作卸载了计算最不敏感的子层到CPU，然而PowerInfoer卸载冷神经元到CPU，这显著降低了模型的精度。除了复杂性以外，大模型需要根据应用的需求运行不同batch size的推理操作。虚拟助手、搜索引擎这这类面向用户接口的请求具有小batch 低延迟的特点，快速响应影响了用户的使用体验。相反，benchmark、信息提取之类的任务具有延迟不敏感性的任务，大batch、高带宽是非常重要的。 ​\t为了处理这些挑战，我们推出了LIA框架，一个单GPU推理加速框架，使用了AMX和CXL技术适配了小batch和大batch场景。LIA主要有3点贡献： AMX矩阵乘法的综合性能分析。 AMX驱动的CPU-GPU协同大模型推理激素 使用CXL内存来拓展带宽 LIA主要有两个组件组成：C1:前端算法，决定了那些子层卸载到CPU；C2：后端执行，无缝的集成了AMX CPU 、GPU。LIA允许所有的子层卸载到AMX使能的CPU上。 前端组件考虑了一些历史因素来决策最优的卸载策略，这些因素包括每个给定了batch size 和token length子层的每字节操作量、CPU-GPU传输数据量、CPU与GPU的计算吞吐量和内存带宽。将这些变量考虑在内，LIA最大化了资源利用率，最小化了端到端延迟LIA利用了大模型的一个特性：也就是每字节操作比会随着批次大小和输入长度动态波动。这使得在以后系统中，利用所给的批大小和输入长度决定卸载策略，能够得到最小的卸载延迟。 后端拓展了IPEX(这个拓展原本是用来给GPU或者单一的CPU进行加速)来无缝的集成CPU和GPU。后端拓展同样引入了进一步增强GPU内存和CPU-GPU计算资源的优化。 使用CXL内存时主要将模型参数卸载 到CXL内存，而DDR内存则存储中间值。 2.Background2.1 LLM inference 大模型由embeddingencoding layer组成，N个decoder layers ,LM head(linear 和 softmax). decoder layers主导了推理时间和内存消耗 一个decoder layer由多个子层组成，包括矩阵乘法、 层归一化、残差和sotfmax。 N个decoder 有一样的结构，不一样的参数。 模型接受了输入token序列，然后生成输出token，而后又把新输出的token作为输入，直到结束。 3 Performance Bottlenecks of Offloading Frameworks for LLM Inference","tags":[null,null]},{"title":"OCP China 2024 CXL 论坛学习笔记","path":"/notebooks/paper/OCP-China-2024-CXL-论坛学习笔记.html","content":"OCP China 2024 CXL论坛 学习笔记会议链接 阿里云 数据中心高性能Scale Up 互联系统趋势 孔阳 阿里云超高速互联负责人 胡文普 CXL部分 Scale Up 云的角度 关注两个计算 ： 通用计算、GPU计算 通用计算上： 考虑弹性分析：存储上-云盘技术 网络-CIPU网络虚拟化 内存上-CXL GPU上：大模型单次任务，数据并行、流水线并行、tensor并行、专家并行，都具有较高的带宽要求 CXL 机柜内的资源弹性实现需要满足高性能接口、资源共享、极致弹性、软件生态兼容性等要求。CXL特性符合这些要求。CXL将一致性访问从CPU内部拓展到CPU和内存之间，实现多服务器之间的互联。 CXL的演进 GIM ： P2P ： DSP与type3互联，type3可以分配给DSP或者host HBR-PBR:不止树状、星状、网络状 E3.s 热插拔，便于可拓展 JBOM 大容量 PEMEM ：支持2.0，满足redis实时性持久化等要求 基于 内存与CPU实现资源解耦 CXL在小数据上传输性能大大提高； intel: CXL on Intel Xeon@ Platform 赵森林 CXL overview CXL Specification Summary","tags":[null]},{"title":"MICRO-25-LongSight ｜Compute-Enabled Memory to Accelerate Large-Context LLMs via Sparse Attention","path":"/notebooks/paper/MICRO-25-LongSight: Compute-Enabled Memory to Accelerate Large-Context LLMs via Sparse Attention.html","content":"摘要基于 Transformer 的大型语言模型（LLMs）中的大输入上下文窗口有助于减少幻觉现象，提高输出准确性和个性化程度。然而，随着上下文窗口的扩大，注意力阶段在执行时间中的占比逐渐增加。键值（KV）缓存通过避免重复计算缓解了部分成本，但 KV 缓存本身可能很快超出当今 GPU 高带宽内存（HBM）的容量。在本研究中，我们提出了 LongSight，这是一种用于在大上下文场景下加速注意力计算的算法 - 硬件协同设计框架。LongSight 利用原本为密集检索加速设计的计算使能 CXL 内存设备，实现 KV 缓存存储和检索的卸载。因此，LongSight 有效地将成本相对较低的 LPDDR DRAM 的价值提升至高端 HBM 的水平。我们的实验表明，仅使用单个 GPU 和单个计算使能 CXL 内存扩展器，LongSight 就能为最先进的 Llama 模型高效支持高达 100 万个令牌的上下文长度。 1 引言预训练大型语言模型（LLMs）需要获取最新且相关的信息，以减少幻觉现象并生成准确、个性化的输出。这些信息通常作为模型输入上下文的一部分提供。事实上，在测试时技术（如思维链、少样本提示、草稿本提示、ReAct 等）、检索增强生成 以及输入数据（如长文档、代码或多轮交互）复杂度不断提升的推动下，LLMs 正越来越多地应用于需要扩展上下文窗口的场景中。 随着上下文长度的增加，推理过程的计算和内存需求也随之增长。特别是，随着上下文窗口的扩大，基于 Transformer 的 LLMs 中的注意力阶段往往会主导执行时间。键值（KV）缓存通过以增加内存压力为代价避免重复计算，在一定程度上缓解了这一问题。然而，对于更大的上下文窗口，KV 缓存的大小可能很快超出当前神经处理单元（NPUs）（如 GPU 或 TPU）上可用的高带宽内存（HBM）容量。 DReX是一种最新的计算使能 CXL 内存扩展器，用于加速密集检索。密集检索正越来越多地被用于实现检索增强生成（RAG），其中可检索项以高维嵌入向量的形式存储在向量数据库中，通过计算查询向量与嵌入向量之间的余弦相似度来识别语义相关项。DReX 在高容量 CXL 内存扩展器的 LPDDR DRAM 芯片内部及附近集成了轻量级加速器。它还引入了符号位过滤机制，无需从 DRAM 中获取完整的嵌入向量即可快速修剪搜索空间，从而显著提高密集检索的性能。 在本研究中，我们提出了 LongSight，这是一种用于在大上下文推理中加速注意力计算的算法 - 硬件协同设计框架。基于 DReX 的基础，LongSight 将其功能扩展到检索增强生成（RAG）之外，重新利用相同的计算使能 CXL 内存扩展器来加速基于 Transformer 的 LLMs 中的注意力机制。因此，LongSight 能够在极长的注意力上下文下实现高性能。 具体而言，LongSight 使神经处理单元（NPU）能够通过 DReX 中 CXL 提供的加载 存储接口存储 KV 缓存。LongSight 实现了一种混合密集 - 稀疏注意力算法：神经处理单元（NPU）在其本地高带宽内存（HBM）中保留最近的键值（KV）对滑动窗口，并对该窗口执行密集注意力计算，同时将稀疏注意力计算卸载到 DReX。对于稀疏部分，神经处理单元（NPU）通过 CXL 接口向 DReX 提交包含查询向量的注意力请求。然后，DReX 高效检索与查询向量点积相似度最高的前 k 个键。最后，神经处理单元（NPU）通过对密集键和稀疏键的组合集应用 softmax 函数，完成注意力操作。 我们的实验表明，配备单个 GPU 和单个 DReX 单元的 LongSight，能够为最先进的 Llama-3 1B 和 8B 模型高效支持高达 100 万个令牌的上下文长度。在当前系统中，只有使用 2 个 H100 GPU 才能支持如此长的上下文长度。在单个 GPU 支持的最大上下文长度下，LongSight 为 Llama-3 模型实现了高达 8.1-9.6 倍的吞吐量提升，以及每用户每秒 3.6-11.9 倍的令牌生成速度提升。 我们的主要贡献如下： 我们证实了先前的研究发现，即 Transformer 中的注意力主要受一小部分历史令牌的影响，这些令牌的键向量与当前查询向量表现出高的点积相似度。基于这一见解，我们利用密集检索加速的最新进展，实现了大上下文注意力计算。 我们提出了一种混合密集 - 稀疏注意力算法，该算法将短期注意力窗口保留在神经处理单元（NPU）的高带宽内存（HBM）中，并将长期注意力实现为键值向量数据库，通过前 k 点积相似度进行访问。 我们重新利用了最近提出的计算使能 CXL 内存扩展器 DReX（其最初设计用于密集检索），以加速我们的混合注意力机制。 2 背景2.1 基于 Transformer 的大型语言模型（LLM）图 1 展示了最先进的大型语言模型（LLM）架构。大型语言模型（LLM）推理包括预填充阶段和解码阶段。预填充阶段为用户输入提示构建键值（KV）缓存，而解码阶段则利用键值（KV）缓存以自回归方式生成新令牌。两个阶段共享相同的权重矩阵和模型架构，包括令牌嵌入层和多个解码器层。每个解码器层依次执行查询 - 键 - 值（QKV）生成、多头注意力计算、输出投影和前馈网络操作。查询 - 键 - 值（QKV）生成利用该层的权重矩阵，为每个输入令牌并行创建查询（Q）、键（K）和值（V）张量。之后，查询（Q）、键（K）和值（V）张量被划分为多个头，并传递到多头注意力层。在多头注意力层中，每个头计算每个查询（Q）张量与输入序列中所有先前的键（K）张量的注意力分数。注意力分数经过 softmax 函数处理后，再与相应的值（V）张量相乘。使用多个头使模型能够关注不同位置的不同表示子空间的信息。多头注意力使 Transformer 能够捕获多样化的上下文关系 ；例如，一个头可能用于确定词性，而另一个头可能用于确定情感。得到的向量通过输出投影层进行投影，并通过残差连接与输入嵌入相加，随后进入前馈网络。 图 1 的左侧和右侧分别展示了预填充阶段和解码阶段的解码器层。无论处于哪个阶段，查询 - 键 - 值（QKV）生成、输出投影和前馈网络都可以通过在多个用户之间共享权重矩阵来从批处理中受益，从而实现矩阵 - 矩阵乘法。相比之下，由于注意力计算利用的是编码每个输入令牌信息的查询（Q）、键（K）和值（V）张量，因此由于用户提示的差异，键值（KV）数据无法在一个批次中重复使用。输入长度因提示而异：在预填充阶段，输入长度对应于完整的用户提示长度，而解码阶段仅接收单个输入令牌。因此，预填充阶段的注意力计算涉及矩阵 - 矩阵乘法，而解码阶段则需要向量 - 矩阵乘法。 总之，除了解码阶段的注意力计算外，大多数大型语言模型（LLM）操作都是矩阵 - 矩阵乘法，这使得 GPU 能够高效利用计算资源。然而，解码阶段的注意力计算涉及向量 - 矩阵乘法，导致对内存带宽的高需求，并使 GPU 计算资源未得到充分利用。此外，随着输入上下文长度的增加，注意力计算需要更多的键（K）和值（V）张量。先前的研究表明，由于这些原因，解码器阶段的注意力计算可能成为主要的性能瓶颈，显著影响令牌生成吞吐量 。 2.2 分层 GPU 内存与 CXL近年来，以 GPU 为中心的架构和系统的进步推动了分层 GPU 内存的发展，将 GPU 的字节可寻址内存空间从本地高带宽内存（HBM）扩展到包括主机 DDR 内存甚至 NVMe 固态硬盘 。这些以 GPU 为中心的方法允许 GPU 按需访问内存或存储中的数据，而无需依赖 CPU 来启动或触发此类访问。 分层 GPU 内存的最简单实现是将主机内存映射到 GPU 的地址空间，使 GPU 能够通过 GPU 线程执行的加载 存储指令访问主机内存，或启动直接内存访问（DMA）传输。NVIDIA 最近推出的可扩展加速数据访问（SCADA）API，使 GPU 线程能够跨分层内存层次结构对无限大小的数据集执行多粒度和随机访问 。 计算快速链路（CXL）是一项行业标准，旨在提供对分布式内存的低延迟、字节级访问，同时支持传统上通过 PCIe 互连连接且无一致性的设备之间的缓存一致性。CXL 已被超大规模数据中心采用 ，并正被用于构建机架级共享内存系统 。在本研究中，我们重点关注 CXL 通过 PCIe 将基于 DDR 的内存（“3 类” 设备）连接到处理器的能力，使其能够通过标准加载 存储指令直接访问。 3 大上下文生成的最新技术先前的研究表明，随着序列长度的增加，全注意力计算的成本会逐渐主导运行时间。总序列长度包括输入上下文的大小和推理过程中生成的输出令牌数量。这两个维度在现代大型语言模型（LLM）应用中都变得越来越重要。需要更长的输入上下文来提供背景信息，并使大型语言模型（LLM）能够获取新鲜、相关的信息，以进行准确和最新的生成。同时，在需要推理和多步骤规划的任务中，生成更长的输出序列至关重要，特别是在智能体人工智能系统中，如 OpenAI 的 DeepResearch ，该系统利用强化学习，基于先前生成的令牌指导多轮生成和推理。这些系统需要大型语言模型（LLM）能够同时关注大量输入令牌和不断增加的生成输出令牌。 随着总上下文长度的增加，注意力机制在浮点运算次数（FLOPs）和内存容量方面都成为一个显著的瓶颈。此外，与前馈网络或其他受益于批处理的组件不同，注意力计算的成本无法在多个用户之间分摊。随着批处理大小的增加以提高计算效率，这一限制变得越来越突出。因此，这些挑战推动了非二次部分注意力机制（即稀疏注意力）的发展，以及利用内存内和近内存计算架构加速注意力执行的研究。 3.1 基于软件的稀疏注意力首先，我们讨论基于软件的稀疏注意力方法，这种方法已成为降低注意力计算成本的一种流行手段。Reformer [15] 通过使用局部敏感哈希（LSH）过滤掉不太可能相关的上下文令牌，在软件中实现了稀疏注意力。然后，仅将保留的令牌传递到注意力机制。这种概率过滤降低了后续注意力阶段的计算复杂度。 然而，Reformer 的基于局部敏感哈希（LSH）的过滤引入了具有线性时间复杂度的每令牌开销。此外，Reformer 执行多轮过滤，每轮过滤都需要额外的存储或重新计算哈希桶。因此，在针对密集点积计算进行高度优化的现代硬件上执行时，稀疏性带来的好处可能会被这些开销所抵消。 此外，Reformer 假设查询和键是相同的，这使得无法使用键值头数量少于查询头数量的配置 —— 这是一种常用的最新技术，用于减少内存占用并提高注意力计算效率。 Longformer [2] 采用了不同的稀疏注意力方法，通过结合滑动窗口和有限的全局注意力来实现非精确注意力。滑动窗口机制的一个关键优势是其计算简单性以及与当前硬件的兼容性，能够实现高效执行。 然而，仅靠滑动窗口注意力本身在捕获长距离依赖关系方面存在固有的局限性。为了解决这个问题，Longformer 通过一组少量的全局注意力令牌来增强局部滑动窗口注意力，这些全局注意力令牌可以广泛地关注整个序列。Longformer 中的稀疏注意力掩码是预定义且静态的，允许模型针对特定任务进行微调，以实现可定制的注意力模式。通过根据每个任务的结构定制掩码，这种方法能够相对于基准模型提高稀疏性。 尽管有这些优点，Longformer 设计的一个关键局限性是，注意力掩码必须针对每个任务进行手动配置，这被认为是一个显著的可用性和泛化性挑战 [2]。 深度求索人工智能（DeepSeek AI）[44] 报告称，“块级选择对于在现代 GPU 上实现高效计算至关重要。” 基于这一见解，他们提出了 NSA（可原生训练的稀疏注意力），该方法将块级稀疏性与压缩密集注意力和滑动窗口注意力相结合。虽然块级稀疏性能够在 GPU 硬件上高效实现，但由于其粗粒度，它限制了可实现的整体稀疏性。此外，NSA 需要进行长上下文微调才能有效执行，这带来了额外的计算和财务成本。 总之，现有的基于软件的稀疏注意力实现难以在不进行重大模型或任务特定修改的情况下，同时实现高稀疏性和低过滤开销。 3.2 基于硬件的注意力加速NeuPIMs [9] 和 AttAcc [29] 探索了混合架构，将传统的神经处理单元（NPUs）（如 GPU）与内存内处理（PIM）硬件相结合，用于大规模大型语言模型（LLM）推理。批处理大型语言模型（LLM）推理工作负载具有显著的异构性；因此，NeuPIMsAttAcc 在神经处理单元（NPU）上执行计算密集型流水线阶段（如预填充和前馈网络），同时将解码阶段的内存密集型注意力计算卸载到内存内处理（PIM）单元。然而，这些研究的一个关键局限性是它们使用全密集注意力，即使在内存内处理（PIM）硬件上执行，这种注意力计算仍然成本高昂。此外，NeuPIMs 依赖于双行缓冲机制，这带来了重大的实现挑战，需要对 DRAM 电路设计进行大量修改。 CENT [8] 采用了一种系统级、内存中心的方法，将所有 Transformer 操作卸载到近内存和内存内设备。CENT 的一个关键优势是它利用高带宽内存内处理（PIM）单元来加速内存密集型计算。然而，CENT 在内存内为注意力计算实现了 BFloat16 乘加（MAC）单元，这导致了显著的面积和能量开销。此外，CENT 的全系统设计用定制硬件替代了高度优化的 GPU 或神经处理单元（NPUs），以处理计算密集型 Transformer 组件，从而增加了整体系统成本。与 NeuPIMs 一样，CENT 也实现了密集注意力，这使得难以扩展该架构以支持高效的稀疏注意力。 DynaX [42] 采用了不同的方法，利用查询向量中的稀疏性，并对查询和键采用 4 位或 6 位量化，以降低计算近似注意力分数的成本。然后，这些近似分数被用于构建基于块的稀疏注意力掩码。DynaX 通过定制硬件进一步利用过滤块内的结构化稀疏性。然而，其性能最终受到过滤过程中加载键的成本的限制。即使采用量化，至少需要加载键内存占用的 14・616≈9.4% 来评估注意力分数，这对可实现的加速比构成了限制。 4 LongSight：大上下文注意力的算法 - 系统协同设计为了克服现有的基于软件和硬件的大上下文注意力方法的局限性，我们提出了 LongSight。LongSight 将基于 Transformer 的大型语言模型（LLM）中的注意力算法与系统和硬件架构进行协同设计，以实现任意长的上下文注意力，同时不影响现有基于 GPU 的机器学习框架的可组合性或可编程性。LongSight 基于以下观察结果引入了一种稀疏注意力算法：Transformer 主要关注先前的令牌，这些令牌对应的键向量与当前查询向量表现出高的点积相似度 [12]。在高层级上，LongSight 将键值（KV）缓存视为一个向量数据库，通过前 k 点积相似度查询进行访问，仅检索语义最相关的键，而不是关注整个键值（KV）缓存历史。 然而，这个向量数据库在几个关键方面与传统向量数据库有显著差异： 粒度和规模：它为每个键值头、层和用户维护独立的数据库。例如，在 Llama-3-8B（具有 8 个键值头和 32 个解码器层）中，每个用户对应 256 个独立的数据库。随着用户数量的增加，这个数字呈线性增长。 访问速率：数据库的访问速率为每秒令牌数（TPS）× 层数（L）× 注意力头数（H）× 批处理用户数（U），其中每秒令牌数（TPS）是令牌生成速率，层数（L）是解码器层数，注意力头数（H）是注意力头的数量，批处理用户数（U）是批处理中的用户数量。对于在优化系统上运行的 Llama-3-8B，即使是中等批处理大小，每秒令牌数（TPS）也可以达到数百个令牌 秒，这意味着每秒需要执行数十万个向量数据库查询。 延迟敏感性：由于基于 Transformer 的模型的自回归和顺序特性，这些向量数据库访问位于令牌生成的关键路径上。对于单用户批处理和 32 层，每秒生成 100 个令牌的情况下，每个层的注意力计算延迟预算约为几百微秒。我们将这个延迟预算称为注意力请求的服务水平目标（SLO）。 动态更新：与通常静态或变化缓慢的传统向量数据库不同，这个向量数据库需要频繁更新。在预填充阶段，整个数据库被初始化。在解码阶段，为每个用户生成的每个令牌添加一个新的键值对，导致高更新速率。 当前的方法无法满足上述要求。传统向量数据库通常使用穷举最近邻搜索（ENNS）或近似最近邻搜索（ANNS）来检索前 k 个最相似的向量。穷举最近邻搜索（ENNS）速度极慢，违反了注意力请求的服务水平目标（SLO）。实际上，密集注意力相当于对整个键值（KV）缓存执行穷举最近邻搜索（ENNS），使得基于穷举最近邻搜索（ENNS）的 LongSight 操作具有倒退性。 基于聚类和基于图的近似最近邻搜索（ANNS）虽然速度较快，但在索引构建和维护方面会产生显著开销。每次添加新向量时，都必须更新索引 —— 这个过程既昂贵又耗时。这就是为什么先前的研究只能使用近似最近邻搜索（ANNS）支持固定的长上下文 [12]。 为了在大上下文规模下实现准确且高性能的稀疏注意力，LongSight 使神经处理单元（NPU）（如 GPU）能够将用户的键值（KV）缓存存储在单独的计算使能内存设备上，并将大部分注意力计算卸载到该计算使能内存设备。LongSight 基于三个关键思想构建： 思想 1：多阶段内存内和近内存过滤与检索。第一个思想是摆脱依赖预处理索引的传统近似最近邻搜索（ANNS）方法，转而采用由 DReX [34] 开创的分层过滤机制，该机制通过多个阶段提供穷举最近邻搜索（ENNS）的准确性和近似最近邻搜索（ANNS）的速度： 阶段 1：内存内过滤。所有量化的键向量都排列在 DRAM 中，以便内存内处理（PIM）单元能够高效访问（图 2b ②’）。内存内处理（PIM）单元快速计算量化查询和键之间的相似度。我们基于键向量维度的全精度数据表示的符号位，对键向量维度进行一位量化（第 5 节）。使用宽松的阈值来过滤掉距离较远的键，同时确保不过滤掉语义相关的键。这极大地简化了存储，并消除了对复杂索引的需求。这使得低开销、动态过滤和高更新速率成为可能。此外，与其他仅支持有限数据类型的内存内处理方法不同，内存内过滤与任何有符号数据类型兼容。 阶段 2：近内存前 k 检索。通过阶段 1 过滤的向量被转发到近内存加速器，该加速器执行穷举全精度点积相似度搜索，以识别最相关的键和值（图 2b ③, ④）。然后，得到的前 k 个集合被返回给 GPU，用于最终的注意力计算（图 2b ⑤）。 思想 2：利用 CXL 实现 GPU 的细粒度访问。DReX 内部的这种多阶段内存内和近内存过滤满足了高带宽、低延迟和高精度的要求。为了支持细粒度访问，DReX 使用 CXL 将其内部内存和内存映射 IO（MMIO）寄存器直接暴露到 GPU 地址空间。现代 GPU API 通过诸如 SCADA 之类的机制支持这一点，允许 GPU 使用标准加载 存储指令访问远程内存。 思想 3：混合密集和稀疏注意力。由于 LongSight 需要在生成过程中频繁更新每个头、每层和每个用户的键值（KV）数据库，我们实现了一种混合密集 - 稀疏注意力策略。TODO","tags":[null,null]},{"title":"OSDI'25 Tiered Memory Management Beyond Hotness","path":"/notebooks/paper/OSDI-25-Tiered-Memory-Management-Beyond-Hotness.html","content":"Tiered Memory Management Beyond Hotness Jinshu Liu Hamid Hadian Hanchen Xu Huaicheng Li Virginia Tech https://github.com/MoatLab/SoarAlto/ Virginia 这个MoatLab对CXL内存的研究很深入，之前的Pond(ASPLOS `23)、Melody（ASPLOS`25）都出自这个实验室 1.IntroductionHot data is not always performance-critical and can reside in the slow-tier without degrading performance . Latency mitigation techniques, such as memory-level parallelism (MLP), obscure the true cost of memory accesses 1.先前的工作通过启发式或者内存访问成本间接的反应MLP的影响，但是仍然缺少准确的MLP建模和指标。 2.现有的内存分层方法具有难以轻量化和不准确的特点。尤其是一开始先放置本地内存的方法本身就是次优的，并且激进的迁移策略会导致过分的迁移。 所以作者就定义了一个MLP影响的指标，AOL，并且用来辅助放置决策和迁移决策。 Propose Amortized Offcore Latency (AOL), a novel performance metric that accurately quantifies the performance impact of memory accesses by integrating memory latency and MLP. 放置决策策略SOAR基于AOL进行排序，然后以此决定放置。ALTO依靠AOL来进行页面提升的过滤，可以与TPP等策略进行结合。 2.Background and MotivationMLP反映等待内存控制器实现的内存请求数量。 high-MLP access patterns： array traversals。 low MLP：pointer-chasing with depedent requests 把两个类型的访存一起跑，然后不同的分层策略依照不同的策略跑了测性能，发现把数组访问的放到快速层，反而会使性能降低。 3.Memory Performance PredictionRelating Slow-tier Performance to CPU Stalls离线分析 Performance degradation on the slow-tier is predominantly caused by increased CPU stalls due to LLC misses, which we refer to as LLC-Stalls 强调区分LLC-miss和LLC-Stall 慢速层单次miss造成的延迟更长，假设相同的miss,慢速层也会有更长的LLC-Stall. 论文中讲到基于LLC-Stall来预测减速的误差低于4%，开源以后可以预测一下基于LLC-Miss的（考虑预取器的影响）。 LLC-Stalls for Performance Prediction在线预测 发现快速层发生CPUStall的在慢速层也会发生。 所以用P SLLCc来预测慢速层的减速。 AOL for Accurate Prediction进一步研究发现，P在低MLP的场景下准确，但是在高MLP的场景下并不准确、主要是忽略了MLP的影响，高MLP会减少长延迟的影响。 随着延迟的增加，MLP的延迟掩盖受益会降低。 因此定义了AOL： 指标 事件 含义 𝑠𝐿𝐿𝐶 CYCLE ACTIVITY.STALLS L3 MISS L3 Miss时导致的Stall c CPU_CLK_UNHALTED.THREAD 非Halt下的时钟周期数 A1 OFFCORE_REQUESTS_OUTSTANDING.CYCLES_WITH_DATA_RD L2 Miss 后、请求完成前，这些内存读取请求在 SQ 中等待的周期数换言之，每个时钟周期检查是否存在至少一个load请求，有就加1 A2 OFFCORE_REQUESTS_OUTSTANDING.DEMAND_DATA_RD L2 miss 后，每个周期有多少个未完成的 Demand Load 请求在SQ中等待，即：请求堆积的深度压力。 A3 OFFCORE REQUESTS.DEMAND DATA RD L2 miss 后，被发往 uncore 的 load 请求的次数。 延迟计算运用到了排队论中的Little`s法则:平均在系统中的项数 L = 到达率 λ × 平均响应时间 W 结合事件： L 某个时间段内，在 uncore 正在等待完成的请求数（单位：个），即 “每周期 outstanding 的请求数” λ 请求到达速率（单位：请求周期）≈ 总请求数 总周期数 W 每个请求在系统中停留的时间（单位：周期）→ 平均延迟 W = L / λ = 平均 outstanding 请求数 / 到达速率 根据排队理论，计算得1。 MLP是平均每个周期内多少个inflight内存请求，衡量内存访问的并行度。 A2代表总的堆积请求数，总的堆积请求数（A2）除以总的堆积周期数（A1),得到每个周期的平均inflight请求数。 1、2代入的AOL。 延迟除以并行请求，得到了一个并行请求下单个请求的延迟影响。 然后作者定义了减速模型：S P x K. k f(AOL)的函数是用SP与AOL进行分析，反向呈现出特定的渐进双曲线，反推出了其数学模型。 关于a和b，与硬件相关而与工作负载无关的常数，两个特定的场景（指针追踪、数组访问）能够推出（估计待定系数法） 分析： AOL增加时（MLP减小或者Latency增大），K趋近到上界1，S接近P,预测由LLC.stallc决定。相反（MLP增大或者Latency减少），K趋近下界0，S减小 有了预测模型以后，基于时间序列预测了 4.Soar: Rank-based Static Object Allocation现有的初次放置的分层方案目的是最大化利用快速层内存。 作者希望寻找一种方法最初就能精准放置内存，降低内存迁移开销。 读到这句话的时候这难受。。 挑战： While AOL-based prediction is effective at the workload level, it falls short for individual objects due to the semantic gap between architectural events and object-level memory accesses. 尽管AOL预测在workload级别能够表现得很好，但是却无法在单个变量上表现很好。 key insight： ​\tdistribute CPU stalls across objects proportionally to their relative access frequencies based on the observed MLP and latencies, thereby approximating each object’s performance impact to application performance accurately. Object-Level Performance Profiling​\tPeriodically collects and processes three types of metrics: object metadata via object tracking memory accesses via PEBS-based LLC-miss sampling temporal performance via AOL-based prediction ①-②Object TrackingFlow 通过LD_PRELOAD的方式拦截修改，记录五元组对象流 ③-④用PEBS记录LLC misses、访问时间戳和vaddr ⑤-⑥基于AOL预测性能 ⑦ 合并三个对象流，基于时间戳来判断地址，有了访问时间戳，可以计算访问次数以及访问比例。 ⑧将访存比例与AOL减速预测结合，计算减速得分 具体计算算法： 极端场景下并行少，MLP1，减速打分等于时间段减速P*访存比例R 高MLP时，缩小评分 低MLP时，放大评分 作者随后解释了怎样设计的factor，以及计算单位字节得分等。 Object Allocation依然是打分之后进行排序，topk 放置到快速层 影响排名不一定与请求顺序相同，如果打分低的先到了，后续打分高的请求到了会使得打分低的请求降级。 问题：是依据调用栈来进行分组对变量进行标识的，这样在一个函数内部进行内存分配时，大家调用栈都相同，这样并无法区分。 具体要看代码实现是否区分时空调用？ 特别指出可以与一些异构内存感知的内存分配器同时使用（memkind、Unified Memory Framework) Use Cases and Limitations1.HPC、在线服务这种长时间访问的应用，静态分配不再最优 2.假设对象是均匀的，对象内部的访问每一页频率都差不多。 5.Alto: AOL-based Adaptive Page Migrations现在方案的不足： 1.某些迁移没有。只是表面热 2.迁移开销很大，策略到单次迁移需要12us,访问到迁移中的页导致CPU stall 3.CXL与local的延迟和带宽都在缩小，迁移开销的影响就显得很大 4.冷页不是真的冷。 虽然用 AOL（Amortized Offcore Latency）来设计基于性能感知的页迁移（page-level migration）策略是很有前景的，但目前仍面临一些独特的挑战，特别是在如何用现有粗粒度硬件性能计数器（performance counters）准确估算单个内存页的性能影响方面。 方法很简单，就是用AOL辅助平时的方法决策一下： 而后讲了与TPP、Nomad、NBT等方法的集成。 6.Evaluation 关注1：CXL模拟方式：SKX lowering the uncore frequency and disabling cores on one NUMA node 关注2：workloads : GAPBS、ML、caching、SPEC2017 执行过程中的排序 ALTO效果。 不足与机会：1.从MLP的角度分析，和预取有关系吗？对于预取的影响（如果这个东西能量化，也能分析出很多东西） 2.并没有考虑区分读写比例的影响？ 3.本文中第4节提到的通过访存比例来分配内存slowdown的做法是否合理？ 是因为其指标是基于perf stat的，如果全部用perf record的方法是否会更加精确？这一点作者没有详细描述。 hard 4.全部放在运行时进行变量分析会不会发生采样不准确的问题，在NeoMem中也有？但是如何解决？可不可以结合NeoMem完全捕获数据流？ 基于采样，长生命周期的可能采集到，短生命周期取样。 机器的拓展性，针对SPX，其他的SPR、EMR的对应事件的拓展性替代如何，是否都只是PEBS？ 乱序校正的影响大不大？ 例如三段式的，第一次进行热点代码识别，第二次将热点代码全部卸载到CXL，然后利用Neomem捕获trace，从而准确感知，最后根据决策实现数据放置？ hard 5.结合内存分配器进行小变量页内集中优化，大变量的访问是否集中？不集中的话可以用perf采集地址，然后绘制访存直方图。","tags":[null]},{"title":"龙蜥CXL讲解-高显扬","path":"/notebooks/paper/龙蜥CXL讲解-高显扬.html","content":"CXL技术介绍对视频链接的PPT搬运，方便快速阅读 高显杨 浪潮 综述 协议协议演进 1.1内存拓展 2.0内存赤化 3.0特性 CXL子协议 CXL设备 CXL Fabric 参考VPN，下边两张为单switch CXL RAS特性 内存热插拔？如何避免宕机 CXL初始化 RCRB CXL 1.1跑CXL2实现协议兼容 CXL方案 池化管理：FM中的bind和UnBind 业界方案 补充","tags":[null]},{"title":"Hexo","path":"/notebooks/other/hexo.html","content":"Hexo部署过程中遇到的一些问题图片与Typora兼容网上给了很多方法，尝试没有效果，自己尝试的方案： 卸载插件hexo-asset-image: yarn remove hexo-asset-image,GitHub有修改的对应插件，但是使用无效果 typora配置 Typora新插入图片路径显示为： Hexo部署时正常显示 Hexo不生效问题Hexo部署到github上不生效，网上解决方案说需要除Main或者Master分支以外额外构建一个分支，然后网站推送到这个分支上，实际使用不管用。 首先，只需要维护一个主分支，不需要其他分支； 其次，hexo -d执行以后，main分支中即使已经更新了代码，但是github page更新也需要一段时间，部署频繁会产生排队 仓库主页github-pages上可以观察到生成静态页面的进度。","tags":[null]},{"title":"perf使用","path":"/notebooks/other/perf使用.html","content":"Perf使用 杂记1.文档1.man+perf -h遇到新工具当然最好的方式是读文档，但是网络上找半天也没有找到很好的perf手册，最后发现在GPT的指引下，最好的手册害得是linux man手册。 上手最快最全面的方式：help+man 首先perf -h查看各条子命令基本功能。 在使用子命令时 man perf-record、man perf-script查看各条子命令的详细功能。 踩坑案例： 折腾半天perf record记录的L3 miss，perf script查看对应的结果，发现里面有一些地址，和程序malloc时的地址进行匹配发现都没有访问程序malloc的位置，最后发现其中的地址是ip而不是访问的内存地址。几番查找资料无果，最后发现perf record中使用-d参数才会显示addr 2.event查看各个事件的详细描述，在intel 官方的事件库中查找对应的事件功能 https://perfmon-events.intel.com/index.html?pltfrm=skylake_server.html 3.文章与优化案例https://www.brendangregg.com/ https://weedge.github.io/perf-book-cn/zh/chapters/3-CPU-Microarchitecture/3-8_Modern_CPU_design_cn.html 2.踩坑杂记perf事件冲突","tags":[null]},{"title":"LLVM","path":"/notebooks/compiler_kernel/LLVM/LLVM.html","content":"入门llvm笔记 1.新增pass流程新增教程： https://llvm.org/docs/WritingAnLLVMNewPMPass.html1.新增pass 的流程从基于虚函数的运行时多态变更为了基于CRTP的编译期多态。一个优点是降低了运行时开销。 2CRTP（奇异递归模板模式）(以下内容基于AI生成后修改) 通常通过继承的方式实现单例模式也是这样，只是不知道叫这个名字 CRTP（Curiously Recurring Template Pattern，奇异递归模板模式）是C++中的一种高级模板编程技术，通过让一个类继承自以自身为模板参数的基类模板，实现静态多态或代码复用。 template typename Derivedclass Base /* 基类使用Derived类型 */ ;class MyClass : public BaseMyClass /* 派生类将自身作为模板参数传递给基类 */ ; 核心作用1. 静态多态（编译时多态） 动态多态（虚函数）的问题：运行时虚表查找导致性能开销。 CRTP的解决方案：基类在编译时通过模板参数直接调用派生类的方法。 template typename Derivedclass Animal public: void speak() static_castDerived*(this)-speakImpl(); // 编译时确定调用 ;class Cat : public AnimalCat public: void speakImpl() std::cout Meow ; ;class Dog : public AnimalDog public: void speakImpl() std::cout Woof ; ;// 使用AnimalCat cat;cat.speak(); // 输出 Meow（无虚函数开销） 2. 代码复用 基类可提供通用逻辑，派生类通过特化实现差异部分。 template typename Derivedclass Counter protected: static int count;public: Counter() ++count; static int getCount() return count; ;template typename Derivedint CounterDerived::count = 0;// 统计对象实例数的类class Widget : public CounterWidget ;class Gadget : public CounterGadget ;// 使用Widget w1, w2;Gadget g1;std::cout Widget::getCount(); // 输出 2std::cout Gadget::getCount(); // 输出 1 CRTP vs 虚函数 特性 CRTP 虚函数 多态时机 编译时 运行时 性能 无额外开销（直接调用） 虚表查找开销 灵活性 类型固定（模板参数需明确） 支持运行时类型动态替换 适用场景 高性能库、框架基础设施 需要运行时动态行为的情况 CRTP的典型应用场景 编译时多态：如数学库中的向量矩阵运算（Eigen库）。 对象计数：统计不同派生类的实例数量。 Mixin模式：为类动态添加功能（如LLVM的PassInfoMixin）。 链式调用：返回派生类引用以实现链式语法（return static_castDerived(*this);）。 3 LLVM与CPP实现DenseMap与stdMapdyn_cast与RTTI isadef-use user-value为什么llvm中User会继承value 4 支配树基本概念支配树 wiki 严格支配（Strict Domination）：如果 A 支配 B 且 A ≠ B，则称 A 严格支配 B。 立即支配者（Immediate Dominator, idom）：对于某个基本块 B，其被严格支配的所有基本块中，离 B 最近的那个称为 B 的立即支配者。 LLVM中的使用场景 死代码消除 (Dead Code Elimination) 死代码消除有多种方法多种粒度，这里block为粒度可以使用支配树方法 如果一个基本块不在入口块的支配树上，那说明它在某些路径上根本无法到达，因此可能是不可达代码。 if (!DT-dominates(entry, B)) B-eraseFromParent(); // 删除不可达的基本块 循环识别 (Loop Identification) LICM（Loop-Invariant Code Motion）是一种经典的循环优化策略。要安全地将一条指令从循环体中移出（外提到前置块中），必须保证其在所有循环入口都被执行。 判断某条指令的基本开是够被循环入口块支配，如果支配，则说明每次都会执行这条指令，可以外提 静态单赋值节点Φ 节点插入 关键类与关键接口 DominatorTree：核心支配树实现，llvm/IR/Dominators.h DominatorTreeWrapperPass：将支配树封装为分析Pass DominatorTreeAnalysis：支配树分析 DominatorTree DT = getAnalysisDominatorTreeWrapperPass().getDomTree(); 作用： invoke callCallBaseInvokeInst","tags":[null]},{"title":"Kernel-compile","path":"/notebooks/compiler_kernel/Linux kernel/Kernel-compile.html","content":"内核编译#查看当前内核版本uname -a#源码获取sudo apt-get install linux-sourcecd /usr/srctar xvf linux-source-*.tar.bz2cd linux-source-*#补丁应用patch -p1 /path/to/patch.diff#`-p1`选项可能需要根据补丁文件的格式进行调整。#编译一些老版本时，config文件差异比较大， 解决方法是使用一些老版本的pve kernel 的配置文件# http://mirrors.ustc.edu.cn/proxmox/debian/dists/bullseye/pve-no-subscription/binary-amd64/#wget 下载好以后，dpkg -i 安装，之后copy对应的内核config文件进行编译make menuconfig #或者oldconfig,本质是配置/usr/src/configmake -j$(nproc)#根据 .config 配置文件编译内核、内核模块和其他必要的文件，生成内核镜像（vmlinuz）和其他相关的文件#编译过程中可能出现每包头文件或者其他错误，可能是版本原因，针对报错解决即可#内核安装#安装过程可能会出现头文件报错或者缺库、或安装完成后缺库，只需要apt安装对应的库就好，make过程中会自动hook#安装内核模块(在运行时可以加载或卸载的模块)#从编译输出的目录（如 lib/modules/kernel_version/）,并执行 depmod 来生成模块依赖关系sudo make modules_install#内核镜像（如 vmlinuz）、配置文件（如 config）、符号表文件（如 System.map）和 initrd 镜像安装到 /boot 目录sudo make install#更新启动引导程序sudo update-grup 关于配置选项相关操作，参考：:star::star:Linux 内核动手编译实用指南 KGDB配置编译选项 参考资料KGDB原理分析及远程挂载调试ARM64内核 内核启动参数 kgdboc=ttyS0,115200 kgdbwait kgdbtcp=192.168.1.2:1234 kgdboc=ttyS0,115200：设置串口调试（可选） kgdbwait：启动时等待调试器连接 kgdbtcp=192.168.1.2:1234：被调试主机的IP和端口 永久修改：/etc/default/grub下GRUB_CMDLINE_LINUX变量 proxmox-boot-toolproxmox-boot-tool，一个脚本，设置启动内核、增删内核等,使用设置grub内核启动参数的方式不一定生效的情况下可以使用这个工具。 proxmox-boot-tool git clone https://git.proxmox.com/git/proxmox-kernel-helper.git之后 make deb会在当前路径下生成对应的deb包，apt install安装后可以正常使用/usr/sbin/proxmox-boot-tool 参考链接：https://kernelnewbies.org/KernelBuild","tags":[null]},{"title":"Neoperf_study","path":"/notebooks/compiler_kernel/Linux kernel/Neoperf-study.html","content":"NeoPerf study 本文主要为学习论文《NeoMem: HardwareSoftware Co-Design for CXL-Native Memory Tiering》的工作，分为三个部分，用户态、内核以及FPGA部分，内核开源仓库地址为： PKUZHOUlinux 代码基于linux内核代码6.0开始修改。 由于初步探索linux内核代码，所以没有按照自顶向下的视角分析代码，而是基于git提交记录，借助AI与互联网搜索，平铺遇到的相关知识。版本不断更新…… a naive neoprof drivercommit ID 9bd35383 本次主要在driver目录下提交了一个驱动neoperf: 主要是实现了一些对外设的IO操作 Linux内核配置文件KconfigKconfig文件用于定义内核配置菜单，这些菜单可以在编译内核时启用或禁用特定的功能。 #Kconfig文件config NEOPROF#定义了一个名为`NEOPROF`的内核配置选项，将在内核配置菜单中创建选项\tbool Enable Neoprofiler #bool类型，是否启用 default n #默认不启用 在内核编译过程中可以在Drivers下查找到 Kconfig有其独特的语法，也是可以一层一层包裹下去：menu、source、endmenu等组成了编译选项配置过程中的树状菜单 Kconfig设置对应的编译变量后，makefile指导构建编译的过程中会利用这些变量，从而实现选择性的编译 neoperf.h主要新增了四个接口，对neoprof设备（此处指Type2-CXL设备）进行访问： /* * The following functions are used to access the neoprof device*/u64 get_nr_hotpages(void);//获取当前系统中的热页数量u64* get_hotpages(void);//获取热页u64 get_hotness_threshold(void);//获取热度阈值void set_hotness_threshold(u64 threshold);//设置热度阈值 neoperf.c驱动开发hello worldneoperf.c 以下部分参考驱动开发知识：https://www.cnblogs.com/downey-blog/p/10500828.html module_init(neoprof_init);module_exit(neoprof_exit);MODULE_LICENSE(GPL v2);MODULE_AUTHOR(PKUZHOU);MODULE_DESCRIPTION(Neoprofiler Linux driver); io地址映射相关知识需要理解 IO端口的编址方式： 包括IO指令的端口映射方式、MMIO的统一内存映射方式 一些常见的IO操作函数 void * ioremap(unsigned long phys_addr, unsigned long size, unsigned long flags); //memset_io\\memcpy_fromio\\readb 参考：https://blog.csdn.net/do2jiang/article/details/5450839 neomem migration skeletoncommit ID 26cabad18 新增 NeoMem模块: migrate.c主要是提供接口migrate_misplaced_page_no_vma 调用一些mm中的内存操作函数，进行页面隔离、迁移 neomem.h eomem.c主要就是启动 neomem模块（调用core文件中启动守护进程） late_initcall()在内核启动后期适当时间执行，理解module_init等init宏的顺序，在include/linux/init.h中 linux设备驱动加载的先后顺序 neomem_core.c 内存中的各种分配函数 kthread_run内核线程 FPGA端侧模块结构顶层模块： cxltyp3_memexp_ddr4_top-ed_top_wrapper_typ3 ddr内存参数调整cxl ip考虑了不同ddr的，包括是否支持DBI、内存通道数量等。采用宏的方式区分，设置不同的方式时，需要对ip内通过宏定义来确定相关的内存参数，同时也需要在顶层模块对相关参数进行修改。 .ewiflfomhwoi{zoom:50%;} .zjedgavxqyoc{zoom:50%;} .kmksdqqyojvw{zoom:50%;} 或者通过更改ip文件，重新生成新的IP文件夹 .djcxlntwijgd{zoom:50%;} quartusset_global_assignment -name OPTIMIZATION_MODE AGGRESSIVE COMPILE TIME 烧录模式AS Jtag ps 三种烧录模式 Neomem todo代码存在一些可以完善的地方： CXL地址采用硬编码，可以引入设备树或者其他检测CXL物理地址的工具进行优化，参考","tags":[null,null,null,null,null]},{"title":"linux脚本备忘录","path":"/notebooks/compiler_kernel/Linux kernel/linux脚本备忘录.html","content":"Linux脚本备忘录安装系统后的环境准备添加新用户 adduser #封装命令，处理完添加用户的全部过程 su到新用户显示username@hostname~$:useradd #底层命令，什么都没有加,su到新用户显示$:# 生成 8 位强密码PASSWORD=$(openssl rand -base64 6 | cut -c1-8)echo Password: $PASSWORD 用户添加sudo组 usermod -aG sudo new_user#加完以后记得:newgrp sudo#作用有3：#1.切换到指定的组上下文#2.即时生效组更改#3.启动一个子 shell 配置sshd#服务端安装apt install ssh-server#配置sudo vim /etc/ssh/sshd_config#重启服务sudo service restart sshd VimVim配置推荐 - ma6174 wget 47.93.11.51:88/install_vim.shbash install_vim.sh zsh#安装zshsudo apt install zsh#修改默认shell为zshchsh -s /bin/zsh#安装oh-my-zshsh -c $(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)##如果不成功，请执行下面两条命令，成功了就不需要做下面两条wget 47.93.11.51:88/install_zsh.shbash install_zsh.sh#安装zsh-syntax-highlightinggit clone https://github.com/zsh-users/zsh-syntax-highlighting.git $ZSH_CUSTOM:-~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting 历史记录推荐命令插件##命令自动推荐，根据历史记录git clone https://github.com/zsh-users/zsh-autosuggestions $ZSH_CUSTOM:-~/.oh-my-zsh/custom/plugins/zsh-autosuggestions 命令自动补全##命令自动补全插件mkdir ~/.oh-my-zsh/plugins/incrwget http://mimosa-pudica.net/src/incr-0.2.zsh -O ~/.oh-my-zsh/plugins/incr/incr.plugin.zsh##目录自动跳转插件sudo apt install autojump .zshrc配置文件配置#插件添加zsh-syntax-highlightingplugins=(git zsh-syntax-highlighting) #设置终端颜色，提示符，及上一条指令返回码提示autoload -U colors colorsPROMPT=%$fg[red]%%n%$reset_color%@%$fg[blue]%%m %$fg[yellow]%%1~ %$reset_color%%# RPROMPT=[%$fg[yellow]%%?%$reset_color%]# Useful support for interacting with Terminal.app or other terminal programs[ -r /etc/zshrc_$TERM_PROGRAM ] . /etc/zshrc_$TERM_PROGRAMsource ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions/zsh-autosuggestions.plugin.zshsource /usr/share/autojump/autojump.shsource ~/.oh-my-zsh/plugins/incr/incr*.zsh ctags#安装sudo apt install ctags #建立索引ctags -I __THROW -I __attribute_pure__ -I __nonnull -I __attribute__ --file-scope=yes --langmap=c:+.h --languages=c,c++ --links=yes --c-kinds=+p --c++-kinds=+p --fields=+iaS --extra=+q -f ~/.vim/systags /usr/include/* /usr/include/x86_64-linux-gnu/sys/* /usr/include/x86_64-linux-gnu/bits/* /usr/include/arpa/* .vimrc添加索引 set tags+=~/systags 安装glibc-doc 使用以下命令安装 sudo apt install glibc-doc 常见路径hostname :/etc/hostname host: /ect/hosts tomcat#安装JDK8sudo apt install default-jre -ysudo apt install openjdk-11-jre-headless -ysudo apt install openjdk-8-jre-headless -y #sudo wget https://downloads.apache.org/tomcat/tomcat-8/v8.5.65/bin/apache-tomcat-8.5.65.tar.gzsudo wget https://mirrors.bfsu.edu.cn/apache/tomcat/tomcat-8/v8.5.73/bin/apache-tomcat-8.5.73.tar.gztar zxf apache-tomcat-8.5.73.tar.gzsudo mv apache-tomcat-8.5.73 /usr/local/tomcat#建立软连接sudo ln -s /usr/local/tomcat/bin/* /usr/local/sbin/#启动startup.sh start #端口检查netstat -anput | grep 8080#启动命令startup.sh start #//启动shutdown.sh #//关闭catalina.sh stop #//启动catalina.sh start #//关闭#关闭防火墙sudo ufw disable #tomcat 参数配置vim /usr/local/tomcat/conf/server.xml #.......Connector port=8081 protocol=HTTP/1.1 #将之前8080端口改成8081端口connectionTimeout=20000 # redirectPort=8443 /#目录修 # Host name=localhost appBase=/opt/www #将网站根目录改到/opt/www # unpackWARs=true autoDeploy=true#更改网站家目录，这里的ROOT必须大写，更改完成后需要重启sudo mkdir /opt/www/ROOT -p gdb 多窗口调试通过TUI（Text User Interface）提供多窗口调试环境启动： gdb -tui a.out 或运行过程中 ctrl+X+A开启或者关闭TUI场景下操作： layout src：显示源代码窗口和命令窗口 layout asm：显示汇编代码窗口和命令窗口 layout split：同时显示源代码、汇编代码和命令窗口 layout reg：显示寄存器窗口（会与 src 或 asm 窗口同时显示） focus next 命令可以将焦点切换到命令（cmd）窗口，之后方向键就可以用来翻找历史命令 focus prev 可以切回 src 窗口 LLDBmac 使用gdb要配置签名，lldb 下直接输入gui ，有一个图形化界面 mysqlmysql 8.0下载 wget https://repo.mysql.com//mysql-apt-config_0.8.20-1_all.deb #MySQL 设置#密码sudo mysql -uroot use mysql;update user set authentication_string=PASSWORD(自定义密码) where User=root;update user set plugin=mysql_native_password where User =root;flush privileges;quit; 对于Linux和windows下字符集不兼容的情况，需要替换 • 把文件中的所有的utf8mb4_0900_ai_ci替换为utf8_general_ci• 以及utf8mb4替换为utf8• 如上图所示的位置，上图只是一部分，注意全部替换。 数据库导出 mysqldump -uroot -p c:ShareYunAlbum。sql 数据库导入 use ShareYunAlbum source ~/ShqreYunAlbum.sql 卸载mysql sudo apt purge mysql-* -ysudo rm -rf /etc/mysql/ /var/lib/mysqlsudo apt autoremovesudo apt autorecleansudo apt-get remove mysql-common dpkg -l |grep ^rc|awk print $2 |sudo xargs dpkg -P","tags":[null]},{"title":"llama.cpp","path":"/notebooks/other/大模型/llama-cpp.html","content":"Llama.cpp源码浅析ggml 源码结构学习入口函数简单以llama.cli作为推理学习的入口。其入口函数main位置为：llama.cpp/tool/main.cpp/main() 关键数据结构内存管理Arena 分配器“批发内存，零售指针，整单清场” 批发内存： 零售指针 整场清除： mmap模型加载流程： 核心步骤： llama_model_loadre 核心对象. 之后加载模型架构arch\\超参数hparams\\词表vocab\\元数据信息、以及张量tensors","tags":[null]},{"title":"大模型入门","path":"/notebooks/other/大模型/大模型入门.html","content":"大模型基础概念入门Transformer自注意力（Self-Attention）Q\\K\\V Q：目前关系的问题，当前token; K:token的标签 V:包含的信息 Q*K得到谁更重要，之后再乘以V得到这些重要的人说了什么信息。 除以根号dk与softmax是数学策略。 前馈网络FFN简单而言就是增加维度-增加信息-降低维度。-增加非线性变化。 层数影响逐层抽象。浅层学习低级特征（词性、局部语法），深层捕捉高级语义 输入空间—Layer 1—语法空间—Layer 2—语义空间—…—推理空间 单层表示： Layer(x)LayerNorm(x+FFN(LayerNorm(x+Attention(x)))) 多层复合： Model(x)LayerN(LayerN−1(…Layer1(x))) Prefill Decoder Prefill（预填充）：处理输入的所有已知 tokens，计算它们的隐藏状态并填充 KV Cache。 Decoder（解码）：基于 KV Cache 逐个生成新 token，直到结束。 1.为什么要提前计算所有的tokens？ 2.怎么计算kv的？ 3.什么是 token 的隐藏状态 4.QKV权重矩阵是干嘛的？ 5.什么是PD分离 优化策略计算图优化与算子融合 投机采样 FlashAttention","tags":[null]}]